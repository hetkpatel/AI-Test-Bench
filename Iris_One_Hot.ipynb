{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_train_tensor = F.one_hot(y_train_tensor, num_classes=3).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "y_val_tensor = torch.from_numpy(y_val)\n",
    "y_val_tensor = F.one_hot(y_val_tensor, num_classes=3).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_tensor.shape[0], X_val_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP architecture\n",
    "class Iris_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Iris_MLP, self).__init__()\n",
    "        self.ly1 = nn.Linear(4, 20)\n",
    "        self.ly2 = nn.Linear(20, 20)\n",
    "        self.final = nn.Linear(20, 3)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ly1(x)\n",
    "        out = self.sig(out)\n",
    "        out = self.ly2(out)\n",
    "        out = self.sig(out)\n",
    "        out = self.final(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MLP model\n",
    "model = Iris_MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 0.31159159541130066\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X_val_tensor)\n",
    "before_train = criterion(y_pred.squeeze(), y_val_tensor)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.3195977210998535\n",
      "Epoch 1: train loss: 0.2611084282398224\n",
      "Epoch 2: train loss: 0.26518288254737854\n",
      "Epoch 3: train loss: 0.25922852754592896\n",
      "Epoch 4: train loss: 0.23850348591804504\n",
      "Epoch 5: train loss: 0.2181243747472763\n",
      "Epoch 6: train loss: 0.20907799899578094\n",
      "Epoch 7: train loss: 0.20991556346416473\n",
      "Epoch 8: train loss: 0.2100074291229248\n",
      "Epoch 9: train loss: 0.2009638398885727\n",
      "Epoch 10: train loss: 0.1847846955060959\n",
      "Epoch 11: train loss: 0.17052096128463745\n",
      "Epoch 12: train loss: 0.16504503786563873\n",
      "Epoch 13: train loss: 0.16517874598503113\n",
      "Epoch 14: train loss: 0.16146771609783173\n",
      "Epoch 15: train loss: 0.1501287966966629\n",
      "Epoch 16: train loss: 0.13679829239845276\n",
      "Epoch 17: train loss: 0.12898197770118713\n",
      "Epoch 18: train loss: 0.1273760199546814\n",
      "Epoch 19: train loss: 0.12596315145492554\n",
      "Epoch 20: train loss: 0.12112215161323547\n",
      "Epoch 21: train loss: 0.11643597483634949\n",
      "Epoch 22: train loss: 0.11668068915605545\n",
      "Epoch 23: train loss: 0.11977354437112808\n",
      "Epoch 24: train loss: 0.11958976835012436\n",
      "Epoch 25: train loss: 0.11724883317947388\n",
      "Epoch 26: train loss: 0.11730483919382095\n",
      "Epoch 27: train loss: 0.11894285678863525\n",
      "Epoch 28: train loss: 0.11835683137178421\n",
      "Epoch 29: train loss: 0.11539600789546967\n",
      "Epoch 30: train loss: 0.11298870295286179\n",
      "Epoch 31: train loss: 0.11244896054267883\n",
      "Epoch 32: train loss: 0.11223623901605606\n",
      "Epoch 33: train loss: 0.11114703863859177\n",
      "Epoch 34: train loss: 0.11010050773620605\n",
      "Epoch 35: train loss: 0.1099499985575676\n",
      "Epoch 36: train loss: 0.10998345911502838\n",
      "Epoch 37: train loss: 0.10938017070293427\n",
      "Epoch 38: train loss: 0.10852478444576263\n",
      "Epoch 39: train loss: 0.10804584622383118\n",
      "Epoch 40: train loss: 0.10750892758369446\n",
      "Epoch 41: train loss: 0.10622434318065643\n",
      "Epoch 42: train loss: 0.10463569313287735\n",
      "Epoch 43: train loss: 0.10356765240430832\n",
      "Epoch 44: train loss: 0.10284366458654404\n",
      "Epoch 45: train loss: 0.1017669141292572\n",
      "Epoch 46: train loss: 0.10036918520927429\n",
      "Epoch 47: train loss: 0.09916340559720993\n",
      "Epoch 48: train loss: 0.09805849194526672\n",
      "Epoch 49: train loss: 0.09650436788797379\n",
      "Epoch 50: train loss: 0.09450516104698181\n",
      "Epoch 51: train loss: 0.09247878938913345\n",
      "Epoch 52: train loss: 0.09038148075342178\n",
      "Epoch 53: train loss: 0.08792601525783539\n",
      "Epoch 54: train loss: 0.08524665236473083\n",
      "Epoch 55: train loss: 0.08255945146083832\n",
      "Epoch 56: train loss: 0.07967967540025711\n",
      "Epoch 57: train loss: 0.07640385627746582\n",
      "Epoch 58: train loss: 0.07283953577280045\n",
      "Epoch 59: train loss: 0.06914833188056946\n",
      "Epoch 60: train loss: 0.06517013162374496\n",
      "Epoch 61: train loss: 0.06085372716188431\n",
      "Epoch 62: train loss: 0.05648992210626602\n",
      "Epoch 63: train loss: 0.05211438238620758\n",
      "Epoch 64: train loss: 0.04767055809497833\n",
      "Epoch 65: train loss: 0.04328269511461258\n",
      "Epoch 66: train loss: 0.039142534136772156\n",
      "Epoch 67: train loss: 0.03529712185263634\n",
      "Epoch 68: train loss: 0.03185218945145607\n",
      "Epoch 69: train loss: 0.029003294184803963\n",
      "Epoch 70: train loss: 0.02681884355843067\n",
      "Epoch 71: train loss: 0.025384942069649696\n",
      "Epoch 72: train loss: 0.024751629680395126\n",
      "Epoch 73: train loss: 0.024901919066905975\n",
      "Epoch 74: train loss: 0.027164602652192116\n",
      "Epoch 75: train loss: 0.03272278234362602\n",
      "Epoch 76: train loss: 0.029419275000691414\n",
      "Epoch 77: train loss: 0.02680966630578041\n",
      "Epoch 78: train loss: 0.030869973823428154\n",
      "Epoch 79: train loss: 0.02499799057841301\n",
      "Epoch 80: train loss: 0.028363868594169617\n",
      "Epoch 81: train loss: 0.023669669404625893\n",
      "Epoch 82: train loss: 0.025777656584978104\n",
      "Epoch 83: train loss: 0.022290993481874466\n",
      "Epoch 84: train loss: 0.02392001636326313\n",
      "Epoch 85: train loss: 0.02154717780649662\n",
      "Epoch 86: train loss: 0.023487679660320282\n",
      "Epoch 87: train loss: 0.02164163626730442\n",
      "Epoch 88: train loss: 0.023349639028310776\n",
      "Epoch 89: train loss: 0.021993912756443024\n",
      "Epoch 90: train loss: 0.022893602028489113\n",
      "Epoch 91: train loss: 0.02197003923356533\n",
      "Epoch 92: train loss: 0.021959343925118446\n",
      "Epoch 93: train loss: 0.021658360958099365\n",
      "Epoch 94: train loss: 0.021060317754745483\n",
      "Epoch 95: train loss: 0.02126002125442028\n",
      "Epoch 96: train loss: 0.020488929003477097\n",
      "Epoch 97: train loss: 0.020931608974933624\n",
      "Epoch 98: train loss: 0.020239822566509247\n",
      "Epoch 99: train loss: 0.020685695111751556\n",
      "Epoch 100: train loss: 0.02016230672597885\n",
      "Epoch 101: train loss: 0.020446714013814926\n",
      "Epoch 102: train loss: 0.020127348601818085\n",
      "Epoch 103: train loss: 0.02020522952079773\n",
      "Epoch 104: train loss: 0.020061776041984558\n",
      "Epoch 105: train loss: 0.019944030791521072\n",
      "Epoch 106: train loss: 0.01992679201066494\n",
      "Epoch 107: train loss: 0.019698582589626312\n",
      "Epoch 108: train loss: 0.019763197749853134\n",
      "Epoch 109: train loss: 0.01950301229953766\n",
      "Epoch 110: train loss: 0.0195870790630579\n",
      "Epoch 111: train loss: 0.019380968064069748\n",
      "Epoch 112: train loss: 0.019430872052907944\n",
      "Epoch 113: train loss: 0.01930297538638115\n",
      "Epoch 114: train loss: 0.019289081916213036\n",
      "Epoch 115: train loss: 0.019240831956267357\n",
      "Epoch 116: train loss: 0.019163863733410835\n",
      "Epoch 117: train loss: 0.019162269309163094\n",
      "Epoch 118: train loss: 0.019054584205150604\n",
      "Epoch 119: train loss: 0.01906508207321167\n",
      "Epoch 120: train loss: 0.018962755799293518\n",
      "Epoch 121: train loss: 0.018962854519486427\n",
      "Epoch 122: train loss: 0.01888844184577465\n",
      "Epoch 123: train loss: 0.018863046541810036\n",
      "Epoch 124: train loss: 0.01881837286055088\n",
      "Epoch 125: train loss: 0.018768275156617165\n",
      "Epoch 126: train loss: 0.01874011941254139\n",
      "Epoch 127: train loss: 0.018674254417419434\n",
      "Epoch 128: train loss: 0.018653227016329765\n",
      "Epoch 129: train loss: 0.01858987659215927\n",
      "Epoch 130: train loss: 0.018570376560091972\n",
      "Epoch 131: train loss: 0.018520014360547066\n",
      "Epoch 132: train loss: 0.0184968002140522\n",
      "Epoch 133: train loss: 0.01845807582139969\n",
      "Epoch 134: train loss: 0.01842574216425419\n",
      "Epoch 135: train loss: 0.018393218517303467\n",
      "Epoch 136: train loss: 0.018351921811699867\n",
      "Epoch 137: train loss: 0.018321748822927475\n",
      "Epoch 138: train loss: 0.018277177587151527\n",
      "Epoch 139: train loss: 0.018247902393341064\n",
      "Epoch 140: train loss: 0.01820451021194458\n",
      "Epoch 141: train loss: 0.0181744284927845\n",
      "Epoch 142: train loss: 0.01813429780304432\n",
      "Epoch 143: train loss: 0.01810169219970703\n",
      "Epoch 144: train loss: 0.01806468516588211\n",
      "Epoch 145: train loss: 0.018029222264885902\n",
      "Epoch 146: train loss: 0.01799430325627327\n",
      "Epoch 147: train loss: 0.017956918105483055\n",
      "Epoch 148: train loss: 0.017923368141055107\n",
      "Epoch 149: train loss: 0.017885232344269753\n",
      "Epoch 150: train loss: 0.01785195618867874\n",
      "Epoch 151: train loss: 0.017814086750149727\n",
      "Epoch 152: train loss: 0.017780112102627754\n",
      "Epoch 153: train loss: 0.01774267666041851\n",
      "Epoch 154: train loss: 0.017707470804452896\n",
      "Epoch 155: train loss: 0.01767059415578842\n",
      "Epoch 156: train loss: 0.017634317278862\n",
      "Epoch 157: train loss: 0.01759800687432289\n",
      "Epoch 158: train loss: 0.017560983076691628\n",
      "Epoch 159: train loss: 0.017524966970086098\n",
      "Epoch 160: train loss: 0.01748759113252163\n",
      "Epoch 161: train loss: 0.017451569437980652\n",
      "Epoch 162: train loss: 0.017414042726159096\n",
      "Epoch 163: train loss: 0.017377682030200958\n",
      "Epoch 164: train loss: 0.017339888960123062\n",
      "Epoch 165: train loss: 0.0173027403652668\n",
      "Epoch 166: train loss: 0.01726456545293331\n",
      "Epoch 167: train loss: 0.0172265712171793\n",
      "Epoch 168: train loss: 0.01718798466026783\n",
      "Epoch 169: train loss: 0.017149213701486588\n",
      "Epoch 170: train loss: 0.01711016707122326\n",
      "Epoch 171: train loss: 0.017070621252059937\n",
      "Epoch 172: train loss: 0.017030883580446243\n",
      "Epoch 173: train loss: 0.01699034310877323\n",
      "Epoch 174: train loss: 0.01694958284497261\n",
      "Epoch 175: train loss: 0.01690787263214588\n",
      "Epoch 176: train loss: 0.016865810379385948\n",
      "Epoch 177: train loss: 0.016822652891278267\n",
      "Epoch 178: train loss: 0.01677893102169037\n",
      "Epoch 179: train loss: 0.016734031960368156\n",
      "Epoch 180: train loss: 0.016688339412212372\n",
      "Epoch 181: train loss: 0.016641369089484215\n",
      "Epoch 182: train loss: 0.01659349352121353\n",
      "Epoch 183: train loss: 0.0165444053709507\n",
      "Epoch 184: train loss: 0.0164945125579834\n",
      "Epoch 185: train loss: 0.016443762928247452\n",
      "Epoch 186: train loss: 0.01639263890683651\n",
      "Epoch 187: train loss: 0.016341157257556915\n",
      "Epoch 188: train loss: 0.01628979668021202\n",
      "Epoch 189: train loss: 0.016238603740930557\n",
      "Epoch 190: train loss: 0.016187800094485283\n",
      "Epoch 191: train loss: 0.016137124970555305\n",
      "Epoch 192: train loss: 0.016086675226688385\n",
      "Epoch 193: train loss: 0.016036199405789375\n",
      "Epoch 194: train loss: 0.01598574034869671\n",
      "Epoch 195: train loss: 0.015935149043798447\n",
      "Epoch 196: train loss: 0.015884479507803917\n",
      "Epoch 197: train loss: 0.015833605080842972\n",
      "Epoch 198: train loss: 0.015782425180077553\n",
      "Epoch 199: train loss: 0.015730807557702065\n",
      "Epoch 200: train loss: 0.01567862741649151\n",
      "Epoch 201: train loss: 0.015625881031155586\n",
      "Epoch 202: train loss: 0.015572542324662209\n",
      "Epoch 203: train loss: 0.015518737025558949\n",
      "Epoch 204: train loss: 0.015464505180716515\n",
      "Epoch 205: train loss: 0.015409925021231174\n",
      "Epoch 206: train loss: 0.015354982577264309\n",
      "Epoch 207: train loss: 0.015299689024686813\n",
      "Epoch 208: train loss: 0.015244017355144024\n",
      "Epoch 209: train loss: 0.015187995508313179\n",
      "Epoch 210: train loss: 0.015131657011806965\n",
      "Epoch 211: train loss: 0.015075036324560642\n",
      "Epoch 212: train loss: 0.015018182806670666\n",
      "Epoch 213: train loss: 0.014961103908717632\n",
      "Epoch 214: train loss: 0.014903808943927288\n",
      "Epoch 215: train loss: 0.01484627090394497\n",
      "Epoch 216: train loss: 0.014788486063480377\n",
      "Epoch 217: train loss: 0.014730440452694893\n",
      "Epoch 218: train loss: 0.014672120101749897\n",
      "Epoch 219: train loss: 0.014613522216677666\n",
      "Epoch 220: train loss: 0.014554638415575027\n",
      "Epoch 221: train loss: 0.014495464973151684\n",
      "Epoch 222: train loss: 0.014435980468988419\n",
      "Epoch 223: train loss: 0.014376195147633553\n",
      "Epoch 224: train loss: 0.014316107146441936\n",
      "Epoch 225: train loss: 0.014255737885832787\n",
      "Epoch 226: train loss: 0.01419511716812849\n",
      "Epoch 227: train loss: 0.014134270139038563\n",
      "Epoch 228: train loss: 0.01407322846353054\n",
      "Epoch 229: train loss: 0.014012016355991364\n",
      "Epoch 230: train loss: 0.013950656168162823\n",
      "Epoch 231: train loss: 0.013889155350625515\n",
      "Epoch 232: train loss: 0.01382752787321806\n",
      "Epoch 233: train loss: 0.013765775598585606\n",
      "Epoch 234: train loss: 0.013703910633921623\n",
      "Epoch 235: train loss: 0.01364193670451641\n",
      "Epoch 236: train loss: 0.013579867780208588\n",
      "Epoch 237: train loss: 0.013517724350094795\n",
      "Epoch 238: train loss: 0.013455523177981377\n",
      "Epoch 239: train loss: 0.013393300585448742\n",
      "Epoch 240: train loss: 0.013331066817045212\n",
      "Epoch 241: train loss: 0.013268866576254368\n",
      "Epoch 242: train loss: 0.013206712901592255\n",
      "Epoch 243: train loss: 0.013144644908607006\n",
      "Epoch 244: train loss: 0.01308267842978239\n",
      "Epoch 245: train loss: 0.013020842336118221\n",
      "Epoch 246: train loss: 0.012959158048033714\n",
      "Epoch 247: train loss: 0.012897648848593235\n",
      "Epoch 248: train loss: 0.012836341746151447\n",
      "Epoch 249: train loss: 0.012775261886417866\n",
      "Epoch 250: train loss: 0.012714443728327751\n",
      "Epoch 251: train loss: 0.01265390869230032\n",
      "Epoch 252: train loss: 0.012593697756528854\n",
      "Epoch 253: train loss: 0.012533843517303467\n",
      "Epoch 254: train loss: 0.012474372051656246\n",
      "Epoch 255: train loss: 0.012415318749845028\n",
      "Epoch 256: train loss: 0.012356707826256752\n",
      "Epoch 257: train loss: 0.012298569083213806\n",
      "Epoch 258: train loss: 0.012240923009812832\n",
      "Epoch 259: train loss: 0.012183795683085918\n",
      "Epoch 260: train loss: 0.012127211317420006\n",
      "Epoch 261: train loss: 0.012071186676621437\n",
      "Epoch 262: train loss: 0.012015735730528831\n",
      "Epoch 263: train loss: 0.011960877105593681\n",
      "Epoch 264: train loss: 0.011906623840332031\n",
      "Epoch 265: train loss: 0.011852983385324478\n",
      "Epoch 266: train loss: 0.011799967847764492\n",
      "Epoch 267: train loss: 0.01174757070839405\n",
      "Epoch 268: train loss: 0.011695807799696922\n",
      "Epoch 269: train loss: 0.011644674465060234\n",
      "Epoch 270: train loss: 0.011594166979193687\n",
      "Epoch 271: train loss: 0.01154429093003273\n",
      "Epoch 272: train loss: 0.01149505004286766\n",
      "Epoch 273: train loss: 0.011446435004472733\n",
      "Epoch 274: train loss: 0.011398453265428543\n",
      "Epoch 275: train loss: 0.011351104825735092\n",
      "Epoch 276: train loss: 0.011304391548037529\n",
      "Epoch 277: train loss: 0.011258319951593876\n",
      "Epoch 278: train loss: 0.01121288537979126\n",
      "Epoch 279: train loss: 0.011168102733790874\n",
      "Epoch 280: train loss: 0.011123964563012123\n",
      "Epoch 281: train loss: 0.011080479249358177\n",
      "Epoch 282: train loss: 0.011037648655474186\n",
      "Epoch 283: train loss: 0.010995475575327873\n",
      "Epoch 284: train loss: 0.010953964665532112\n",
      "Epoch 285: train loss: 0.01091310940682888\n",
      "Epoch 286: train loss: 0.010872922837734222\n",
      "Epoch 287: train loss: 0.010833400301635265\n",
      "Epoch 288: train loss: 0.010794537141919136\n",
      "Epoch 289: train loss: 0.01075634267181158\n",
      "Epoch 290: train loss: 0.010718810372054577\n",
      "Epoch 291: train loss: 0.010681935586035252\n",
      "Epoch 292: train loss: 0.010645718313753605\n",
      "Epoch 293: train loss: 0.010610157623887062\n",
      "Epoch 294: train loss: 0.0105752469971776\n",
      "Epoch 295: train loss: 0.010540984570980072\n",
      "Epoch 296: train loss: 0.010507366620004177\n",
      "Epoch 297: train loss: 0.01047438196837902\n",
      "Epoch 298: train loss: 0.010442032478749752\n",
      "Epoch 299: train loss: 0.010410308837890625\n",
      "Epoch 300: train loss: 0.010379220359027386\n",
      "Epoch 301: train loss: 0.010348791256546974\n",
      "Epoch 302: train loss: 0.010319164954125881\n",
      "Epoch 303: train loss: 0.010290959849953651\n",
      "Epoch 304: train loss: 0.010266924276947975\n",
      "Epoch 305: train loss: 0.010258203372359276\n",
      "Epoch 306: train loss: 0.010305595584213734\n",
      "Epoch 307: train loss: 0.010397006757557392\n",
      "Epoch 308: train loss: 0.010394670069217682\n",
      "Epoch 309: train loss: 0.010157628916203976\n",
      "Epoch 310: train loss: 0.010208643972873688\n",
      "Epoch 311: train loss: 0.010275937616825104\n",
      "Epoch 312: train loss: 0.01012306846678257\n",
      "Epoch 313: train loss: 0.010129539296030998\n",
      "Epoch 314: train loss: 0.010148832574486732\n",
      "Epoch 315: train loss: 0.010087525472044945\n",
      "Epoch 316: train loss: 0.01003911904990673\n",
      "Epoch 317: train loss: 0.01005375012755394\n",
      "Epoch 318: train loss: 0.010033641941845417\n",
      "Epoch 319: train loss: 0.009944845922291279\n",
      "Epoch 320: train loss: 0.00999049935489893\n",
      "Epoch 321: train loss: 0.009955063462257385\n",
      "Epoch 322: train loss: 0.009878522716462612\n",
      "Epoch 323: train loss: 0.009930718690156937\n",
      "Epoch 324: train loss: 0.009875024668872356\n",
      "Epoch 325: train loss: 0.00983506627380848\n",
      "Epoch 326: train loss: 0.009861767292022705\n",
      "Epoch 327: train loss: 0.009811955504119396\n",
      "Epoch 328: train loss: 0.009795173071324825\n",
      "Epoch 329: train loss: 0.00979205034673214\n",
      "Epoch 330: train loss: 0.009764770977199078\n",
      "Epoch 331: train loss: 0.009750971570611\n",
      "Epoch 332: train loss: 0.009730895049870014\n",
      "Epoch 333: train loss: 0.009724018163979053\n",
      "Epoch 334: train loss: 0.009704429656267166\n",
      "Epoch 335: train loss: 0.009680256247520447\n",
      "Epoch 336: train loss: 0.00968316849321127\n",
      "Epoch 337: train loss: 0.00965930987149477\n",
      "Epoch 338: train loss: 0.009637484326958656\n",
      "Epoch 339: train loss: 0.009640798904001713\n",
      "Epoch 340: train loss: 0.009617537260055542\n",
      "Epoch 341: train loss: 0.009599394164979458\n",
      "Epoch 342: train loss: 0.009598263539373875\n",
      "Epoch 343: train loss: 0.009579038247466087\n",
      "Epoch 344: train loss: 0.009563973173499107\n",
      "Epoch 345: train loss: 0.009557310491800308\n",
      "Epoch 346: train loss: 0.009542813524603844\n",
      "Epoch 347: train loss: 0.009530281648039818\n",
      "Epoch 348: train loss: 0.009519105777144432\n",
      "Epoch 349: train loss: 0.009507945738732815\n",
      "Epoch 350: train loss: 0.009497767314314842\n",
      "Epoch 351: train loss: 0.009484064765274525\n",
      "Epoch 352: train loss: 0.009474127553403378\n",
      "Epoch 353: train loss: 0.009465940296649933\n",
      "Epoch 354: train loss: 0.009451966732740402\n",
      "Epoch 355: train loss: 0.009441694244742393\n",
      "Epoch 356: train loss: 0.009434484876692295\n",
      "Epoch 357: train loss: 0.009422002360224724\n",
      "Epoch 358: train loss: 0.009411175735294819\n",
      "Epoch 359: train loss: 0.009403624571859837\n",
      "Epoch 360: train loss: 0.009393072687089443\n",
      "Epoch 361: train loss: 0.009382589720189571\n",
      "Epoch 362: train loss: 0.009374084882438183\n",
      "Epoch 363: train loss: 0.009364565834403038\n",
      "Epoch 364: train loss: 0.009355193004012108\n",
      "Epoch 365: train loss: 0.009346375241875648\n",
      "Epoch 366: train loss: 0.009336864575743675\n",
      "Epoch 367: train loss: 0.009328181855380535\n",
      "Epoch 368: train loss: 0.009319981560111046\n",
      "Epoch 369: train loss: 0.009310612455010414\n",
      "Epoch 370: train loss: 0.009301763027906418\n",
      "Epoch 371: train loss: 0.009294004179537296\n",
      "Epoch 372: train loss: 0.009285484440624714\n",
      "Epoch 373: train loss: 0.009276628494262695\n",
      "Epoch 374: train loss: 0.00926859863102436\n",
      "Epoch 375: train loss: 0.009260660968720913\n",
      "Epoch 376: train loss: 0.009252420626580715\n",
      "Epoch 377: train loss: 0.009244381450116634\n",
      "Epoch 378: train loss: 0.00923642423003912\n",
      "Epoch 379: train loss: 0.009228471666574478\n",
      "Epoch 380: train loss: 0.009220818057656288\n",
      "Epoch 381: train loss: 0.009213162586092949\n",
      "Epoch 382: train loss: 0.009205274283885956\n",
      "Epoch 383: train loss: 0.009197596460580826\n",
      "Epoch 384: train loss: 0.009190226905047894\n",
      "Epoch 385: train loss: 0.009182757697999477\n",
      "Epoch 386: train loss: 0.009175203740596771\n",
      "Epoch 387: train loss: 0.009167812764644623\n",
      "Epoch 388: train loss: 0.00916049350053072\n",
      "Epoch 389: train loss: 0.009153173305094242\n",
      "Epoch 390: train loss: 0.009145974181592464\n",
      "Epoch 391: train loss: 0.009138836525380611\n",
      "Epoch 392: train loss: 0.00913163647055626\n",
      "Epoch 393: train loss: 0.00912446342408657\n",
      "Epoch 394: train loss: 0.009117422625422478\n",
      "Epoch 395: train loss: 0.009110437706112862\n",
      "Epoch 396: train loss: 0.009103444404900074\n",
      "Epoch 397: train loss: 0.009096493013203144\n",
      "Epoch 398: train loss: 0.009089588187634945\n",
      "Epoch 399: train loss: 0.009082687087357044\n",
      "Epoch 400: train loss: 0.009075800888240337\n",
      "Epoch 401: train loss: 0.009068991988897324\n",
      "Epoch 402: train loss: 0.009062224067747593\n",
      "Epoch 403: train loss: 0.009055464528501034\n",
      "Epoch 404: train loss: 0.009048725478351116\n",
      "Epoch 405: train loss: 0.009042029269039631\n",
      "Epoch 406: train loss: 0.00903534609824419\n",
      "Epoch 407: train loss: 0.009028671309351921\n",
      "Epoch 408: train loss: 0.009022019803524017\n",
      "Epoch 409: train loss: 0.009015406481921673\n",
      "Epoch 410: train loss: 0.009008800610899925\n",
      "Epoch 411: train loss: 0.009002205915749073\n",
      "Epoch 412: train loss: 0.008995641954243183\n",
      "Epoch 413: train loss: 0.00898909941315651\n",
      "Epoch 414: train loss: 0.008982568047940731\n",
      "Epoch 415: train loss: 0.008976043201982975\n",
      "Epoch 416: train loss: 0.008969535119831562\n",
      "Epoch 417: train loss: 0.008963048458099365\n",
      "Epoch 418: train loss: 0.00895657017827034\n",
      "Epoch 419: train loss: 0.008950095623731613\n",
      "Epoch 420: train loss: 0.008943642489612103\n",
      "Epoch 421: train loss: 0.008937204256653786\n",
      "Epoch 422: train loss: 0.008930781856179237\n",
      "Epoch 423: train loss: 0.008924396708607674\n",
      "Epoch 424: train loss: 0.00891808420419693\n",
      "Epoch 425: train loss: 0.008911947719752789\n",
      "Epoch 426: train loss: 0.008906223811209202\n",
      "Epoch 427: train loss: 0.008901507593691349\n",
      "Epoch 428: train loss: 0.008899537846446037\n",
      "Epoch 429: train loss: 0.00890410877764225\n",
      "Epoch 430: train loss: 0.008927304297685623\n",
      "Epoch 431: train loss: 0.008982428349554539\n",
      "Epoch 432: train loss: 0.00910888984799385\n",
      "Epoch 433: train loss: 0.009160622023046017\n",
      "Epoch 434: train loss: 0.009126120246946812\n",
      "Epoch 435: train loss: 0.008928736671805382\n",
      "Epoch 436: train loss: 0.008876797743141651\n",
      "Epoch 437: train loss: 0.008956622332334518\n",
      "Epoch 438: train loss: 0.008964745327830315\n",
      "Epoch 439: train loss: 0.008921466767787933\n",
      "Epoch 440: train loss: 0.008890067227184772\n",
      "Epoch 441: train loss: 0.008873009122908115\n",
      "Epoch 442: train loss: 0.008857198059558868\n",
      "Epoch 443: train loss: 0.00886128842830658\n",
      "Epoch 444: train loss: 0.008879661560058594\n",
      "Epoch 445: train loss: 0.008851333521306515\n",
      "Epoch 446: train loss: 0.008798214606940746\n",
      "Epoch 447: train loss: 0.008802307769656181\n",
      "Epoch 448: train loss: 0.008837016299366951\n",
      "Epoch 449: train loss: 0.008821651339530945\n",
      "Epoch 450: train loss: 0.008774759247899055\n",
      "Epoch 451: train loss: 0.008764746598899364\n",
      "Epoch 452: train loss: 0.008786299265921116\n",
      "Epoch 453: train loss: 0.008784707635641098\n",
      "Epoch 454: train loss: 0.008756019175052643\n",
      "Epoch 455: train loss: 0.008742089383304119\n",
      "Epoch 456: train loss: 0.008748985826969147\n",
      "Epoch 457: train loss: 0.008746592327952385\n",
      "Epoch 458: train loss: 0.008729983121156693\n",
      "Epoch 459: train loss: 0.008719701319932938\n",
      "Epoch 460: train loss: 0.00872108805924654\n",
      "Epoch 461: train loss: 0.008718308992683887\n",
      "Epoch 462: train loss: 0.008703752420842648\n",
      "Epoch 463: train loss: 0.008692419156432152\n",
      "Epoch 464: train loss: 0.008692067116498947\n",
      "Epoch 465: train loss: 0.008691920898854733\n",
      "Epoch 466: train loss: 0.00868313480168581\n",
      "Epoch 467: train loss: 0.008670046925544739\n",
      "Epoch 468: train loss: 0.008663172833621502\n",
      "Epoch 469: train loss: 0.008661615662276745\n",
      "Epoch 470: train loss: 0.00865758303552866\n",
      "Epoch 471: train loss: 0.008649404160678387\n",
      "Epoch 472: train loss: 0.00864123273640871\n",
      "Epoch 473: train loss: 0.008635999634861946\n",
      "Epoch 474: train loss: 0.008631094358861446\n",
      "Epoch 475: train loss: 0.008624020032584667\n",
      "Epoch 476: train loss: 0.008616355247795582\n",
      "Epoch 477: train loss: 0.008610674180090427\n",
      "Epoch 478: train loss: 0.008606547489762306\n",
      "Epoch 479: train loss: 0.008601401001214981\n",
      "Epoch 480: train loss: 0.008594236336648464\n",
      "Epoch 481: train loss: 0.008586682379245758\n",
      "Epoch 482: train loss: 0.008580398745834827\n",
      "Epoch 483: train loss: 0.008575046434998512\n",
      "Epoch 484: train loss: 0.008569257333874702\n",
      "Epoch 485: train loss: 0.008562549017369747\n",
      "Epoch 486: train loss: 0.008555807173252106\n",
      "Epoch 487: train loss: 0.008549838326871395\n",
      "Epoch 488: train loss: 0.008544431999325752\n",
      "Epoch 489: train loss: 0.008538831025362015\n",
      "Epoch 490: train loss: 0.008532688021659851\n",
      "Epoch 491: train loss: 0.008526491932570934\n",
      "Epoch 492: train loss: 0.008520765230059624\n",
      "Epoch 493: train loss: 0.008515718393027782\n",
      "Epoch 494: train loss: 0.008511107414960861\n",
      "Epoch 495: train loss: 0.008507221937179565\n",
      "Epoch 496: train loss: 0.008504748344421387\n",
      "Epoch 497: train loss: 0.008506222628057003\n",
      "Epoch 498: train loss: 0.00851279404014349\n",
      "Epoch 499: train loss: 0.008533214218914509\n",
      "Epoch 500: train loss: 0.008561454713344574\n",
      "Epoch 501: train loss: 0.008622018620371819\n",
      "Epoch 502: train loss: 0.008649582043290138\n",
      "Epoch 503: train loss: 0.008685006760060787\n",
      "Epoch 504: train loss: 0.008610700257122517\n",
      "Epoch 505: train loss: 0.008532680571079254\n",
      "Epoch 506: train loss: 0.00846728403121233\n",
      "Epoch 507: train loss: 0.008445278741419315\n",
      "Epoch 508: train loss: 0.008459778502583504\n",
      "Epoch 509: train loss: 0.008481486700475216\n",
      "Epoch 510: train loss: 0.008500240743160248\n",
      "Epoch 511: train loss: 0.008489654399454594\n",
      "Epoch 512: train loss: 0.008459735661745071\n",
      "Epoch 513: train loss: 0.008422041311860085\n",
      "Epoch 514: train loss: 0.008394134230911732\n",
      "Epoch 515: train loss: 0.008388778194785118\n",
      "Epoch 516: train loss: 0.008401487022638321\n",
      "Epoch 517: train loss: 0.00841593835502863\n",
      "Epoch 518: train loss: 0.008414705283939838\n",
      "Epoch 519: train loss: 0.008394576609134674\n",
      "Epoch 520: train loss: 0.00836698617786169\n",
      "Epoch 521: train loss: 0.008346837013959885\n",
      "Epoch 522: train loss: 0.008340113796293736\n",
      "Epoch 523: train loss: 0.008341886103153229\n",
      "Epoch 524: train loss: 0.008342945016920567\n",
      "Epoch 525: train loss: 0.008338718675076962\n",
      "Epoch 526: train loss: 0.008330827578902245\n",
      "Epoch 527: train loss: 0.00832292065024376\n",
      "Epoch 528: train loss: 0.008318347856402397\n",
      "Epoch 529: train loss: 0.008312338031828403\n",
      "Epoch 530: train loss: 0.008305348455905914\n",
      "Epoch 531: train loss: 0.008293461054563522\n",
      "Epoch 532: train loss: 0.008281216025352478\n",
      "Epoch 533: train loss: 0.008270295336842537\n",
      "Epoch 534: train loss: 0.008262591436505318\n",
      "Epoch 535: train loss: 0.008257167413830757\n",
      "Epoch 536: train loss: 0.008252142928540707\n",
      "Epoch 537: train loss: 0.008246286772191525\n",
      "Epoch 538: train loss: 0.008239589631557465\n",
      "Epoch 539: train loss: 0.00823329109698534\n",
      "Epoch 540: train loss: 0.008228243328630924\n",
      "Epoch 541: train loss: 0.00822545401751995\n",
      "Epoch 542: train loss: 0.008224479854106903\n",
      "Epoch 543: train loss: 0.008227025158703327\n",
      "Epoch 544: train loss: 0.00823246594518423\n",
      "Epoch 545: train loss: 0.008248892612755299\n",
      "Epoch 546: train loss: 0.008269045501947403\n",
      "Epoch 547: train loss: 0.008317016065120697\n",
      "Epoch 548: train loss: 0.008336219936609268\n",
      "Epoch 549: train loss: 0.008379485458135605\n",
      "Epoch 550: train loss: 0.008316864259541035\n",
      "Epoch 551: train loss: 0.008260758593678474\n",
      "Epoch 552: train loss: 0.00818469189107418\n",
      "Epoch 553: train loss: 0.008148157969117165\n",
      "Epoch 554: train loss: 0.008149874396622181\n",
      "Epoch 555: train loss: 0.008171181194484234\n",
      "Epoch 556: train loss: 0.008198201656341553\n",
      "Epoch 557: train loss: 0.00819034781306982\n",
      "Epoch 558: train loss: 0.008175061084330082\n",
      "Epoch 559: train loss: 0.008141125552356243\n",
      "Epoch 560: train loss: 0.00811730232089758\n",
      "Epoch 561: train loss: 0.008106186054646969\n",
      "Epoch 562: train loss: 0.008104316890239716\n",
      "Epoch 563: train loss: 0.008106417953968048\n",
      "Epoch 564: train loss: 0.00810234621167183\n",
      "Epoch 565: train loss: 0.008097754791378975\n",
      "Epoch 566: train loss: 0.008084570057690144\n",
      "Epoch 567: train loss: 0.008073370903730392\n",
      "Epoch 568: train loss: 0.00806182436645031\n",
      "Epoch 569: train loss: 0.008052305318415165\n",
      "Epoch 570: train loss: 0.008044006302952766\n",
      "Epoch 571: train loss: 0.00803598016500473\n",
      "Epoch 572: train loss: 0.008028498850762844\n",
      "Epoch 573: train loss: 0.008021264337003231\n",
      "Epoch 574: train loss: 0.008016054518520832\n",
      "Epoch 575: train loss: 0.00801108404994011\n",
      "Epoch 576: train loss: 0.00800851546227932\n",
      "Epoch 577: train loss: 0.00800475012511015\n",
      "Epoch 578: train loss: 0.008002141490578651\n",
      "Epoch 579: train loss: 0.007996944710612297\n",
      "Epoch 580: train loss: 0.007992193102836609\n",
      "Epoch 581: train loss: 0.00798474159091711\n",
      "Epoch 582: train loss: 0.007978747598826885\n",
      "Epoch 583: train loss: 0.007970845326781273\n",
      "Epoch 584: train loss: 0.007966563105583191\n",
      "Epoch 585: train loss: 0.007960645481944084\n",
      "Epoch 586: train loss: 0.007961004972457886\n",
      "Epoch 587: train loss: 0.007958120666444302\n",
      "Epoch 588: train loss: 0.007964691147208214\n",
      "Epoch 589: train loss: 0.007962941192090511\n",
      "Epoch 590: train loss: 0.007973804138600826\n",
      "Epoch 591: train loss: 0.007966735400259495\n",
      "Epoch 592: train loss: 0.007973052561283112\n",
      "Epoch 593: train loss: 0.007953370921313763\n",
      "Epoch 594: train loss: 0.007944800890982151\n",
      "Epoch 595: train loss: 0.007916544564068317\n",
      "Epoch 596: train loss: 0.007897131145000458\n",
      "Epoch 597: train loss: 0.007873304188251495\n",
      "Epoch 598: train loss: 0.00785617996007204\n",
      "Epoch 599: train loss: 0.00784207135438919\n",
      "Epoch 600: train loss: 0.007831981405615807\n",
      "Epoch 601: train loss: 0.00782456248998642\n",
      "Epoch 602: train loss: 0.007819104939699173\n",
      "Epoch 603: train loss: 0.007815060205757618\n",
      "Epoch 604: train loss: 0.007812127470970154\n",
      "Epoch 605: train loss: 0.007811245042830706\n",
      "Epoch 606: train loss: 0.0078114103525877\n",
      "Epoch 607: train loss: 0.007817190140485764\n",
      "Epoch 608: train loss: 0.007823217660188675\n",
      "Epoch 609: train loss: 0.007843240164220333\n",
      "Epoch 610: train loss: 0.007854816503822803\n",
      "Epoch 611: train loss: 0.007892037741839886\n",
      "Epoch 612: train loss: 0.00788639672100544\n",
      "Epoch 613: train loss: 0.007906951010227203\n",
      "Epoch 614: train loss: 0.007852842099964619\n",
      "Epoch 615: train loss: 0.007821097038686275\n",
      "Epoch 616: train loss: 0.007767692673951387\n",
      "Epoch 617: train loss: 0.007743169087916613\n",
      "Epoch 618: train loss: 0.00774086220189929\n",
      "Epoch 619: train loss: 0.00776233896613121\n",
      "Epoch 620: train loss: 0.007799084763973951\n",
      "Epoch 621: train loss: 0.007846713997423649\n",
      "Epoch 622: train loss: 0.007892344146966934\n",
      "Epoch 623: train loss: 0.007934178225696087\n",
      "Epoch 624: train loss: 0.007950423285365105\n",
      "Epoch 625: train loss: 0.007951722480356693\n",
      "Epoch 626: train loss: 0.007917038165032864\n",
      "Epoch 627: train loss: 0.007885551080107689\n",
      "Epoch 628: train loss: 0.007855196483433247\n",
      "Epoch 629: train loss: 0.007892433553934097\n",
      "Epoch 630: train loss: 0.00787301454693079\n",
      "Epoch 631: train loss: 0.007910504005849361\n",
      "Epoch 632: train loss: 0.007759038358926773\n",
      "Epoch 633: train loss: 0.007654936518520117\n",
      "Epoch 634: train loss: 0.007587574888020754\n",
      "Epoch 635: train loss: 0.0075944880954921246\n",
      "Epoch 636: train loss: 0.007653784938156605\n",
      "Epoch 637: train loss: 0.007694844622164965\n",
      "Epoch 638: train loss: 0.007731057237833738\n",
      "Epoch 639: train loss: 0.007668324280530214\n",
      "Epoch 640: train loss: 0.007621734403073788\n",
      "Epoch 641: train loss: 0.007584035396575928\n",
      "Epoch 642: train loss: 0.007573029026389122\n",
      "Epoch 643: train loss: 0.007569887675344944\n",
      "Epoch 644: train loss: 0.007558818440884352\n",
      "Epoch 645: train loss: 0.007542858365923166\n",
      "Epoch 646: train loss: 0.007524390239268541\n",
      "Epoch 647: train loss: 0.007520956918597221\n",
      "Epoch 648: train loss: 0.007527598645538092\n",
      "Epoch 649: train loss: 0.007537176366895437\n",
      "Epoch 650: train loss: 0.007535642944276333\n",
      "Epoch 651: train loss: 0.007520684972405434\n",
      "Epoch 652: train loss: 0.007494545076042414\n",
      "Epoch 653: train loss: 0.00746906828135252\n",
      "Epoch 654: train loss: 0.007451457902789116\n",
      "Epoch 655: train loss: 0.007444384507834911\n",
      "Epoch 656: train loss: 0.007441933732479811\n",
      "Epoch 657: train loss: 0.007439582142978907\n",
      "Epoch 658: train loss: 0.00743069825693965\n",
      "Epoch 659: train loss: 0.00741777615621686\n",
      "Epoch 660: train loss: 0.0074026030488312244\n",
      "Epoch 661: train loss: 0.00738981319591403\n",
      "Epoch 662: train loss: 0.007381025236099958\n",
      "Epoch 663: train loss: 0.007375651970505714\n",
      "Epoch 664: train loss: 0.007371414918452501\n",
      "Epoch 665: train loss: 0.007366028614342213\n",
      "Epoch 666: train loss: 0.007358359172940254\n",
      "Epoch 667: train loss: 0.0073486617766320705\n",
      "Epoch 668: train loss: 0.007338201627135277\n",
      "Epoch 669: train loss: 0.007328283041715622\n",
      "Epoch 670: train loss: 0.0073197949677705765\n",
      "Epoch 671: train loss: 0.00731288967654109\n",
      "Epoch 672: train loss: 0.0073072221130132675\n",
      "Epoch 673: train loss: 0.0073023405857384205\n",
      "Epoch 674: train loss: 0.007298476528376341\n",
      "Epoch 675: train loss: 0.007296684663742781\n",
      "Epoch 676: train loss: 0.007301111705601215\n",
      "Epoch 677: train loss: 0.007316740695387125\n",
      "Epoch 678: train loss: 0.00736570730805397\n",
      "Epoch 679: train loss: 0.007447437383234501\n",
      "Epoch 680: train loss: 0.007664947770535946\n",
      "Epoch 681: train loss: 0.007795010693371296\n",
      "Epoch 682: train loss: 0.008151783607900143\n",
      "Epoch 683: train loss: 0.007949620485305786\n",
      "Epoch 684: train loss: 0.007951202802360058\n",
      "Epoch 685: train loss: 0.00784650444984436\n",
      "Epoch 686: train loss: 0.007728732656687498\n",
      "Epoch 687: train loss: 0.007487877272069454\n",
      "Epoch 688: train loss: 0.007269089110195637\n",
      "Epoch 689: train loss: 0.007204323075711727\n",
      "Epoch 690: train loss: 0.007313234265893698\n",
      "Epoch 691: train loss: 0.007480524946004152\n",
      "Epoch 692: train loss: 0.0075249457731842995\n",
      "Epoch 693: train loss: 0.007441557943820953\n",
      "Epoch 694: train loss: 0.007290668319910765\n",
      "Epoch 695: train loss: 0.007234117016196251\n",
      "Epoch 696: train loss: 0.007237786892801523\n",
      "Epoch 697: train loss: 0.007252444047480822\n",
      "Epoch 698: train loss: 0.007200559135526419\n",
      "Epoch 699: train loss: 0.007135995198041201\n",
      "Epoch 700: train loss: 0.007130399812012911\n",
      "Epoch 701: train loss: 0.007176630198955536\n",
      "Epoch 702: train loss: 0.007224885281175375\n",
      "Epoch 703: train loss: 0.007178135681897402\n",
      "Epoch 704: train loss: 0.0071172332391142845\n",
      "Epoch 705: train loss: 0.007076260633766651\n",
      "Epoch 706: train loss: 0.007079080678522587\n",
      "Epoch 707: train loss: 0.007085734978318214\n",
      "Epoch 708: train loss: 0.0070695471949875355\n",
      "Epoch 709: train loss: 0.007046935148537159\n",
      "Epoch 710: train loss: 0.007042032200843096\n",
      "Epoch 711: train loss: 0.007057487033307552\n",
      "Epoch 712: train loss: 0.007064218167215586\n",
      "Epoch 713: train loss: 0.007050470449030399\n",
      "Epoch 714: train loss: 0.007023190148174763\n",
      "Epoch 715: train loss: 0.007006542757153511\n",
      "Epoch 716: train loss: 0.006999236065894365\n",
      "Epoch 717: train loss: 0.006993868388235569\n",
      "Epoch 718: train loss: 0.006976454518735409\n",
      "Epoch 719: train loss: 0.00695616751909256\n",
      "Epoch 720: train loss: 0.0069424924440681934\n",
      "Epoch 721: train loss: 0.0069380407221615314\n",
      "Epoch 722: train loss: 0.006935120094567537\n",
      "Epoch 723: train loss: 0.0069270352832973\n",
      "Epoch 724: train loss: 0.006915554404258728\n",
      "Epoch 725: train loss: 0.006906593218445778\n",
      "Epoch 726: train loss: 0.006903068628162146\n",
      "Epoch 727: train loss: 0.006901459768414497\n",
      "Epoch 728: train loss: 0.006898385006934404\n",
      "Epoch 729: train loss: 0.006893252953886986\n",
      "Epoch 730: train loss: 0.006891931872814894\n",
      "Epoch 731: train loss: 0.0068944417871534824\n",
      "Epoch 732: train loss: 0.006908523850142956\n",
      "Epoch 733: train loss: 0.006919493433088064\n",
      "Epoch 734: train loss: 0.0069511886686086655\n",
      "Epoch 735: train loss: 0.006968067958950996\n",
      "Epoch 736: train loss: 0.007025375496596098\n",
      "Epoch 737: train loss: 0.007041378412395716\n",
      "Epoch 738: train loss: 0.007104182615876198\n",
      "Epoch 739: train loss: 0.007078163791447878\n",
      "Epoch 740: train loss: 0.007085625547915697\n",
      "Epoch 741: train loss: 0.007000517565757036\n",
      "Epoch 742: train loss: 0.006941596511751413\n",
      "Epoch 743: train loss: 0.006854848936200142\n",
      "Epoch 744: train loss: 0.006796620786190033\n",
      "Epoch 745: train loss: 0.006756482180207968\n",
      "Epoch 746: train loss: 0.006739413365721703\n",
      "Epoch 747: train loss: 0.00673984969034791\n",
      "Epoch 748: train loss: 0.006751595996320248\n",
      "Epoch 749: train loss: 0.006771427113562822\n",
      "Epoch 750: train loss: 0.00678626261651516\n",
      "Epoch 751: train loss: 0.006806083954870701\n",
      "Epoch 752: train loss: 0.006802117917686701\n",
      "Epoch 753: train loss: 0.00680327694863081\n",
      "Epoch 754: train loss: 0.006778045557439327\n",
      "Epoch 755: train loss: 0.0067587364464998245\n",
      "Epoch 756: train loss: 0.006726563908159733\n",
      "Epoch 757: train loss: 0.006700217258185148\n",
      "Epoch 758: train loss: 0.006673403084278107\n",
      "Epoch 759: train loss: 0.006652684882283211\n",
      "Epoch 760: train loss: 0.00663647660985589\n",
      "Epoch 761: train loss: 0.006624985486268997\n",
      "Epoch 762: train loss: 0.006616831291466951\n",
      "Epoch 763: train loss: 0.006610962096601725\n",
      "Epoch 764: train loss: 0.0066068279556930065\n",
      "Epoch 765: train loss: 0.006603967864066362\n",
      "Epoch 766: train loss: 0.006602851673960686\n",
      "Epoch 767: train loss: 0.006601892411708832\n",
      "Epoch 768: train loss: 0.006603730842471123\n",
      "Epoch 769: train loss: 0.006604377180337906\n",
      "Epoch 770: train loss: 0.006610686890780926\n",
      "Epoch 771: train loss: 0.006614580284804106\n",
      "Epoch 772: train loss: 0.006628556177020073\n",
      "Epoch 773: train loss: 0.006636893376708031\n",
      "Epoch 774: train loss: 0.006661367602646351\n",
      "Epoch 775: train loss: 0.006671681068837643\n",
      "Epoch 776: train loss: 0.006705308798700571\n",
      "Epoch 777: train loss: 0.006707641761749983\n",
      "Epoch 778: train loss: 0.006736848969012499\n",
      "Epoch 779: train loss: 0.006716039497405291\n",
      "Epoch 780: train loss: 0.006718209944665432\n",
      "Epoch 781: train loss: 0.0066708712838590145\n",
      "Epoch 782: train loss: 0.006639275699853897\n",
      "Epoch 783: train loss: 0.006581151857972145\n",
      "Epoch 784: train loss: 0.006535751279443502\n",
      "Epoch 785: train loss: 0.006489184685051441\n",
      "Epoch 786: train loss: 0.006455773022025824\n",
      "Epoch 787: train loss: 0.006432109046727419\n",
      "Epoch 788: train loss: 0.006418602075427771\n",
      "Epoch 789: train loss: 0.006412717048078775\n",
      "Epoch 790: train loss: 0.006412187125533819\n",
      "Epoch 791: train loss: 0.006415411829948425\n",
      "Epoch 792: train loss: 0.006419675890356302\n",
      "Epoch 793: train loss: 0.006426145788282156\n",
      "Epoch 794: train loss: 0.006428562104701996\n",
      "Epoch 795: train loss: 0.006433579139411449\n",
      "Epoch 796: train loss: 0.006430938374251127\n",
      "Epoch 797: train loss: 0.006432221736758947\n",
      "Epoch 798: train loss: 0.0064248573035001755\n",
      "Epoch 799: train loss: 0.006421823985874653\n",
      "Epoch 800: train loss: 0.006410287227481604\n",
      "Epoch 801: train loss: 0.0064030783250927925\n",
      "Epoch 802: train loss: 0.006388448644429445\n",
      "Epoch 803: train loss: 0.006378427613526583\n",
      "Epoch 804: train loss: 0.006362521089613438\n",
      "Epoch 805: train loss: 0.006350964307785034\n",
      "Epoch 806: train loss: 0.006335265934467316\n",
      "Epoch 807: train loss: 0.006323240231722593\n",
      "Epoch 808: train loss: 0.00630901288241148\n",
      "Epoch 809: train loss: 0.006297703366726637\n",
      "Epoch 810: train loss: 0.0062854536809027195\n",
      "Epoch 811: train loss: 0.006275349296629429\n",
      "Epoch 812: train loss: 0.006264840718358755\n",
      "Epoch 813: train loss: 0.006256059743463993\n",
      "Epoch 814: train loss: 0.006247120909392834\n",
      "Epoch 815: train loss: 0.006239788606762886\n",
      "Epoch 816: train loss: 0.006232334766536951\n",
      "Epoch 817: train loss: 0.006226567085832357\n",
      "Epoch 818: train loss: 0.0062207626178860664\n",
      "Epoch 819: train loss: 0.006217150017619133\n",
      "Epoch 820: train loss: 0.006213763728737831\n",
      "Epoch 821: train loss: 0.006213705986738205\n",
      "Epoch 822: train loss: 0.006214195862412453\n",
      "Epoch 823: train loss: 0.006220356561243534\n",
      "Epoch 824: train loss: 0.006227175239473581\n",
      "Epoch 825: train loss: 0.006244244519621134\n",
      "Epoch 826: train loss: 0.006260678172111511\n",
      "Epoch 827: train loss: 0.006295105908066034\n",
      "Epoch 828: train loss: 0.006322535686194897\n",
      "Epoch 829: train loss: 0.006377692334353924\n",
      "Epoch 830: train loss: 0.006407559383660555\n",
      "Epoch 831: train loss: 0.006469869986176491\n",
      "Epoch 832: train loss: 0.0064737931825220585\n",
      "Epoch 833: train loss: 0.0065000238828361034\n",
      "Epoch 834: train loss: 0.006443292833864689\n",
      "Epoch 835: train loss: 0.006393302697688341\n",
      "Epoch 836: train loss: 0.006287041585892439\n",
      "Epoch 837: train loss: 0.006195024587213993\n",
      "Epoch 838: train loss: 0.0061095659621059895\n",
      "Epoch 839: train loss: 0.00605785520747304\n",
      "Epoch 840: train loss: 0.006039712578058243\n",
      "Epoch 841: train loss: 0.006049102637916803\n",
      "Epoch 842: train loss: 0.006074267439544201\n",
      "Epoch 843: train loss: 0.006099353544414043\n",
      "Epoch 844: train loss: 0.006119166035205126\n",
      "Epoch 845: train loss: 0.0061155762523412704\n",
      "Epoch 846: train loss: 0.006101164501160383\n",
      "Epoch 847: train loss: 0.006066702771931887\n",
      "Epoch 848: train loss: 0.006031633820384741\n",
      "Epoch 849: train loss: 0.005998510401695967\n",
      "Epoch 850: train loss: 0.0059758988209068775\n",
      "Epoch 851: train loss: 0.005964671261608601\n",
      "Epoch 852: train loss: 0.005962982773780823\n",
      "Epoch 853: train loss: 0.005966986063867807\n",
      "Epoch 854: train loss: 0.005971641279757023\n",
      "Epoch 855: train loss: 0.005974217317998409\n",
      "Epoch 856: train loss: 0.005970454309135675\n",
      "Epoch 857: train loss: 0.005962226539850235\n",
      "Epoch 858: train loss: 0.005948367062956095\n",
      "Epoch 859: train loss: 0.005933144595474005\n",
      "Epoch 860: train loss: 0.005917377769947052\n",
      "Epoch 861: train loss: 0.005903689190745354\n",
      "Epoch 862: train loss: 0.005892653483897448\n",
      "Epoch 863: train loss: 0.0058845700696110725\n",
      "Epoch 864: train loss: 0.00587897514924407\n",
      "Epoch 865: train loss: 0.005874997470527887\n",
      "Epoch 866: train loss: 0.005871708504855633\n",
      "Epoch 867: train loss: 0.005868101492524147\n",
      "Epoch 868: train loss: 0.005863866303116083\n",
      "Epoch 869: train loss: 0.005858381278812885\n",
      "Epoch 870: train loss: 0.005852075293660164\n",
      "Epoch 871: train loss: 0.005844651721417904\n",
      "Epoch 872: train loss: 0.005836754105985165\n",
      "Epoch 873: train loss: 0.005828336346894503\n",
      "Epoch 874: train loss: 0.005819937214255333\n",
      "Epoch 875: train loss: 0.005811591632664204\n",
      "Epoch 876: train loss: 0.005803569685667753\n",
      "Epoch 877: train loss: 0.0057958560064435005\n",
      "Epoch 878: train loss: 0.005788509733974934\n",
      "Epoch 879: train loss: 0.005781499203294516\n",
      "Epoch 880: train loss: 0.005774795077741146\n",
      "Epoch 881: train loss: 0.005768341012299061\n",
      "Epoch 882: train loss: 0.005762071348726749\n",
      "Epoch 883: train loss: 0.005755936726927757\n",
      "Epoch 884: train loss: 0.005749898497015238\n",
      "Epoch 885: train loss: 0.0057439422234892845\n",
      "Epoch 886: train loss: 0.005738050676882267\n",
      "Epoch 887: train loss: 0.005732222460210323\n",
      "Epoch 888: train loss: 0.0057264408096671104\n",
      "Epoch 889: train loss: 0.005720746237784624\n",
      "Epoch 890: train loss: 0.005715131759643555\n",
      "Epoch 891: train loss: 0.005709668155759573\n",
      "Epoch 892: train loss: 0.0057043530978262424\n",
      "Epoch 893: train loss: 0.005699310917407274\n",
      "Epoch 894: train loss: 0.005694536026567221\n",
      "Epoch 895: train loss: 0.005690272431820631\n",
      "Epoch 896: train loss: 0.005686508491635323\n",
      "Epoch 897: train loss: 0.005683715455234051\n",
      "Epoch 898: train loss: 0.005681868176907301\n",
      "Epoch 899: train loss: 0.005681908689439297\n",
      "Epoch 900: train loss: 0.005683742929250002\n",
      "Epoch 901: train loss: 0.005689367186278105\n",
      "Epoch 902: train loss: 0.005698330234736204\n",
      "Epoch 903: train loss: 0.005714930593967438\n",
      "Epoch 904: train loss: 0.005737259518355131\n",
      "Epoch 905: train loss: 0.005774544086307287\n",
      "Epoch 906: train loss: 0.0058195278979837894\n",
      "Epoch 907: train loss: 0.005891073029488325\n",
      "Epoch 908: train loss: 0.0059644915163517\n",
      "Epoch 909: train loss: 0.0060741794295609\n",
      "Epoch 910: train loss: 0.00615114439278841\n",
      "Epoch 911: train loss: 0.006248855963349342\n",
      "Epoch 912: train loss: 0.0062368568032979965\n",
      "Epoch 913: train loss: 0.006196448113769293\n",
      "Epoch 914: train loss: 0.006021234206855297\n",
      "Epoch 915: train loss: 0.005832234863191843\n",
      "Epoch 916: train loss: 0.005651284474879503\n",
      "Epoch 917: train loss: 0.005558783188462257\n",
      "Epoch 918: train loss: 0.005566399544477463\n",
      "Epoch 919: train loss: 0.005639162380248308\n",
      "Epoch 920: train loss: 0.005719900596886873\n",
      "Epoch 921: train loss: 0.005744670517742634\n",
      "Epoch 922: train loss: 0.0057111638598144054\n",
      "Epoch 923: train loss: 0.005625433288514614\n",
      "Epoch 924: train loss: 0.005546411499381065\n",
      "Epoch 925: train loss: 0.005509659182280302\n",
      "Epoch 926: train loss: 0.005522933788597584\n",
      "Epoch 927: train loss: 0.005560821853578091\n",
      "Epoch 928: train loss: 0.005585355684161186\n",
      "Epoch 929: train loss: 0.005578845273703337\n",
      "Epoch 930: train loss: 0.005540975369513035\n",
      "Epoch 931: train loss: 0.0054985941387712955\n",
      "Epoch 932: train loss: 0.005473426543176174\n",
      "Epoch 933: train loss: 0.005473493132740259\n",
      "Epoch 934: train loss: 0.0054882545955479145\n",
      "Epoch 935: train loss: 0.0054991282522678375\n",
      "Epoch 936: train loss: 0.005495165474712849\n",
      "Epoch 937: train loss: 0.005475793033838272\n",
      "Epoch 938: train loss: 0.00545286713168025\n",
      "Epoch 939: train loss: 0.00543729355558753\n",
      "Epoch 940: train loss: 0.0054336730390787125\n",
      "Epoch 941: train loss: 0.005437783896923065\n",
      "Epoch 942: train loss: 0.00544105377048254\n",
      "Epoch 943: train loss: 0.005437585525214672\n",
      "Epoch 944: train loss: 0.005426590330898762\n",
      "Epoch 945: train loss: 0.005413064733147621\n",
      "Epoch 946: train loss: 0.005402335897088051\n",
      "Epoch 947: train loss: 0.005397114437073469\n",
      "Epoch 948: train loss: 0.005396068096160889\n",
      "Epoch 949: train loss: 0.005395525600761175\n",
      "Epoch 950: train loss: 0.005392386578023434\n",
      "Epoch 951: train loss: 0.005385654512792826\n",
      "Epoch 952: train loss: 0.005376969929784536\n",
      "Epoch 953: train loss: 0.005368715617805719\n",
      "Epoch 954: train loss: 0.005362666677683592\n",
      "Epoch 955: train loss: 0.00535892928019166\n",
      "Epoch 956: train loss: 0.0053562624379992485\n",
      "Epoch 957: train loss: 0.005353106185793877\n",
      "Epoch 958: train loss: 0.005348485894501209\n",
      "Epoch 959: train loss: 0.005342536140233278\n",
      "Epoch 960: train loss: 0.005336056929081678\n",
      "Epoch 961: train loss: 0.005330064333975315\n",
      "Epoch 962: train loss: 0.0053250850178301334\n",
      "Epoch 963: train loss: 0.00532102445140481\n",
      "Epoch 964: train loss: 0.005317342467606068\n",
      "Epoch 965: train loss: 0.005313429050147533\n",
      "Epoch 966: train loss: 0.00530893262475729\n",
      "Epoch 967: train loss: 0.005303883459419012\n",
      "Epoch 968: train loss: 0.005298596806824207\n",
      "Epoch 969: train loss: 0.00529345590621233\n",
      "Epoch 970: train loss: 0.005288700573146343\n",
      "Epoch 971: train loss: 0.0052843582816421986\n",
      "Epoch 972: train loss: 0.0052802590653300285\n",
      "Epoch 973: train loss: 0.005276180803775787\n",
      "Epoch 974: train loss: 0.005271944683045149\n",
      "Epoch 975: train loss: 0.005267490167170763\n",
      "Epoch 976: train loss: 0.005262871738523245\n",
      "Epoch 977: train loss: 0.005258214194327593\n",
      "Epoch 978: train loss: 0.005253644194453955\n",
      "Epoch 979: train loss: 0.005249227862805128\n",
      "Epoch 980: train loss: 0.0052449703216552734\n",
      "Epoch 981: train loss: 0.005240817554295063\n",
      "Epoch 982: train loss: 0.0052367039024829865\n",
      "Epoch 983: train loss: 0.0052325548604130745\n",
      "Epoch 984: train loss: 0.005228347610682249\n",
      "Epoch 985: train loss: 0.005224086809903383\n",
      "Epoch 986: train loss: 0.00521980132907629\n",
      "Epoch 987: train loss: 0.005215529352426529\n",
      "Epoch 988: train loss: 0.005211304873228073\n",
      "Epoch 989: train loss: 0.0052071427926421165\n",
      "Epoch 990: train loss: 0.005203042645007372\n",
      "Epoch 991: train loss: 0.005198991391807795\n",
      "Epoch 992: train loss: 0.005194965284317732\n",
      "Epoch 993: train loss: 0.005190953612327576\n",
      "Epoch 994: train loss: 0.005186939612030983\n",
      "Epoch 995: train loss: 0.005182921886444092\n",
      "Epoch 996: train loss: 0.005178903695195913\n",
      "Epoch 997: train loss: 0.0051748910918831825\n",
      "Epoch 998: train loss: 0.005170896183699369\n",
      "Epoch 999: train loss: 0.005166924558579922\n",
      "Epoch 1000: train loss: 0.005162977147847414\n",
      "Epoch 1001: train loss: 0.005159064661711454\n",
      "Epoch 1002: train loss: 0.005155173130333424\n",
      "Epoch 1003: train loss: 0.005151309538632631\n",
      "Epoch 1004: train loss: 0.005147470161318779\n",
      "Epoch 1005: train loss: 0.005143646616488695\n",
      "Epoch 1006: train loss: 0.00513983890414238\n",
      "Epoch 1007: train loss: 0.005136044230312109\n",
      "Epoch 1008: train loss: 0.005132265388965607\n",
      "Epoch 1009: train loss: 0.005128495395183563\n",
      "Epoch 1010: train loss: 0.005124744493514299\n",
      "Epoch 1011: train loss: 0.005121007561683655\n",
      "Epoch 1012: train loss: 0.005117284599691629\n",
      "Epoch 1013: train loss: 0.005113586783409119\n",
      "Epoch 1014: train loss: 0.005109900608658791\n",
      "Epoch 1015: train loss: 0.005106235854327679\n",
      "Epoch 1016: train loss: 0.005102590192109346\n",
      "Epoch 1017: train loss: 0.005098963156342506\n",
      "Epoch 1018: train loss: 0.00509535102173686\n",
      "Epoch 1019: train loss: 0.005091763101518154\n",
      "Epoch 1020: train loss: 0.005088185891509056\n",
      "Epoch 1021: train loss: 0.0050846305675804615\n",
      "Epoch 1022: train loss: 0.005081093870103359\n",
      "Epoch 1023: train loss: 0.005077569279819727\n",
      "Epoch 1024: train loss: 0.005074066109955311\n",
      "Epoch 1025: train loss: 0.005070578772574663\n",
      "Epoch 1026: train loss: 0.005067107267677784\n",
      "Epoch 1027: train loss: 0.00506365392357111\n",
      "Epoch 1028: train loss: 0.0050602140836417675\n",
      "Epoch 1029: train loss: 0.005056793801486492\n",
      "Epoch 1030: train loss: 0.00505339028313756\n",
      "Epoch 1031: train loss: 0.005050002131611109\n",
      "Epoch 1032: train loss: 0.005046632140874863\n",
      "Epoch 1033: train loss: 0.005043279379606247\n",
      "Epoch 1034: train loss: 0.005039941985160112\n",
      "Epoch 1035: train loss: 0.005036620888859034\n",
      "Epoch 1036: train loss: 0.005033324006944895\n",
      "Epoch 1037: train loss: 0.0050300415605306625\n",
      "Epoch 1038: train loss: 0.005026781931519508\n",
      "Epoch 1039: train loss: 0.005023544188588858\n",
      "Epoch 1040: train loss: 0.005020324606448412\n",
      "Epoch 1041: train loss: 0.005017137620598078\n",
      "Epoch 1042: train loss: 0.005013976711779833\n",
      "Epoch 1043: train loss: 0.005010859575122595\n",
      "Epoch 1044: train loss: 0.005007788073271513\n",
      "Epoch 1045: train loss: 0.00500478083267808\n",
      "Epoch 1046: train loss: 0.005001860670745373\n",
      "Epoch 1047: train loss: 0.004999062046408653\n",
      "Epoch 1048: train loss: 0.004996440373361111\n",
      "Epoch 1049: train loss: 0.004994071088731289\n",
      "Epoch 1050: train loss: 0.004992080386728048\n",
      "Epoch 1051: train loss: 0.0049906885251402855\n",
      "Epoch 1052: train loss: 0.004990147892385721\n",
      "Epoch 1053: train loss: 0.004991032648831606\n",
      "Epoch 1054: train loss: 0.00499396026134491\n",
      "Epoch 1055: train loss: 0.005000394769012928\n",
      "Epoch 1056: train loss: 0.005011728499084711\n",
      "Epoch 1057: train loss: 0.005031871143728495\n",
      "Epoch 1058: train loss: 0.005063478834927082\n",
      "Epoch 1059: train loss: 0.005116750486195087\n",
      "Epoch 1060: train loss: 0.0051939417608082294\n",
      "Epoch 1061: train loss: 0.005318960174918175\n",
      "Epoch 1062: train loss: 0.005475975107401609\n",
      "Epoch 1063: train loss: 0.005705547984689474\n",
      "Epoch 1064: train loss: 0.005899437703192234\n",
      "Epoch 1065: train loss: 0.006090990733355284\n",
      "Epoch 1066: train loss: 0.006018948275595903\n",
      "Epoch 1067: train loss: 0.005781441926956177\n",
      "Epoch 1068: train loss: 0.005344445817172527\n",
      "Epoch 1069: train loss: 0.005009238608181477\n",
      "Epoch 1070: train loss: 0.00494428351521492\n",
      "Epoch 1071: train loss: 0.005117477383464575\n",
      "Epoch 1072: train loss: 0.005312475375831127\n",
      "Epoch 1073: train loss: 0.005308864638209343\n",
      "Epoch 1074: train loss: 0.00512689771130681\n",
      "Epoch 1075: train loss: 0.004946134053170681\n",
      "Epoch 1076: train loss: 0.0049355048686265945\n",
      "Epoch 1077: train loss: 0.005052958149462938\n",
      "Epoch 1078: train loss: 0.005120652262121439\n",
      "Epoch 1079: train loss: 0.005053779110312462\n",
      "Epoch 1080: train loss: 0.004937066696584225\n",
      "Epoch 1081: train loss: 0.004905576352030039\n",
      "Epoch 1082: train loss: 0.004967029206454754\n",
      "Epoch 1083: train loss: 0.0050145224668085575\n",
      "Epoch 1084: train loss: 0.0049798754043877125\n",
      "Epoch 1085: train loss: 0.004910195246338844\n",
      "Epoch 1086: train loss: 0.004891095217317343\n",
      "Epoch 1087: train loss: 0.0049277255311608315\n",
      "Epoch 1088: train loss: 0.004952783230692148\n",
      "Epoch 1089: train loss: 0.004927753936499357\n",
      "Epoch 1090: train loss: 0.004886397626250982\n",
      "Epoch 1091: train loss: 0.004878470208495855\n",
      "Epoch 1092: train loss: 0.004900916013866663\n",
      "Epoch 1093: train loss: 0.004911715164780617\n",
      "Epoch 1094: train loss: 0.004892646335065365\n",
      "Epoch 1095: train loss: 0.004868393763899803\n",
      "Epoch 1096: train loss: 0.004866328556090593\n",
      "Epoch 1097: train loss: 0.0048795295879244804\n",
      "Epoch 1098: train loss: 0.004882522393018007\n",
      "Epoch 1099: train loss: 0.004868478048592806\n",
      "Epoch 1100: train loss: 0.004854185972362757\n",
      "Epoch 1101: train loss: 0.004853850230574608\n",
      "Epoch 1102: train loss: 0.004860885441303253\n",
      "Epoch 1103: train loss: 0.0048604849725961685\n",
      "Epoch 1104: train loss: 0.004850553814321756\n",
      "Epoch 1105: train loss: 0.004841732792556286\n",
      "Epoch 1106: train loss: 0.004841386340558529\n",
      "Epoch 1107: train loss: 0.004844638053327799\n",
      "Epoch 1108: train loss: 0.004843016155064106\n",
      "Epoch 1109: train loss: 0.004836024716496468\n",
      "Epoch 1110: train loss: 0.004830080084502697\n",
      "Epoch 1111: train loss: 0.004829142242670059\n",
      "Epoch 1112: train loss: 0.00483016949146986\n",
      "Epoch 1113: train loss: 0.004828289616852999\n",
      "Epoch 1114: train loss: 0.004823278170078993\n",
      "Epoch 1115: train loss: 0.004818849265575409\n",
      "Epoch 1116: train loss: 0.004817342851310968\n",
      "Epoch 1117: train loss: 0.004817094188183546\n",
      "Epoch 1118: train loss: 0.00481525668874383\n",
      "Epoch 1119: train loss: 0.00481152068823576\n",
      "Epoch 1120: train loss: 0.004807936027646065\n",
      "Epoch 1121: train loss: 0.004806022625416517\n",
      "Epoch 1122: train loss: 0.004805027041584253\n",
      "Epoch 1123: train loss: 0.004803275689482689\n",
      "Epoch 1124: train loss: 0.004800345748662949\n",
      "Epoch 1125: train loss: 0.004797300323843956\n",
      "Epoch 1126: train loss: 0.004795156419277191\n",
      "Epoch 1127: train loss: 0.004793702159076929\n",
      "Epoch 1128: train loss: 0.004791992250829935\n",
      "Epoch 1129: train loss: 0.004789573606103659\n",
      "Epoch 1130: train loss: 0.004786911886185408\n",
      "Epoch 1131: train loss: 0.004784672986716032\n",
      "Epoch 1132: train loss: 0.004782929550856352\n",
      "Epoch 1133: train loss: 0.004781204275786877\n",
      "Epoch 1134: train loss: 0.004779099952429533\n",
      "Epoch 1135: train loss: 0.0047767371870577335\n",
      "Epoch 1136: train loss: 0.004774508997797966\n",
      "Epoch 1137: train loss: 0.004772593732923269\n",
      "Epoch 1138: train loss: 0.004770809318870306\n",
      "Epoch 1139: train loss: 0.004768881015479565\n",
      "Epoch 1140: train loss: 0.004766749683767557\n",
      "Epoch 1141: train loss: 0.004764600191265345\n",
      "Epoch 1142: train loss: 0.004762603435665369\n",
      "Epoch 1143: train loss: 0.004760757088661194\n",
      "Epoch 1144: train loss: 0.004758906085044146\n",
      "Epoch 1145: train loss: 0.004756936337798834\n",
      "Epoch 1146: train loss: 0.004754898604005575\n",
      "Epoch 1147: train loss: 0.0047529018484056\n",
      "Epoch 1148: train loss: 0.004751008003950119\n",
      "Epoch 1149: train loss: 0.004749165382236242\n",
      "Epoch 1150: train loss: 0.004747295752167702\n",
      "Epoch 1151: train loss: 0.004745371174067259\n",
      "Epoch 1152: train loss: 0.0047434251755476\n",
      "Epoch 1153: train loss: 0.004741520155221224\n",
      "Epoch 1154: train loss: 0.004739674739539623\n",
      "Epoch 1155: train loss: 0.00473784701898694\n",
      "Epoch 1156: train loss: 0.004736003931611776\n",
      "Epoch 1157: train loss: 0.00473413011059165\n",
      "Epoch 1158: train loss: 0.00473225861787796\n",
      "Epoch 1159: train loss: 0.004730413667857647\n",
      "Epoch 1160: train loss: 0.004728603642433882\n",
      "Epoch 1161: train loss: 0.004726801533252001\n",
      "Epoch 1162: train loss: 0.004724994767457247\n",
      "Epoch 1163: train loss: 0.004723174963146448\n",
      "Epoch 1164: train loss: 0.0047213612124323845\n",
      "Epoch 1165: train loss: 0.004719563294202089\n",
      "Epoch 1166: train loss: 0.004717783071100712\n",
      "Epoch 1167: train loss: 0.004716015886515379\n",
      "Epoch 1168: train loss: 0.004714251961559057\n",
      "Epoch 1169: train loss: 0.004712480586022139\n",
      "Epoch 1170: train loss: 0.004710712935775518\n",
      "Epoch 1171: train loss: 0.004708955995738506\n",
      "Epoch 1172: train loss: 0.004707212094217539\n",
      "Epoch 1173: train loss: 0.004705478437244892\n",
      "Epoch 1174: train loss: 0.004703748505562544\n",
      "Epoch 1175: train loss: 0.004702020902186632\n",
      "Epoch 1176: train loss: 0.004700296558439732\n",
      "Epoch 1177: train loss: 0.004698579199612141\n",
      "Epoch 1178: train loss: 0.004696870222687721\n",
      "Epoch 1179: train loss: 0.004695170093327761\n",
      "Epoch 1180: train loss: 0.00469347694888711\n",
      "Epoch 1181: train loss: 0.004691786132752895\n",
      "Epoch 1182: train loss: 0.004690101835876703\n",
      "Epoch 1183: train loss: 0.004688419867306948\n",
      "Epoch 1184: train loss: 0.004686746280640364\n",
      "Epoch 1185: train loss: 0.004685078281909227\n",
      "Epoch 1186: train loss: 0.004683420527726412\n",
      "Epoch 1187: train loss: 0.0046817646361887455\n",
      "Epoch 1188: train loss: 0.004680114332586527\n",
      "Epoch 1189: train loss: 0.004678469616919756\n",
      "Epoch 1190: train loss: 0.004676828160881996\n",
      "Epoch 1191: train loss: 0.004675194155424833\n",
      "Epoch 1192: train loss: 0.0046735648065805435\n",
      "Epoch 1193: train loss: 0.004671942908316851\n",
      "Epoch 1194: train loss: 0.004670325201004744\n",
      "Epoch 1195: train loss: 0.004668713081628084\n",
      "Epoch 1196: train loss: 0.004667104221880436\n",
      "Epoch 1197: train loss: 0.004665501415729523\n",
      "Epoch 1198: train loss: 0.00466390373185277\n",
      "Epoch 1199: train loss: 0.004662313498556614\n",
      "Epoch 1200: train loss: 0.004660721868276596\n",
      "Epoch 1201: train loss: 0.004659140948206186\n",
      "Epoch 1202: train loss: 0.0046575628221035\n",
      "Epoch 1203: train loss: 0.004655993543565273\n",
      "Epoch 1204: train loss: 0.004654423333704472\n",
      "Epoch 1205: train loss: 0.004652859643101692\n",
      "Epoch 1206: train loss: 0.004651299677789211\n",
      "Epoch 1207: train loss: 0.004649748560041189\n",
      "Epoch 1208: train loss: 0.004648195113986731\n",
      "Epoch 1209: train loss: 0.004646650515496731\n",
      "Epoch 1210: train loss: 0.004645111039280891\n",
      "Epoch 1211: train loss: 0.004643576685339212\n",
      "Epoch 1212: train loss: 0.004642043728381395\n",
      "Epoch 1213: train loss: 0.004640513565391302\n",
      "Epoch 1214: train loss: 0.004638994112610817\n",
      "Epoch 1215: train loss: 0.004637474659830332\n",
      "Epoch 1216: train loss: 0.00463595986366272\n",
      "Epoch 1217: train loss: 0.004634448792785406\n",
      "Epoch 1218: train loss: 0.00463294330984354\n",
      "Epoch 1219: train loss: 0.004631440620869398\n",
      "Epoch 1220: train loss: 0.004629942588508129\n",
      "Epoch 1221: train loss: 0.0046284496784210205\n",
      "Epoch 1222: train loss: 0.004626958630979061\n",
      "Epoch 1223: train loss: 0.004625470377504826\n",
      "Epoch 1224: train loss: 0.004623990971595049\n",
      "Epoch 1225: train loss: 0.004622509703040123\n",
      "Epoch 1226: train loss: 0.004621035419404507\n",
      "Epoch 1227: train loss: 0.0046195657923817635\n",
      "Epoch 1228: train loss: 0.004618097096681595\n",
      "Epoch 1229: train loss: 0.004616633057594299\n",
      "Epoch 1230: train loss: 0.004615175072103739\n",
      "Epoch 1231: train loss: 0.00461371848359704\n",
      "Epoch 1232: train loss: 0.004612264223396778\n",
      "Epoch 1233: train loss: 0.004610812291502953\n",
      "Epoch 1234: train loss: 0.004609365947544575\n",
      "Epoch 1235: train loss: 0.0046079265885055065\n",
      "Epoch 1236: train loss: 0.004606486763805151\n",
      "Epoch 1237: train loss: 0.0046050529927015305\n",
      "Epoch 1238: train loss: 0.004603615961968899\n",
      "Epoch 1239: train loss: 0.004602188244462013\n",
      "Epoch 1240: train loss: 0.004600762855261564\n",
      "Epoch 1241: train loss: 0.00459933839738369\n",
      "Epoch 1242: train loss: 0.004597916733473539\n",
      "Epoch 1243: train loss: 0.004596501123160124\n",
      "Epoch 1244: train loss: 0.004595086444169283\n",
      "Epoch 1245: train loss: 0.004593676421791315\n",
      "Epoch 1246: train loss: 0.004592267330735922\n",
      "Epoch 1247: train loss: 0.0045908596366643906\n",
      "Epoch 1248: train loss: 0.004589460324496031\n",
      "Epoch 1249: train loss: 0.0045880600810050964\n",
      "Epoch 1250: train loss: 0.0045866649597883224\n",
      "Epoch 1251: train loss: 0.0045852684415876865\n",
      "Epoch 1252: train loss: 0.004583875183016062\n",
      "Epoch 1253: train loss: 0.00458248658105731\n",
      "Epoch 1254: train loss: 0.004581100307404995\n",
      "Epoch 1255: train loss: 0.0045797196216881275\n",
      "Epoch 1256: train loss: 0.00457833893597126\n",
      "Epoch 1257: train loss: 0.004576956853270531\n",
      "Epoch 1258: train loss: 0.004575582221150398\n",
      "Epoch 1259: train loss: 0.0045742071233689785\n",
      "Epoch 1260: train loss: 0.0045728362165391445\n",
      "Epoch 1261: train loss: 0.00457146717235446\n",
      "Epoch 1262: train loss: 0.0045701004564762115\n",
      "Epoch 1263: train loss: 0.0045687370002269745\n",
      "Epoch 1264: train loss: 0.004567374009639025\n",
      "Epoch 1265: train loss: 0.004566016141325235\n",
      "Epoch 1266: train loss: 0.004564656410366297\n",
      "Epoch 1267: train loss: 0.0045633018016815186\n",
      "Epoch 1268: train loss: 0.004561946261674166\n",
      "Epoch 1269: train loss: 0.004560594912618399\n",
      "Epoch 1270: train loss: 0.004559245891869068\n",
      "Epoch 1271: train loss: 0.004557899199426174\n",
      "Epoch 1272: train loss: 0.004556550644338131\n",
      "Epoch 1273: train loss: 0.004555207211524248\n",
      "Epoch 1274: train loss: 0.004553864244371653\n",
      "Epoch 1275: train loss: 0.004552523139864206\n",
      "Epoch 1276: train loss: 0.004551185294985771\n",
      "Epoch 1277: train loss: 0.004549846984446049\n",
      "Epoch 1278: train loss: 0.004548512864857912\n",
      "Epoch 1279: train loss: 0.004547179210931063\n",
      "Epoch 1280: train loss: 0.004545846488326788\n",
      "Epoch 1281: train loss: 0.00454451609402895\n",
      "Epoch 1282: train loss: 0.004543188493698835\n",
      "Epoch 1283: train loss: 0.004541859496384859\n",
      "Epoch 1284: train loss: 0.004540532827377319\n",
      "Epoch 1285: train loss: 0.004539208486676216\n",
      "Epoch 1286: train loss: 0.004537883680313826\n",
      "Epoch 1287: train loss: 0.004536563064903021\n",
      "Epoch 1288: train loss: 0.004535244312137365\n",
      "Epoch 1289: train loss: 0.004533919971436262\n",
      "Epoch 1290: train loss: 0.0045326040126383305\n",
      "Epoch 1291: train loss: 0.004531285725533962\n",
      "Epoch 1292: train loss: 0.004529966507107019\n",
      "Epoch 1293: train loss: 0.004528652410954237\n",
      "Epoch 1294: train loss: 0.004527340177446604\n",
      "Epoch 1295: train loss: 0.004526026081293821\n",
      "Epoch 1296: train loss: 0.004524714779108763\n",
      "Epoch 1297: train loss: 0.004523403476923704\n",
      "Epoch 1298: train loss: 0.0045220935717225075\n",
      "Epoch 1299: train loss: 0.004520784597843885\n",
      "Epoch 1300: train loss: 0.004519477486610413\n",
      "Epoch 1301: train loss: 0.004518175031989813\n",
      "Epoch 1302: train loss: 0.0045168763026595116\n",
      "Epoch 1303: train loss: 0.0045155794359743595\n",
      "Epoch 1304: train loss: 0.004514297004789114\n",
      "Epoch 1305: train loss: 0.004513023886829615\n",
      "Epoch 1306: train loss: 0.0045117782428860664\n",
      "Epoch 1307: train loss: 0.004510573577135801\n",
      "Epoch 1308: train loss: 0.004509440623223782\n",
      "Epoch 1309: train loss: 0.0045084357261657715\n",
      "Epoch 1310: train loss: 0.004507648758590221\n",
      "Epoch 1311: train loss: 0.0045072538778185844\n",
      "Epoch 1312: train loss: 0.004507550038397312\n",
      "Epoch 1313: train loss: 0.004509108606725931\n",
      "Epoch 1314: train loss: 0.0045128813944756985\n",
      "Epoch 1315: train loss: 0.004520842805504799\n",
      "Epoch 1316: train loss: 0.004536006599664688\n",
      "Epoch 1317: train loss: 0.004565313458442688\n",
      "Epoch 1318: train loss: 0.004617707338184118\n",
      "Epoch 1319: train loss: 0.004717282485216856\n",
      "Epoch 1320: train loss: 0.004883436486124992\n",
      "Epoch 1321: train loss: 0.005189104471355677\n",
      "Epoch 1322: train loss: 0.005609432701021433\n",
      "Epoch 1323: train loss: 0.006263666786253452\n",
      "Epoch 1324: train loss: 0.006662493571639061\n",
      "Epoch 1325: train loss: 0.0068097421899437904\n",
      "Epoch 1326: train loss: 0.005897178314626217\n",
      "Epoch 1327: train loss: 0.004867217969149351\n",
      "Epoch 1328: train loss: 0.004499814938753843\n",
      "Epoch 1329: train loss: 0.004994839429855347\n",
      "Epoch 1330: train loss: 0.005475410260260105\n",
      "Epoch 1331: train loss: 0.0051080831326544285\n",
      "Epoch 1332: train loss: 0.004548463970422745\n",
      "Epoch 1333: train loss: 0.004605013411492109\n",
      "Epoch 1334: train loss: 0.004979014862328768\n",
      "Epoch 1335: train loss: 0.004904907196760178\n",
      "Epoch 1336: train loss: 0.004525548312813044\n",
      "Epoch 1337: train loss: 0.004570475779473782\n",
      "Epoch 1338: train loss: 0.004819854162633419\n",
      "Epoch 1339: train loss: 0.004691168200224638\n",
      "Epoch 1340: train loss: 0.004478733520954847\n",
      "Epoch 1341: train loss: 0.0045813280157744884\n",
      "Epoch 1342: train loss: 0.004690483212471008\n",
      "Epoch 1343: train loss: 0.004548912402242422\n",
      "Epoch 1344: train loss: 0.004474986810237169\n",
      "Epoch 1345: train loss: 0.004588622134178877\n",
      "Epoch 1346: train loss: 0.0045902603305876255\n",
      "Epoch 1347: train loss: 0.004475036170333624\n",
      "Epoch 1348: train loss: 0.004498129244893789\n",
      "Epoch 1349: train loss: 0.004565930459648371\n",
      "Epoch 1350: train loss: 0.004504209849983454\n",
      "Epoch 1351: train loss: 0.004459303338080645\n",
      "Epoch 1352: train loss: 0.004511933773756027\n",
      "Epoch 1353: train loss: 0.0045144204050302505\n",
      "Epoch 1354: train loss: 0.004458785988390446\n",
      "Epoch 1355: train loss: 0.004469045903533697\n",
      "Epoch 1356: train loss: 0.0044997381046414375\n",
      "Epoch 1357: train loss: 0.00447024405002594\n",
      "Epoch 1358: train loss: 0.004448638763278723\n",
      "Epoch 1359: train loss: 0.004473470617085695\n",
      "Epoch 1360: train loss: 0.004473862703889608\n",
      "Epoch 1361: train loss: 0.004445984959602356\n",
      "Epoch 1362: train loss: 0.00445107277482748\n",
      "Epoch 1363: train loss: 0.004464977886527777\n",
      "Epoch 1364: train loss: 0.004449629690498114\n",
      "Epoch 1365: train loss: 0.004438579082489014\n",
      "Epoch 1366: train loss: 0.004450219217687845\n",
      "Epoch 1367: train loss: 0.004449476022273302\n",
      "Epoch 1368: train loss: 0.004435518756508827\n",
      "Epoch 1369: train loss: 0.004436758812516928\n",
      "Epoch 1370: train loss: 0.004443020559847355\n",
      "Epoch 1371: train loss: 0.004435639828443527\n",
      "Epoch 1372: train loss: 0.0044288719072937965\n",
      "Epoch 1373: train loss: 0.004433511756360531\n",
      "Epoch 1374: train loss: 0.004433337599039078\n",
      "Epoch 1375: train loss: 0.004425890278071165\n",
      "Epoch 1376: train loss: 0.004424917045980692\n",
      "Epoch 1377: train loss: 0.004427680745720863\n",
      "Epoch 1378: train loss: 0.004424036480486393\n",
      "Epoch 1379: train loss: 0.004419503267854452\n",
      "Epoch 1380: train loss: 0.004420628771185875\n",
      "Epoch 1381: train loss: 0.004420536570250988\n",
      "Epoch 1382: train loss: 0.004416476935148239\n",
      "Epoch 1383: train loss: 0.004414509050548077\n",
      "Epoch 1384: train loss: 0.004415285773575306\n",
      "Epoch 1385: train loss: 0.0044135223142802715\n",
      "Epoch 1386: train loss: 0.004410315304994583\n",
      "Epoch 1387: train loss: 0.004409623797982931\n",
      "Epoch 1388: train loss: 0.004409379325807095\n",
      "Epoch 1389: train loss: 0.004407038446515799\n",
      "Epoch 1390: train loss: 0.0044048624113202095\n",
      "Epoch 1391: train loss: 0.004404416773468256\n",
      "Epoch 1392: train loss: 0.004403336904942989\n",
      "Epoch 1393: train loss: 0.004401090554893017\n",
      "Epoch 1394: train loss: 0.0043996041640639305\n",
      "Epoch 1395: train loss: 0.004398914519697428\n",
      "Epoch 1396: train loss: 0.004397425334900618\n",
      "Epoch 1397: train loss: 0.004395469091832638\n",
      "Epoch 1398: train loss: 0.004394281189888716\n",
      "Epoch 1399: train loss: 0.004393292125314474\n",
      "Epoch 1400: train loss: 0.004391663242131472\n",
      "Epoch 1401: train loss: 0.004389979410916567\n",
      "Epoch 1402: train loss: 0.004388846457004547\n",
      "Epoch 1403: train loss: 0.004387637600302696\n",
      "Epoch 1404: train loss: 0.004386007785797119\n",
      "Epoch 1405: train loss: 0.0043845027685165405\n",
      "Epoch 1406: train loss: 0.00438331812620163\n",
      "Epoch 1407: train loss: 0.004381982143968344\n",
      "Epoch 1408: train loss: 0.004380400758236647\n",
      "Epoch 1409: train loss: 0.0043789842166006565\n",
      "Epoch 1410: train loss: 0.004377724602818489\n",
      "Epoch 1411: train loss: 0.004376320634037256\n",
      "Epoch 1412: train loss: 0.004374792333692312\n",
      "Epoch 1413: train loss: 0.004373399540781975\n",
      "Epoch 1414: train loss: 0.004372084513306618\n",
      "Epoch 1415: train loss: 0.004370647482573986\n",
      "Epoch 1416: train loss: 0.0043691545724868774\n",
      "Epoch 1417: train loss: 0.004367756191641092\n",
      "Epoch 1418: train loss: 0.004366394132375717\n",
      "Epoch 1419: train loss: 0.004364942200481892\n",
      "Epoch 1420: train loss: 0.004363462328910828\n",
      "Epoch 1421: train loss: 0.004362053703516722\n",
      "Epoch 1422: train loss: 0.004360649734735489\n",
      "Epoch 1423: train loss: 0.00435918802395463\n",
      "Epoch 1424: train loss: 0.004357712808996439\n",
      "Epoch 1425: train loss: 0.00435628229752183\n",
      "Epoch 1426: train loss: 0.004354848992079496\n",
      "Epoch 1427: train loss: 0.004353380296379328\n",
      "Epoch 1428: train loss: 0.004351900424808264\n",
      "Epoch 1429: train loss: 0.004350443370640278\n",
      "Epoch 1430: train loss: 0.004348988179117441\n",
      "Epoch 1431: train loss: 0.004347503185272217\n",
      "Epoch 1432: train loss: 0.00434601167216897\n",
      "Epoch 1433: train loss: 0.004344535060226917\n",
      "Epoch 1434: train loss: 0.004343054257333279\n",
      "Epoch 1435: train loss: 0.004341558553278446\n",
      "Epoch 1436: train loss: 0.004340049810707569\n",
      "Epoch 1437: train loss: 0.0043385517783463\n",
      "Epoch 1438: train loss: 0.00433705048635602\n",
      "Epoch 1439: train loss: 0.004335533827543259\n",
      "Epoch 1440: train loss: 0.004334008786827326\n",
      "Epoch 1441: train loss: 0.004332487005740404\n",
      "Epoch 1442: train loss: 0.004330961033701897\n",
      "Epoch 1443: train loss: 0.004329427611082792\n",
      "Epoch 1444: train loss: 0.0043278830125927925\n",
      "Epoch 1445: train loss: 0.004326339811086655\n",
      "Epoch 1446: train loss: 0.004324791487306356\n",
      "Epoch 1447: train loss: 0.004323234781622887\n",
      "Epoch 1448: train loss: 0.004321670159697533\n",
      "Epoch 1449: train loss: 0.004320099484175444\n",
      "Epoch 1450: train loss: 0.004318528342992067\n",
      "Epoch 1451: train loss: 0.0043169488199055195\n",
      "Epoch 1452: train loss: 0.004315361380577087\n",
      "Epoch 1453: train loss: 0.00431376788765192\n",
      "Epoch 1454: train loss: 0.004312172532081604\n",
      "Epoch 1455: train loss: 0.004310568794608116\n",
      "Epoch 1456: train loss: 0.0043089548125863075\n",
      "Epoch 1457: train loss: 0.004307338036596775\n",
      "Epoch 1458: train loss: 0.004305713810026646\n",
      "Epoch 1459: train loss: 0.0043040853925049305\n",
      "Epoch 1460: train loss: 0.004302447196096182\n",
      "Epoch 1461: train loss: 0.004300801083445549\n",
      "Epoch 1462: train loss: 0.004299153108149767\n",
      "Epoch 1463: train loss: 0.004297495819628239\n",
      "Epoch 1464: train loss: 0.0042958310805261135\n",
      "Epoch 1465: train loss: 0.004294159822165966\n",
      "Epoch 1466: train loss: 0.00429247971624136\n",
      "Epoch 1467: train loss: 0.004290794022381306\n",
      "Epoch 1468: train loss: 0.004289100877940655\n",
      "Epoch 1469: train loss: 0.004287398420274258\n",
      "Epoch 1470: train loss: 0.004285688046365976\n",
      "Epoch 1471: train loss: 0.0042839739471673965\n",
      "Epoch 1472: train loss: 0.004282249603420496\n",
      "Epoch 1473: train loss: 0.0042805178090929985\n",
      "Epoch 1474: train loss: 0.004278781823813915\n",
      "Epoch 1475: train loss: 0.0042770314030349255\n",
      "Epoch 1476: train loss: 0.004275278188288212\n",
      "Epoch 1477: train loss: 0.0042735133320093155\n",
      "Epoch 1478: train loss: 0.004271741956472397\n",
      "Epoch 1479: train loss: 0.004269963130354881\n",
      "Epoch 1480: train loss: 0.004268175456672907\n",
      "Epoch 1481: train loss: 0.004266379866749048\n",
      "Epoch 1482: train loss: 0.0042645749635994434\n",
      "Epoch 1483: train loss: 0.00426276121288538\n",
      "Epoch 1484: train loss: 0.004260937217622995\n",
      "Epoch 1485: train loss: 0.004259105771780014\n",
      "Epoch 1486: train loss: 0.004257265944033861\n",
      "Epoch 1487: train loss: 0.004255417734384537\n",
      "Epoch 1488: train loss: 0.004253560211509466\n",
      "Epoch 1489: train loss: 0.004251692909747362\n",
      "Epoch 1490: train loss: 0.0042498186230659485\n",
      "Epoch 1491: train loss: 0.004247929900884628\n",
      "Epoch 1492: train loss: 0.004246033728122711\n",
      "Epoch 1493: train loss: 0.004244130104780197\n",
      "Epoch 1494: train loss: 0.004242214374244213\n",
      "Epoch 1495: train loss: 0.00424028979614377\n",
      "Epoch 1496: train loss: 0.0042383549734950066\n",
      "Epoch 1497: train loss: 0.004236409906297922\n",
      "Epoch 1498: train loss: 0.004234455991536379\n",
      "Epoch 1499: train loss: 0.004232488572597504\n",
      "Epoch 1500: train loss: 0.004230514168739319\n",
      "Epoch 1501: train loss: 0.0042285313829779625\n",
      "Epoch 1502: train loss: 0.004226533696055412\n",
      "Epoch 1503: train loss: 0.004224526230245829\n",
      "Epoch 1504: train loss: 0.004222508054226637\n",
      "Epoch 1505: train loss: 0.004220481496304274\n",
      "Epoch 1506: train loss: 0.00421844283118844\n",
      "Epoch 1507: train loss: 0.004216388799250126\n",
      "Epoch 1508: train loss: 0.004214329179376364\n",
      "Epoch 1509: train loss: 0.004212255124002695\n",
      "Epoch 1510: train loss: 0.004210169892758131\n",
      "Epoch 1511: train loss: 0.004208078142255545\n",
      "Epoch 1512: train loss: 0.004205968696624041\n",
      "Epoch 1513: train loss: 0.004203847609460354\n",
      "Epoch 1514: train loss: 0.004201716743409634\n",
      "Epoch 1515: train loss: 0.004199574235826731\n",
      "Epoch 1516: train loss: 0.004197418689727783\n",
      "Epoch 1517: train loss: 0.0041952491737902164\n",
      "Epoch 1518: train loss: 0.0041930703446269035\n",
      "Epoch 1519: train loss: 0.004190876614302397\n",
      "Epoch 1520: train loss: 0.004188671242445707\n",
      "Epoch 1521: train loss: 0.004186453763395548\n",
      "Epoch 1522: train loss: 0.004184223245829344\n",
      "Epoch 1523: train loss: 0.004181978292763233\n",
      "Epoch 1524: train loss: 0.004179721232503653\n",
      "Epoch 1525: train loss: 0.004177451599389315\n",
      "Epoch 1526: train loss: 0.004175171256065369\n",
      "Epoch 1527: train loss: 0.004172872751951218\n",
      "Epoch 1528: train loss: 0.004170560743659735\n",
      "Epoch 1529: train loss: 0.0041682361625134945\n",
      "Epoch 1530: train loss: 0.0041658999398350716\n",
      "Epoch 1531: train loss: 0.004163549281656742\n",
      "Epoch 1532: train loss: 0.004161180462688208\n",
      "Epoch 1533: train loss: 0.004158800467848778\n",
      "Epoch 1534: train loss: 0.004156406037509441\n",
      "Epoch 1535: train loss: 0.004153997637331486\n",
      "Epoch 1536: train loss: 0.004151571076363325\n",
      "Epoch 1537: train loss: 0.0041491324082016945\n",
      "Epoch 1538: train loss: 0.004146677907556295\n",
      "Epoch 1539: train loss: 0.004144211299717426\n",
      "Epoch 1540: train loss: 0.004141724668443203\n",
      "Epoch 1541: train loss: 0.004139226861298084\n",
      "Epoch 1542: train loss: 0.004136711359024048\n",
      "Epoch 1543: train loss: 0.004134180024266243\n",
      "Epoch 1544: train loss: 0.00413163285702467\n",
      "Epoch 1545: train loss: 0.004129070322960615\n",
      "Epoch 1546: train loss: 0.00412649055942893\n",
      "Epoch 1547: train loss: 0.004123895429074764\n",
      "Epoch 1548: train loss: 0.004121283069252968\n",
      "Epoch 1549: train loss: 0.004118654876947403\n",
      "Epoch 1550: train loss: 0.004116009920835495\n",
      "Epoch 1551: train loss: 0.004113347269594669\n",
      "Epoch 1552: train loss: 0.004110667854547501\n",
      "Epoch 1553: train loss: 0.004107972607016563\n",
      "Epoch 1554: train loss: 0.004105258267372847\n",
      "Epoch 1555: train loss: 0.004102529026567936\n",
      "Epoch 1556: train loss: 0.004099780227988958\n",
      "Epoch 1557: train loss: 0.004097010008990765\n",
      "Epoch 1558: train loss: 0.004094227682799101\n",
      "Epoch 1559: train loss: 0.004091425333172083\n",
      "Epoch 1560: train loss: 0.004088603891432285\n",
      "Epoch 1561: train loss: 0.004085766151547432\n",
      "Epoch 1562: train loss: 0.004082906525582075\n",
      "Epoch 1563: train loss: 0.004080029670149088\n",
      "Epoch 1564: train loss: 0.004077132325619459\n",
      "Epoch 1565: train loss: 0.004074216820299625\n",
      "Epoch 1566: train loss: 0.004071282688528299\n",
      "Epoch 1567: train loss: 0.004068327601999044\n",
      "Epoch 1568: train loss: 0.004065356682986021\n",
      "Epoch 1569: train loss: 0.004062359221279621\n",
      "Epoch 1570: train loss: 0.00405934639275074\n",
      "Epoch 1571: train loss: 0.004056309815496206\n",
      "Epoch 1572: train loss: 0.0040532550774514675\n",
      "Epoch 1573: train loss: 0.004050182178616524\n",
      "Epoch 1574: train loss: 0.004047085531055927\n",
      "Epoch 1575: train loss: 0.004043968860059977\n",
      "Epoch 1576: train loss: 0.00404083076864481\n",
      "Epoch 1577: train loss: 0.004037671722471714\n",
      "Epoch 1578: train loss: 0.004034491255879402\n",
      "Epoch 1579: train loss: 0.004031287040561438\n",
      "Epoch 1580: train loss: 0.0040280623361468315\n",
      "Epoch 1581: train loss: 0.004024815279990435\n",
      "Epoch 1582: train loss: 0.0040215435437858105\n",
      "Epoch 1583: train loss: 0.004018254578113556\n",
      "Epoch 1584: train loss: 0.004014941398054361\n",
      "Epoch 1585: train loss: 0.00401160167530179\n",
      "Epoch 1586: train loss: 0.004008242394775152\n",
      "Epoch 1587: train loss: 0.004004857502877712\n",
      "Epoch 1588: train loss: 0.004001447930932045\n",
      "Epoch 1589: train loss: 0.003998020198196173\n",
      "Epoch 1590: train loss: 0.003994564991444349\n",
      "Epoch 1591: train loss: 0.003991083241999149\n",
      "Epoch 1592: train loss: 0.003987579140812159\n",
      "Epoch 1593: train loss: 0.0039840517565608025\n",
      "Epoch 1594: train loss: 0.003980499226599932\n",
      "Epoch 1595: train loss: 0.003976919688284397\n",
      "Epoch 1596: train loss: 0.0039733173325657845\n",
      "Epoch 1597: train loss: 0.0039696888998150826\n",
      "Epoch 1598: train loss: 0.003966029267758131\n",
      "Epoch 1599: train loss: 0.0039623505435884\n",
      "Epoch 1600: train loss: 0.003958644345402718\n",
      "Epoch 1601: train loss: 0.003954910207539797\n",
      "Epoch 1602: train loss: 0.003951151855289936\n",
      "Epoch 1603: train loss: 0.0039473664946854115\n",
      "Epoch 1604: train loss: 0.003943550866097212\n",
      "Epoch 1605: train loss: 0.003939708694815636\n",
      "Epoch 1606: train loss: 0.003935841377824545\n",
      "Epoch 1607: train loss: 0.003931945655494928\n",
      "Epoch 1608: train loss: 0.003928020596504211\n",
      "Epoch 1609: train loss: 0.003924067597836256\n",
      "Epoch 1610: train loss: 0.003920086659491062\n",
      "Epoch 1611: train loss: 0.003916074521839619\n",
      "Epoch 1612: train loss: 0.003912037704139948\n",
      "Epoch 1613: train loss: 0.0039079682901501656\n",
      "Epoch 1614: train loss: 0.0039038711693137884\n",
      "Epoch 1615: train loss: 0.0038997423835098743\n",
      "Epoch 1616: train loss: 0.003895585658028722\n",
      "Epoch 1617: train loss: 0.0038913970347493887\n",
      "Epoch 1618: train loss: 0.003887177910655737\n",
      "Epoch 1619: train loss: 0.0038829296827316284\n",
      "Epoch 1620: train loss: 0.003878652350977063\n",
      "Epoch 1621: train loss: 0.0038743389304727316\n",
      "Epoch 1622: train loss: 0.0038699950091540813\n",
      "Epoch 1623: train loss: 0.003865620121359825\n",
      "Epoch 1624: train loss: 0.003861214267089963\n",
      "Epoch 1625: train loss: 0.00385677395388484\n",
      "Epoch 1626: train loss: 0.0038523059338331223\n",
      "Epoch 1627: train loss: 0.0038478050846606493\n",
      "Epoch 1628: train loss: 0.0038432797882705927\n",
      "Epoch 1629: train loss: 0.0038387333042919636\n",
      "Epoch 1630: train loss: 0.0038341882172971964\n",
      "Epoch 1631: train loss: 0.0038296906277537346\n",
      "Epoch 1632: train loss: 0.0038253702223300934\n",
      "Epoch 1633: train loss: 0.0038215310778468847\n",
      "Epoch 1634: train loss: 0.0038189846090972424\n",
      "Epoch 1635: train loss: 0.0038198470138013363\n",
      "Epoch 1636: train loss: 0.0038297188002616167\n",
      "Epoch 1637: train loss: 0.0038636757526546717\n",
      "Epoch 1638: train loss: 0.00396234355866909\n",
      "Epoch 1639: train loss: 0.004234642256051302\n",
      "Epoch 1640: train loss: 0.004955064970999956\n",
      "Epoch 1641: train loss: 0.006700345780700445\n",
      "Epoch 1642: train loss: 0.010083540342748165\n",
      "Epoch 1643: train loss: 0.013010336086153984\n",
      "Epoch 1644: train loss: 0.011306598782539368\n",
      "Epoch 1645: train loss: 0.004909023642539978\n",
      "Epoch 1646: train loss: 0.005605646874755621\n",
      "Epoch 1647: train loss: 0.008806322701275349\n",
      "Epoch 1648: train loss: 0.004682306200265884\n",
      "Epoch 1649: train loss: 0.005450384225696325\n",
      "Epoch 1650: train loss: 0.006962689571082592\n",
      "Epoch 1651: train loss: 0.0040152124129235744\n",
      "Epoch 1652: train loss: 0.006094819400459528\n",
      "Epoch 1653: train loss: 0.005087637808173895\n",
      "Epoch 1654: train loss: 0.004412181209772825\n",
      "Epoch 1655: train loss: 0.005631320644170046\n",
      "Epoch 1656: train loss: 0.003891922999173403\n",
      "Epoch 1657: train loss: 0.005266177002340555\n",
      "Epoch 1658: train loss: 0.004058764316141605\n",
      "Epoch 1659: train loss: 0.004661924205720425\n",
      "Epoch 1660: train loss: 0.004380426835268736\n",
      "Epoch 1661: train loss: 0.004196164198219776\n",
      "Epoch 1662: train loss: 0.004524949938058853\n",
      "Epoch 1663: train loss: 0.003911877982318401\n",
      "Epoch 1664: train loss: 0.004510416649281979\n",
      "Epoch 1665: train loss: 0.003876778297126293\n",
      "Epoch 1666: train loss: 0.004400605335831642\n",
      "Epoch 1667: train loss: 0.003885207697749138\n",
      "Epoch 1668: train loss: 0.004243474453687668\n",
      "Epoch 1669: train loss: 0.003981448244303465\n",
      "Epoch 1670: train loss: 0.0040910011157393456\n",
      "Epoch 1671: train loss: 0.004005153197795153\n",
      "Epoch 1672: train loss: 0.003985421732068062\n",
      "Epoch 1673: train loss: 0.004056484438478947\n",
      "Epoch 1674: train loss: 0.0039161983877420425\n",
      "Epoch 1675: train loss: 0.003992702811956406\n",
      "Epoch 1676: train loss: 0.003903644857928157\n",
      "Epoch 1677: train loss: 0.003963693976402283\n",
      "Epoch 1678: train loss: 0.003906112862750888\n",
      "Epoch 1679: train loss: 0.0038575923535972834\n",
      "Epoch 1680: train loss: 0.003920217044651508\n",
      "Epoch 1681: train loss: 0.0037973516155034304\n",
      "Epoch 1682: train loss: 0.003908947110176086\n",
      "Epoch 1683: train loss: 0.0037416100967675447\n",
      "Epoch 1684: train loss: 0.0038697526324540377\n",
      "Epoch 1685: train loss: 0.0037226686254143715\n",
      "Epoch 1686: train loss: 0.0038141754921525717\n",
      "Epoch 1687: train loss: 0.003733877558261156\n",
      "Epoch 1688: train loss: 0.003745939349755645\n",
      "Epoch 1689: train loss: 0.0037485728971660137\n",
      "Epoch 1690: train loss: 0.003695444669574499\n",
      "Epoch 1691: train loss: 0.003750547766685486\n",
      "Epoch 1692: train loss: 0.0036599936429411173\n",
      "Epoch 1693: train loss: 0.0037331602070480585\n",
      "Epoch 1694: train loss: 0.003648580051958561\n",
      "Epoch 1695: train loss: 0.0036982980091124773\n",
      "Epoch 1696: train loss: 0.003653359366580844\n",
      "Epoch 1697: train loss: 0.003658617613837123\n",
      "Epoch 1698: train loss: 0.0036557915154844522\n",
      "Epoch 1699: train loss: 0.003629811340942979\n",
      "Epoch 1700: train loss: 0.0036495972890406847\n",
      "Epoch 1701: train loss: 0.0036134575493633747\n",
      "Epoch 1702: train loss: 0.003631503554061055\n",
      "Epoch 1703: train loss: 0.0036073930095881224\n",
      "Epoch 1704: train loss: 0.003608895931392908\n",
      "Epoch 1705: train loss: 0.0036029971670359373\n",
      "Epoch 1706: train loss: 0.0035902082454413176\n",
      "Epoch 1707: train loss: 0.0035922182723879814\n",
      "Epoch 1708: train loss: 0.0035785329528152943\n",
      "Epoch 1709: train loss: 0.00357731687836349\n",
      "Epoch 1710: train loss: 0.0035698460415005684\n",
      "Epoch 1711: train loss: 0.0035626464523375034\n",
      "Epoch 1712: train loss: 0.003559176344424486\n",
      "Epoch 1713: train loss: 0.003551652655005455\n",
      "Epoch 1714: train loss: 0.003545360406860709\n",
      "Epoch 1715: train loss: 0.0035427866969257593\n",
      "Epoch 1716: train loss: 0.0035317898727953434\n",
      "Epoch 1717: train loss: 0.0035322862677276134\n",
      "Epoch 1718: train loss: 0.003521072445437312\n",
      "Epoch 1719: train loss: 0.0035190361086279154\n",
      "Epoch 1720: train loss: 0.003512223483994603\n",
      "Epoch 1721: train loss: 0.0035055449698120356\n",
      "Epoch 1722: train loss: 0.003502303035929799\n",
      "Epoch 1723: train loss: 0.003494125558063388\n",
      "Epoch 1724: train loss: 0.0034902640618383884\n",
      "Epoch 1725: train loss: 0.0034840204752981663\n",
      "Epoch 1726: train loss: 0.0034781235735863447\n",
      "Epoch 1727: train loss: 0.0034729831386357546\n",
      "Epoch 1728: train loss: 0.003467218717560172\n",
      "Epoch 1729: train loss: 0.0034609572030603886\n",
      "Epoch 1730: train loss: 0.0034565599635243416\n",
      "Epoch 1731: train loss: 0.0034493678249418736\n",
      "Epoch 1732: train loss: 0.003444904228672385\n",
      "Epoch 1733: train loss: 0.0034386005718261003\n",
      "Epoch 1734: train loss: 0.0034328082110732794\n",
      "Epoch 1735: train loss: 0.0034275255165994167\n",
      "Epoch 1736: train loss: 0.003421325469389558\n",
      "Epoch 1737: train loss: 0.0034156988840550184\n",
      "Epoch 1738: train loss: 0.0034100888296961784\n",
      "Epoch 1739: train loss: 0.0034039632882922888\n",
      "Epoch 1740: train loss: 0.003398339031264186\n",
      "Epoch 1741: train loss: 0.003392561338841915\n",
      "Epoch 1742: train loss: 0.0033863817807286978\n",
      "Epoch 1743: train loss: 0.0033809023443609476\n",
      "Epoch 1744: train loss: 0.003374704858288169\n",
      "Epoch 1745: train loss: 0.003368883393704891\n",
      "Epoch 1746: train loss: 0.003362999763339758\n",
      "Epoch 1747: train loss: 0.003356964560225606\n",
      "Epoch 1748: train loss: 0.0033509554341435432\n",
      "Epoch 1749: train loss: 0.0033451106864959\n",
      "Epoch 1750: train loss: 0.003338888753205538\n",
      "Epoch 1751: train loss: 0.003332996740937233\n",
      "Epoch 1752: train loss: 0.003326908452436328\n",
      "Epoch 1753: train loss: 0.003320778952911496\n",
      "Epoch 1754: train loss: 0.0033147260546684265\n",
      "Epoch 1755: train loss: 0.0033086284529417753\n",
      "Epoch 1756: train loss: 0.003302411874756217\n",
      "Epoch 1757: train loss: 0.003296336391940713\n",
      "Epoch 1758: train loss: 0.003290127729997039\n",
      "Epoch 1759: train loss: 0.0032839064951986074\n",
      "Epoch 1760: train loss: 0.003277730429545045\n",
      "Epoch 1761: train loss: 0.0032714817207306623\n",
      "Epoch 1762: train loss: 0.0032651990186423063\n",
      "Epoch 1763: train loss: 0.0032589593902230263\n",
      "Epoch 1764: train loss: 0.0032526524737477303\n",
      "Epoch 1765: train loss: 0.0032463192474097013\n",
      "Epoch 1766: train loss: 0.0032400137279182673\n",
      "Epoch 1767: train loss: 0.003233651863411069\n",
      "Epoch 1768: train loss: 0.0032272704411298037\n",
      "Epoch 1769: train loss: 0.0032208955381065607\n",
      "Epoch 1770: train loss: 0.0032144871074706316\n",
      "Epoch 1771: train loss: 0.0032080456148833036\n",
      "Epoch 1772: train loss: 0.003201617393642664\n",
      "Epoch 1773: train loss: 0.003195153083652258\n",
      "Epoch 1774: train loss: 0.0031886654905974865\n",
      "Epoch 1775: train loss: 0.003182174637913704\n",
      "Epoch 1776: train loss: 0.003175664460286498\n",
      "Epoch 1777: train loss: 0.003169122152030468\n",
      "Epoch 1778: train loss: 0.003162580542266369\n",
      "Epoch 1779: train loss: 0.003156018443405628\n",
      "Epoch 1780: train loss: 0.0031494328286498785\n",
      "Epoch 1781: train loss: 0.003142834873870015\n",
      "Epoch 1782: train loss: 0.0031362285371869802\n",
      "Epoch 1783: train loss: 0.003129593562334776\n",
      "Epoch 1784: train loss: 0.0031229471787810326\n",
      "Epoch 1785: train loss: 0.003116290085017681\n",
      "Epoch 1786: train loss: 0.0031096162274479866\n",
      "Epoch 1787: train loss: 0.003102922346442938\n",
      "Epoch 1788: train loss: 0.003096220549196005\n",
      "Epoch 1789: train loss: 0.003089503850787878\n",
      "Epoch 1790: train loss: 0.0030827668961137533\n",
      "Epoch 1791: train loss: 0.0030760210938751698\n",
      "Epoch 1792: train loss: 0.0030692615546286106\n",
      "Epoch 1793: train loss: 0.003062488976866007\n",
      "Epoch 1794: train loss: 0.0030557005666196346\n",
      "Epoch 1795: train loss: 0.003048904240131378\n",
      "Epoch 1796: train loss: 0.0030420885886996984\n",
      "Epoch 1797: train loss: 0.003035265486687422\n",
      "Epoch 1798: train loss: 0.003028429811820388\n",
      "Epoch 1799: train loss: 0.0030215808656066656\n",
      "Epoch 1800: train loss: 0.0030147223733365536\n",
      "Epoch 1801: train loss: 0.003007852239534259\n",
      "Epoch 1802: train loss: 0.0030009704641997814\n",
      "Epoch 1803: train loss: 0.0029940784443169832\n",
      "Epoch 1804: train loss: 0.002987175714224577\n",
      "Epoch 1805: train loss: 0.002980261342599988\n",
      "Epoch 1806: train loss: 0.0029733399860560894\n",
      "Epoch 1807: train loss: 0.0029664074536412954\n",
      "Epoch 1808: train loss: 0.002959465142339468\n",
      "Epoch 1809: train loss: 0.0029525132849812508\n",
      "Epoch 1810: train loss: 0.0029455521143972874\n",
      "Epoch 1811: train loss: 0.0029385839588940144\n",
      "Epoch 1812: train loss: 0.0029316055588424206\n",
      "Epoch 1813: train loss: 0.00292461970821023\n",
      "Epoch 1814: train loss: 0.0029176264069974422\n",
      "Epoch 1815: train loss: 0.0029106251895427704\n",
      "Epoch 1816: train loss: 0.002903616288676858\n",
      "Epoch 1817: train loss: 0.002896600868552923\n",
      "Epoch 1818: train loss: 0.002889577066525817\n",
      "Epoch 1819: train loss: 0.002882546279579401\n",
      "Epoch 1820: train loss: 0.002875508740544319\n",
      "Epoch 1821: train loss: 0.002868467476218939\n",
      "Epoch 1822: train loss: 0.0028614162001758814\n",
      "Epoch 1823: train loss: 0.0028543605003505945\n",
      "Epoch 1824: train loss: 0.002847300609573722\n",
      "Epoch 1825: train loss: 0.002840233501046896\n",
      "Epoch 1826: train loss: 0.002833161037415266\n",
      "Epoch 1827: train loss: 0.0028260836843401194\n",
      "Epoch 1828: train loss: 0.002819004701450467\n",
      "Epoch 1829: train loss: 0.0028119152411818504\n",
      "Epoch 1830: train loss: 0.0028048232197761536\n",
      "Epoch 1831: train loss: 0.0027977298013865948\n",
      "Epoch 1832: train loss: 0.0027906312607228756\n",
      "Epoch 1833: train loss: 0.002783535746857524\n",
      "Epoch 1834: train loss: 0.002776444423943758\n",
      "Epoch 1835: train loss: 0.002769384067505598\n",
      "Epoch 1836: train loss: 0.002762396587058902\n",
      "Epoch 1837: train loss: 0.0027556277345865965\n",
      "Epoch 1838: train loss: 0.0027494686655700207\n",
      "Epoch 1839: train loss: 0.0027450798079371452\n",
      "Epoch 1840: train loss: 0.0027458735276013613\n",
      "Epoch 1841: train loss: 0.0027626249939203262\n",
      "Epoch 1842: train loss: 0.0028270413167774677\n",
      "Epoch 1843: train loss: 0.003045802004635334\n",
      "Epoch 1844: train loss: 0.0036822378169745207\n",
      "Epoch 1845: train loss: 0.005514948628842831\n",
      "Epoch 1846: train loss: 0.008271424099802971\n",
      "Epoch 1847: train loss: 0.010303300805389881\n",
      "Epoch 1848: train loss: 0.006031758151948452\n",
      "Epoch 1849: train loss: 0.003162872977554798\n",
      "Epoch 1850: train loss: 0.006487121339887381\n",
      "Epoch 1851: train loss: 0.005775074940174818\n",
      "Epoch 1852: train loss: 0.0032741911709308624\n",
      "Epoch 1853: train loss: 0.006151363719254732\n",
      "Epoch 1854: train loss: 0.00435736496001482\n",
      "Epoch 1855: train loss: 0.003946929704397917\n",
      "Epoch 1856: train loss: 0.005176129285246134\n",
      "Epoch 1857: train loss: 0.003126380266621709\n",
      "Epoch 1858: train loss: 0.004603235982358456\n",
      "Epoch 1859: train loss: 0.003360397182404995\n",
      "Epoch 1860: train loss: 0.0037143651861697435\n",
      "Epoch 1861: train loss: 0.003948834724724293\n",
      "Epoch 1862: train loss: 0.003071374958381057\n",
      "Epoch 1863: train loss: 0.0038238507695496082\n",
      "Epoch 1864: train loss: 0.003102135146036744\n",
      "Epoch 1865: train loss: 0.003297914285212755\n",
      "Epoch 1866: train loss: 0.003439007792621851\n",
      "Epoch 1867: train loss: 0.002913202391937375\n",
      "Epoch 1868: train loss: 0.003474896540865302\n",
      "Epoch 1869: train loss: 0.00279570696875453\n",
      "Epoch 1870: train loss: 0.0032595775555819273\n",
      "Epoch 1871: train loss: 0.002899142913520336\n",
      "Epoch 1872: train loss: 0.002950086956843734\n",
      "Epoch 1873: train loss: 0.0030131973326206207\n",
      "Epoch 1874: train loss: 0.0027680955827236176\n",
      "Epoch 1875: train loss: 0.0029810499399900436\n",
      "Epoch 1876: train loss: 0.002801342634484172\n",
      "Epoch 1877: train loss: 0.0027615318540483713\n",
      "Epoch 1878: train loss: 0.00289709959179163\n",
      "Epoch 1879: train loss: 0.0026357739698141813\n",
      "Epoch 1880: train loss: 0.002842515241354704\n",
      "Epoch 1881: train loss: 0.0026949038729071617\n",
      "Epoch 1882: train loss: 0.002676331205293536\n",
      "Epoch 1883: train loss: 0.002742280252277851\n",
      "Epoch 1884: train loss: 0.0026254490949213505\n",
      "Epoch 1885: train loss: 0.002665529726073146\n",
      "Epoch 1886: train loss: 0.0026789891999214888\n",
      "Epoch 1887: train loss: 0.0025761951692402363\n",
      "Epoch 1888: train loss: 0.0026723002083599567\n",
      "Epoch 1889: train loss: 0.0025868513621389866\n",
      "Epoch 1890: train loss: 0.0025869247037917376\n",
      "Epoch 1891: train loss: 0.002613415475934744\n",
      "Epoch 1892: train loss: 0.002554540755227208\n",
      "Epoch 1893: train loss: 0.0025649836752563715\n",
      "Epoch 1894: train loss: 0.0025748673360794783\n",
      "Epoch 1895: train loss: 0.0025192024186253548\n",
      "Epoch 1896: train loss: 0.0025540434289723635\n",
      "Epoch 1897: train loss: 0.0025291580241173506\n",
      "Epoch 1898: train loss: 0.002508048666641116\n",
      "Epoch 1899: train loss: 0.002522173570469022\n",
      "Epoch 1900: train loss: 0.002503035357221961\n",
      "Epoch 1901: train loss: 0.0024858054239302874\n",
      "Epoch 1902: train loss: 0.002499914728105068\n",
      "Epoch 1903: train loss: 0.00247522396966815\n",
      "Epoch 1904: train loss: 0.002469990635290742\n",
      "Epoch 1905: train loss: 0.002473310101777315\n",
      "Epoch 1906: train loss: 0.0024552454706281424\n",
      "Epoch 1907: train loss: 0.0024497152771800756\n",
      "Epoch 1908: train loss: 0.0024510715156793594\n",
      "Epoch 1909: train loss: 0.0024349524173885584\n",
      "Epoch 1910: train loss: 0.002430397318676114\n",
      "Epoch 1911: train loss: 0.0024293167516589165\n",
      "Epoch 1912: train loss: 0.002415861701592803\n",
      "Epoch 1913: train loss: 0.002410818822681904\n",
      "Epoch 1914: train loss: 0.002408028580248356\n",
      "Epoch 1915: train loss: 0.0023981183767318726\n",
      "Epoch 1916: train loss: 0.0023904747795313597\n",
      "Epoch 1917: train loss: 0.0023886519484221935\n",
      "Epoch 1918: train loss: 0.0023791741114109755\n",
      "Epoch 1919: train loss: 0.0023721004836261272\n",
      "Epoch 1920: train loss: 0.0023682662285864353\n",
      "Epoch 1921: train loss: 0.0023617357946932316\n",
      "Epoch 1922: train loss: 0.0023530011530965567\n",
      "Epoch 1923: train loss: 0.002349217189475894\n",
      "Epoch 1924: train loss: 0.0023432583548128605\n",
      "Epoch 1925: train loss: 0.0023352608550339937\n",
      "Epoch 1926: train loss: 0.0023298277519643307\n",
      "Epoch 1927: train loss: 0.0023248896468430758\n",
      "Epoch 1928: train loss: 0.002317700767889619\n",
      "Epoch 1929: train loss: 0.002311060205101967\n",
      "Epoch 1930: train loss: 0.0023063430562615395\n",
      "Epoch 1931: train loss: 0.002299877814948559\n",
      "Epoch 1932: train loss: 0.002293245866894722\n",
      "Epoch 1933: train loss: 0.0022873326670378447\n",
      "Epoch 1934: train loss: 0.0022820772137492895\n",
      "Epoch 1935: train loss: 0.0022754070814698935\n",
      "Epoch 1936: train loss: 0.0022691627964377403\n",
      "Epoch 1937: train loss: 0.00226350175216794\n",
      "Epoch 1938: train loss: 0.002257776679471135\n",
      "Epoch 1939: train loss: 0.0022513088770210743\n",
      "Epoch 1940: train loss: 0.0022451833356171846\n",
      "Epoch 1941: train loss: 0.0022396170534193516\n",
      "Epoch 1942: train loss: 0.00223358953371644\n",
      "Epoch 1943: train loss: 0.0022273713257163763\n",
      "Epoch 1944: train loss: 0.0022212862968444824\n",
      "Epoch 1945: train loss: 0.002215601271018386\n",
      "Epoch 1946: train loss: 0.002209597500041127\n",
      "Epoch 1947: train loss: 0.002203472424298525\n",
      "Epoch 1948: train loss: 0.002197396708652377\n",
      "Epoch 1949: train loss: 0.0021915778052061796\n",
      "Epoch 1950: train loss: 0.00218567973934114\n",
      "Epoch 1951: train loss: 0.002179570961743593\n",
      "Epoch 1952: train loss: 0.0021735243499279022\n",
      "Epoch 1953: train loss: 0.002167588099837303\n",
      "Epoch 1954: train loss: 0.0021617047023028135\n",
      "Epoch 1955: train loss: 0.002155695343390107\n",
      "Epoch 1956: train loss: 0.0021496396511793137\n",
      "Epoch 1957: train loss: 0.0021436342503875494\n",
      "Epoch 1958: train loss: 0.002137699630111456\n",
      "Epoch 1959: train loss: 0.0021317689679563046\n",
      "Epoch 1960: train loss: 0.002125741448253393\n",
      "Epoch 1961: train loss: 0.0021197183523327112\n",
      "Epoch 1962: train loss: 0.002113708993420005\n",
      "Epoch 1963: train loss: 0.002107750391587615\n",
      "Epoch 1964: train loss: 0.0021017848048359156\n",
      "Epoch 1965: train loss: 0.0020957808010280132\n",
      "Epoch 1966: train loss: 0.0020897574722766876\n",
      "Epoch 1967: train loss: 0.002083733445033431\n",
      "Epoch 1968: train loss: 0.00207774736918509\n",
      "Epoch 1969: train loss: 0.0020717547740787268\n",
      "Epoch 1970: train loss: 0.00206576706841588\n",
      "Epoch 1971: train loss: 0.002059751423075795\n",
      "Epoch 1972: train loss: 0.002053722972050309\n",
      "Epoch 1973: train loss: 0.002047700108960271\n",
      "Epoch 1974: train loss: 0.0020416812039911747\n",
      "Epoch 1975: train loss: 0.002035672077909112\n",
      "Epoch 1976: train loss: 0.002029658295214176\n",
      "Epoch 1977: train loss: 0.002023643348366022\n",
      "Epoch 1978: train loss: 0.002017613733187318\n",
      "Epoch 1979: train loss: 0.002011579694226384\n",
      "Epoch 1980: train loss: 0.0020055437926203012\n",
      "Epoch 1981: train loss: 0.001999503467231989\n",
      "Epoch 1982: train loss: 0.0019934659358114004\n",
      "Epoch 1983: train loss: 0.0019874279387295246\n",
      "Epoch 1984: train loss: 0.001981389243155718\n",
      "Epoch 1985: train loss: 0.0019753514789044857\n",
      "Epoch 1986: train loss: 0.0019693048670887947\n",
      "Epoch 1987: train loss: 0.001963261980563402\n",
      "Epoch 1988: train loss: 0.0019572111777961254\n",
      "Epoch 1989: train loss: 0.0019511569989845157\n",
      "Epoch 1990: train loss: 0.0019451016560196877\n",
      "Epoch 1991: train loss: 0.0019390450324863195\n",
      "Epoch 1992: train loss: 0.0019329875940456986\n",
      "Epoch 1993: train loss: 0.0019269248005002737\n",
      "Epoch 1994: train loss: 0.0019208693411201239\n",
      "Epoch 1995: train loss: 0.001914809225127101\n",
      "Epoch 1996: train loss: 0.0019087607506662607\n",
      "Epoch 1997: train loss: 0.0019027210073545575\n",
      "Epoch 1998: train loss: 0.0018967093201354146\n",
      "Epoch 1999: train loss: 0.001890745130367577\n",
      "Epoch 2000: train loss: 0.001884888974018395\n",
      "Epoch 2001: train loss: 0.0018792347982525826\n",
      "Epoch 2002: train loss: 0.0018740141531452537\n",
      "Epoch 2003: train loss: 0.001869712839834392\n",
      "Epoch 2004: train loss: 0.0018674284219741821\n",
      "Epoch 2005: train loss: 0.0018696468323469162\n",
      "Epoch 2006: train loss: 0.0018821414560079575\n",
      "Epoch 2007: train loss: 0.0019185388227924705\n",
      "Epoch 2008: train loss: 0.0020101559348404408\n",
      "Epoch 2009: train loss: 0.0022283955477178097\n",
      "Epoch 2010: train loss: 0.0027038059197366238\n",
      "Epoch 2011: train loss: 0.003605353645980358\n",
      "Epoch 2012: train loss: 0.004706575069576502\n",
      "Epoch 2013: train loss: 0.005088555160909891\n",
      "Epoch 2014: train loss: 0.0036154547706246376\n",
      "Epoch 2015: train loss: 0.001978103071451187\n",
      "Epoch 2016: train loss: 0.002323445398360491\n",
      "Epoch 2017: train loss: 0.0033044922165572643\n",
      "Epoch 2018: train loss: 0.00266137789003551\n",
      "Epoch 2019: train loss: 0.0018625302473083138\n",
      "Epoch 2020: train loss: 0.002547311130911112\n",
      "Epoch 2021: train loss: 0.00267227366566658\n",
      "Epoch 2022: train loss: 0.0019030180992558599\n",
      "Epoch 2023: train loss: 0.0022376489359885454\n",
      "Epoch 2024: train loss: 0.0024937144480645657\n",
      "Epoch 2025: train loss: 0.001921404036693275\n",
      "Epoch 2026: train loss: 0.0020645936019718647\n",
      "Epoch 2027: train loss: 0.002316896803677082\n",
      "Epoch 2028: train loss: 0.0019136684713885188\n",
      "Epoch 2029: train loss: 0.0019346610642969608\n",
      "Epoch 2030: train loss: 0.0021737474016845226\n",
      "Epoch 2031: train loss: 0.0018900971626862884\n",
      "Epoch 2032: train loss: 0.0018197647295892239\n",
      "Epoch 2033: train loss: 0.0020454823970794678\n",
      "Epoch 2034: train loss: 0.0018878369592130184\n",
      "Epoch 2035: train loss: 0.0017473562620580196\n",
      "Epoch 2036: train loss: 0.0018991503166034818\n",
      "Epoch 2037: train loss: 0.0018987096846103668\n",
      "Epoch 2038: train loss: 0.001751256757415831\n",
      "Epoch 2039: train loss: 0.001750229625031352\n",
      "Epoch 2040: train loss: 0.0018545036436989903\n",
      "Epoch 2041: train loss: 0.0018075167899951339\n",
      "Epoch 2042: train loss: 0.0016915849409997463\n",
      "Epoch 2043: train loss: 0.0017384373350068927\n",
      "Epoch 2044: train loss: 0.0017961330013349652\n",
      "Epoch 2045: train loss: 0.0017412371234968305\n",
      "Epoch 2046: train loss: 0.0016739587299525738\n",
      "Epoch 2047: train loss: 0.0016899537295103073\n",
      "Epoch 2048: train loss: 0.0017401236109435558\n",
      "Epoch 2049: train loss: 0.0017057742225006223\n",
      "Epoch 2050: train loss: 0.0016505148960277438\n",
      "Epoch 2051: train loss: 0.0016501095378771424\n",
      "Epoch 2052: train loss: 0.001674646744504571\n",
      "Epoch 2053: train loss: 0.0016747198533266783\n",
      "Epoch 2054: train loss: 0.0016341631999239326\n",
      "Epoch 2055: train loss: 0.0016132040182128549\n",
      "Epoch 2056: train loss: 0.0016299292910844088\n",
      "Epoch 2057: train loss: 0.0016354233957827091\n",
      "Epoch 2058: train loss: 0.001617691945284605\n",
      "Epoch 2059: train loss: 0.0015928610228002071\n",
      "Epoch 2060: train loss: 0.001589081366546452\n",
      "Epoch 2061: train loss: 0.0015999239403754473\n",
      "Epoch 2062: train loss: 0.0015931458910927176\n",
      "Epoch 2063: train loss: 0.0015752604231238365\n",
      "Epoch 2064: train loss: 0.0015629406552761793\n",
      "Epoch 2065: train loss: 0.0015621362254023552\n",
      "Epoch 2066: train loss: 0.00156513846013695\n",
      "Epoch 2067: train loss: 0.001556331291794777\n",
      "Epoch 2068: train loss: 0.0015421630814671516\n",
      "Epoch 2069: train loss: 0.0015346920117735863\n",
      "Epoch 2070: train loss: 0.0015325970016419888\n",
      "Epoch 2071: train loss: 0.0015314504271373153\n",
      "Epoch 2072: train loss: 0.0015239949570968747\n",
      "Epoch 2073: train loss: 0.0015128443483263254\n",
      "Epoch 2074: train loss: 0.0015058330027386546\n",
      "Epoch 2075: train loss: 0.0015021278522908688\n",
      "Epoch 2076: train loss: 0.0014993012882769108\n",
      "Epoch 2077: train loss: 0.0014940343098714948\n",
      "Epoch 2078: train loss: 0.0014853256288915873\n",
      "Epoch 2079: train loss: 0.0014777393080294132\n",
      "Epoch 2080: train loss: 0.001472350093536079\n",
      "Epoch 2081: train loss: 0.001468184287659824\n",
      "Epoch 2082: train loss: 0.0014641659799963236\n",
      "Epoch 2083: train loss: 0.0014581213472411036\n",
      "Epoch 2084: train loss: 0.0014509425964206457\n",
      "Epoch 2085: train loss: 0.00144438655115664\n",
      "Epoch 2086: train loss: 0.0014385764952749014\n",
      "Epoch 2087: train loss: 0.0014338112669065595\n",
      "Epoch 2088: train loss: 0.0014292570995166898\n",
      "Epoch 2089: train loss: 0.0014238288858905435\n",
      "Epoch 2090: train loss: 0.0014178642304614186\n",
      "Epoch 2091: train loss: 0.0014116147067397833\n",
      "Epoch 2092: train loss: 0.0014054413186386228\n",
      "Epoch 2093: train loss: 0.0013999303337186575\n",
      "Epoch 2094: train loss: 0.001394764636643231\n",
      "Epoch 2095: train loss: 0.001389629440382123\n",
      "Epoch 2096: train loss: 0.0013844837667420506\n",
      "Epoch 2097: train loss: 0.0013789772056043148\n",
      "Epoch 2098: train loss: 0.0013732045190408826\n",
      "Epoch 2099: train loss: 0.0013674504589289427\n",
      "Epoch 2100: train loss: 0.001361698959954083\n",
      "Epoch 2101: train loss: 0.0013560813385993242\n",
      "Epoch 2102: train loss: 0.001350691425614059\n",
      "Epoch 2103: train loss: 0.001345367170870304\n",
      "Epoch 2104: train loss: 0.0013400856405496597\n",
      "Epoch 2105: train loss: 0.0013348304200917482\n",
      "Epoch 2106: train loss: 0.001329490914940834\n",
      "Epoch 2107: train loss: 0.0013241050764918327\n",
      "Epoch 2108: train loss: 0.0013186921132728457\n",
      "Epoch 2109: train loss: 0.0013132201274856925\n",
      "Epoch 2110: train loss: 0.0013077486073598266\n",
      "Epoch 2111: train loss: 0.0013023007195442915\n",
      "Epoch 2112: train loss: 0.001296841073781252\n",
      "Epoch 2113: train loss: 0.0012914080871269107\n",
      "Epoch 2114: train loss: 0.0012860109563916922\n",
      "Epoch 2115: train loss: 0.0012806170852854848\n",
      "Epoch 2116: train loss: 0.0012752432376146317\n",
      "Epoch 2117: train loss: 0.0012699057115241885\n",
      "Epoch 2118: train loss: 0.0012645818060263991\n",
      "Epoch 2119: train loss: 0.0012592888670042157\n",
      "Epoch 2120: train loss: 0.0012540440075099468\n",
      "Epoch 2121: train loss: 0.001248854328878224\n",
      "Epoch 2122: train loss: 0.0012437639525160193\n",
      "Epoch 2123: train loss: 0.0012388544855639338\n",
      "Epoch 2124: train loss: 0.0012342368718236685\n",
      "Epoch 2125: train loss: 0.0012301603564992547\n",
      "Epoch 2126: train loss: 0.0012270720908418298\n",
      "Epoch 2127: train loss: 0.0012258955975994468\n",
      "Epoch 2128: train loss: 0.0012282644165679812\n",
      "Epoch 2129: train loss: 0.001237943535670638\n",
      "Epoch 2130: train loss: 0.001261105528101325\n",
      "Epoch 2131: train loss: 0.0013135849731042981\n",
      "Epoch 2132: train loss: 0.001416679355315864\n",
      "Epoch 2133: train loss: 0.0016326075419783592\n",
      "Epoch 2134: train loss: 0.0019917478784918785\n",
      "Epoch 2135: train loss: 0.002634399337694049\n",
      "Epoch 2136: train loss: 0.0031985610257834196\n",
      "Epoch 2137: train loss: 0.003562716767191887\n",
      "Epoch 2138: train loss: 0.002762617776170373\n",
      "Epoch 2139: train loss: 0.0016511587891727686\n",
      "Epoch 2140: train loss: 0.0012135335709899664\n",
      "Epoch 2141: train loss: 0.0018097319407388568\n",
      "Epoch 2142: train loss: 0.0022001720499247313\n",
      "Epoch 2143: train loss: 0.0016179261729121208\n",
      "Epoch 2144: train loss: 0.0012055246625095606\n",
      "Epoch 2145: train loss: 0.0015587813686579466\n",
      "Epoch 2146: train loss: 0.0017570769414305687\n",
      "Epoch 2147: train loss: 0.001381907146424055\n",
      "Epoch 2148: train loss: 0.001203408115543425\n",
      "Epoch 2149: train loss: 0.001476197736337781\n",
      "Epoch 2150: train loss: 0.0015027644112706184\n",
      "Epoch 2151: train loss: 0.0012110016541555524\n",
      "Epoch 2152: train loss: 0.0012214788002893329\n",
      "Epoch 2153: train loss: 0.0014115716330707073\n",
      "Epoch 2154: train loss: 0.0013002152554690838\n",
      "Epoch 2155: train loss: 0.001137029379606247\n",
      "Epoch 2156: train loss: 0.0012128991074860096\n",
      "Epoch 2157: train loss: 0.0012981712352484465\n",
      "Epoch 2158: train loss: 0.0012017461704090238\n",
      "Epoch 2159: train loss: 0.0010935119353234768\n",
      "Epoch 2160: train loss: 0.0011442449176684022\n",
      "Epoch 2161: train loss: 0.0012212612200528383\n",
      "Epoch 2162: train loss: 0.0011672789696604013\n",
      "Epoch 2163: train loss: 0.0010762634919956326\n",
      "Epoch 2164: train loss: 0.0010734664974734187\n",
      "Epoch 2165: train loss: 0.0011335790622979403\n",
      "Epoch 2166: train loss: 0.0011488613672554493\n",
      "Epoch 2167: train loss: 0.0010897814063355327\n",
      "Epoch 2168: train loss: 0.0010389438830316067\n",
      "Epoch 2169: train loss: 0.0010467657120898366\n",
      "Epoch 2170: train loss: 0.0010821361793205142\n",
      "Epoch 2171: train loss: 0.0010913029545918107\n",
      "Epoch 2172: train loss: 0.0010586268035694957\n",
      "Epoch 2173: train loss: 0.0010213940404355526\n",
      "Epoch 2174: train loss: 0.0010109991999343038\n",
      "Epoch 2175: train loss: 0.0010257294634357095\n",
      "Epoch 2176: train loss: 0.0010383843909949064\n",
      "Epoch 2177: train loss: 0.001029092469252646\n",
      "Epoch 2178: train loss: 0.0010067455004900694\n",
      "Epoch 2179: train loss: 0.0009878198616206646\n",
      "Epoch 2180: train loss: 0.000984014361165464\n",
      "Epoch 2181: train loss: 0.000990669010207057\n",
      "Epoch 2182: train loss: 0.0009935335256159306\n",
      "Epoch 2183: train loss: 0.000985554652288556\n",
      "Epoch 2184: train loss: 0.0009704399271868169\n",
      "Epoch 2185: train loss: 0.0009589033434167504\n",
      "Epoch 2186: train loss: 0.0009556825389154255\n",
      "Epoch 2187: train loss: 0.0009574377327226102\n",
      "Epoch 2188: train loss: 0.0009572941344231367\n",
      "Epoch 2189: train loss: 0.0009507413487881422\n",
      "Epoch 2190: train loss: 0.0009405456366948783\n",
      "Epoch 2191: train loss: 0.0009318694937974215\n",
      "Epoch 2192: train loss: 0.0009276295895688236\n",
      "Epoch 2193: train loss: 0.0009264486725442111\n",
      "Epoch 2194: train loss: 0.0009248722344636917\n",
      "Epoch 2195: train loss: 0.0009206371614709496\n",
      "Epoch 2196: train loss: 0.0009137110318988562\n",
      "Epoch 2197: train loss: 0.0009063970646820962\n",
      "Epoch 2198: train loss: 0.0009006554610095918\n",
      "Epoch 2199: train loss: 0.000896934128832072\n",
      "Epoch 2200: train loss: 0.0008943062275648117\n",
      "Epoch 2201: train loss: 0.0008914083009585738\n",
      "Epoch 2202: train loss: 0.0008873783517628908\n",
      "Epoch 2203: train loss: 0.0008821284282021224\n",
      "Epoch 2204: train loss: 0.0008764108642935753\n",
      "Epoch 2205: train loss: 0.0008709848625585437\n",
      "Epoch 2206: train loss: 0.0008663095068186522\n",
      "Epoch 2207: train loss: 0.0008623923058621585\n",
      "Epoch 2208: train loss: 0.000858909625094384\n",
      "Epoch 2209: train loss: 0.0008554304949939251\n",
      "Epoch 2210: train loss: 0.0008516457164660096\n",
      "Epoch 2211: train loss: 0.0008474906790070236\n",
      "Epoch 2212: train loss: 0.0008429973386228085\n",
      "Epoch 2213: train loss: 0.0008383255917578936\n",
      "Epoch 2214: train loss: 0.0008336585015058517\n",
      "Epoch 2215: train loss: 0.000829111784696579\n",
      "Epoch 2216: train loss: 0.0008247307268902659\n",
      "Epoch 2217: train loss: 0.0008205199846997857\n",
      "Epoch 2218: train loss: 0.0008164619212038815\n",
      "Epoch 2219: train loss: 0.0008125135791487992\n",
      "Epoch 2220: train loss: 0.0008086331654340029\n",
      "Epoch 2221: train loss: 0.0008048019954003394\n",
      "Epoch 2222: train loss: 0.0008010095916688442\n",
      "Epoch 2223: train loss: 0.0007972506573423743\n",
      "Epoch 2224: train loss: 0.0007935230387374759\n",
      "Epoch 2225: train loss: 0.0007898674812167883\n",
      "Epoch 2226: train loss: 0.0007862883503548801\n",
      "Epoch 2227: train loss: 0.0007828743546269834\n",
      "Epoch 2228: train loss: 0.0007796813151799142\n",
      "Epoch 2229: train loss: 0.000776911445427686\n",
      "Epoch 2230: train loss: 0.0007746986229903996\n",
      "Epoch 2231: train loss: 0.0007735609542578459\n",
      "Epoch 2232: train loss: 0.000773852167185396\n",
      "Epoch 2233: train loss: 0.000777016393840313\n",
      "Epoch 2234: train loss: 0.0007839720929041505\n",
      "Epoch 2235: train loss: 0.0007992294849827886\n",
      "Epoch 2236: train loss: 0.0008243083721026778\n",
      "Epoch 2237: train loss: 0.0008724012295715511\n",
      "Epoch 2238: train loss: 0.0009402463329024613\n",
      "Epoch 2239: train loss: 0.001049115089699626\n",
      "Epoch 2240: train loss: 0.0011760209454223514\n",
      "Epoch 2241: train loss: 0.00127002177760005\n",
      "Epoch 2242: train loss: 0.0013435027794912457\n",
      "Epoch 2243: train loss: 0.0012013546656817198\n",
      "Epoch 2244: train loss: 0.0010637985542416573\n",
      "Epoch 2245: train loss: 0.0009530376410111785\n",
      "Epoch 2246: train loss: 0.0009825614979490638\n",
      "Epoch 2247: train loss: 0.0008985350141301751\n",
      "Epoch 2248: train loss: 0.000984575948677957\n",
      "Epoch 2249: train loss: 0.0009623010409995914\n",
      "Epoch 2250: train loss: 0.0009144390933215618\n",
      "Epoch 2251: train loss: 0.0009549280512146652\n",
      "Epoch 2252: train loss: 0.0009540606988593936\n",
      "Epoch 2253: train loss: 0.0010391278192400932\n",
      "Epoch 2254: train loss: 0.0008229730883613229\n",
      "Epoch 2255: train loss: 0.000760465394705534\n",
      "Epoch 2256: train loss: 0.0008061645785346627\n",
      "Epoch 2257: train loss: 0.0008969386690296233\n",
      "Epoch 2258: train loss: 0.001008903607726097\n",
      "Epoch 2259: train loss: 0.0008558683330193162\n",
      "Epoch 2260: train loss: 0.0007361369207501411\n",
      "Epoch 2261: train loss: 0.0006667998386546969\n",
      "Epoch 2262: train loss: 0.0007062256918288767\n",
      "Epoch 2263: train loss: 0.0007960887742228806\n",
      "Epoch 2264: train loss: 0.0008035568753257394\n",
      "Epoch 2265: train loss: 0.0007973877945914865\n",
      "Epoch 2266: train loss: 0.0007477246690541506\n",
      "Epoch 2267: train loss: 0.0007125856936909258\n",
      "Epoch 2268: train loss: 0.000705093378201127\n",
      "Epoch 2269: train loss: 0.0007086156401783228\n",
      "Epoch 2270: train loss: 0.0007100974908098578\n",
      "Epoch 2271: train loss: 0.0006643937667831779\n",
      "Epoch 2272: train loss: 0.0006517897127196193\n",
      "Epoch 2273: train loss: 0.0006771477637812495\n",
      "Epoch 2274: train loss: 0.0007309273933060467\n",
      "Epoch 2275: train loss: 0.0008047212613746524\n",
      "Epoch 2276: train loss: 0.0008264600764960051\n",
      "Epoch 2277: train loss: 0.0007790536619722843\n",
      "Epoch 2278: train loss: 0.0006884505273774266\n",
      "Epoch 2279: train loss: 0.0006560494657605886\n",
      "Epoch 2280: train loss: 0.0007161961402744055\n",
      "Epoch 2281: train loss: 0.0007314334507100284\n",
      "Epoch 2282: train loss: 0.0008021382382139564\n",
      "Epoch 2283: train loss: 0.000731564883608371\n",
      "Epoch 2284: train loss: 0.0006802454590797424\n",
      "Epoch 2285: train loss: 0.0006945235654711723\n",
      "Epoch 2286: train loss: 0.0007569388253614306\n",
      "Epoch 2287: train loss: 0.0008776834001764655\n",
      "Epoch 2288: train loss: 0.0007356200949288905\n",
      "Epoch 2289: train loss: 0.0006431145593523979\n",
      "Epoch 2290: train loss: 0.00058589136460796\n",
      "Epoch 2291: train loss: 0.000603092135861516\n",
      "Epoch 2292: train loss: 0.0006568902172148228\n",
      "Epoch 2293: train loss: 0.0006637744954787195\n",
      "Epoch 2294: train loss: 0.000671247486025095\n",
      "Epoch 2295: train loss: 0.0006289155571721494\n",
      "Epoch 2296: train loss: 0.0006078442675061524\n",
      "Epoch 2297: train loss: 0.0006037320126779377\n",
      "Epoch 2298: train loss: 0.0006030230433680117\n",
      "Epoch 2299: train loss: 0.0005925131845287979\n",
      "Epoch 2300: train loss: 0.0005741785862483084\n",
      "Epoch 2301: train loss: 0.0005595899419859052\n",
      "Epoch 2302: train loss: 0.0005452127079479396\n",
      "Epoch 2303: train loss: 0.0005371165461838245\n",
      "Epoch 2304: train loss: 0.0005343761295080185\n",
      "Epoch 2305: train loss: 0.0005340417847037315\n",
      "Epoch 2306: train loss: 0.0005351757863536477\n",
      "Epoch 2307: train loss: 0.0005365008837543428\n",
      "Epoch 2308: train loss: 0.0005377752240747213\n",
      "Epoch 2309: train loss: 0.000538360676728189\n",
      "Epoch 2310: train loss: 0.0005400190129876137\n",
      "Epoch 2311: train loss: 0.0005413124454207718\n",
      "Epoch 2312: train loss: 0.0005426311981864274\n",
      "Epoch 2313: train loss: 0.0005480759427882731\n",
      "Epoch 2314: train loss: 0.0005573591915890574\n",
      "Epoch 2315: train loss: 0.0005719315377064049\n",
      "Epoch 2316: train loss: 0.0005911061889491975\n",
      "Epoch 2317: train loss: 0.000619920261669904\n",
      "Epoch 2318: train loss: 0.0006539776222780347\n",
      "Epoch 2319: train loss: 0.0007029302068985999\n",
      "Epoch 2320: train loss: 0.0007531525916419923\n",
      "Epoch 2321: train loss: 0.0008255902794189751\n",
      "Epoch 2322: train loss: 0.0008761823992244899\n",
      "Epoch 2323: train loss: 0.0009349978063255548\n",
      "Epoch 2324: train loss: 0.0008968174224719405\n",
      "Epoch 2325: train loss: 0.000820901885163039\n",
      "Epoch 2326: train loss: 0.0006668943678960204\n",
      "Epoch 2327: train loss: 0.0005467883311212063\n",
      "Epoch 2328: train loss: 0.0004969245055690408\n",
      "Epoch 2329: train loss: 0.0005313631263561547\n",
      "Epoch 2330: train loss: 0.0006074006087146699\n",
      "Epoch 2331: train loss: 0.0006845311727374792\n",
      "Epoch 2332: train loss: 0.000666527368593961\n",
      "Epoch 2333: train loss: 0.0005545318126678467\n",
      "Epoch 2334: train loss: 0.00048620320740155876\n",
      "Epoch 2335: train loss: 0.000516992702614516\n",
      "Epoch 2336: train loss: 0.0006568472599610686\n",
      "Epoch 2337: train loss: 0.0007322845631279051\n",
      "Epoch 2338: train loss: 0.0008063074783422053\n",
      "Epoch 2339: train loss: 0.0006222180090844631\n",
      "Epoch 2340: train loss: 0.000517850392498076\n",
      "Epoch 2341: train loss: 0.0006749489693902433\n",
      "Epoch 2342: train loss: 0.0007465834496542811\n",
      "Epoch 2343: train loss: 0.0010153427720069885\n",
      "Epoch 2344: train loss: 0.0008697389275766909\n",
      "Epoch 2345: train loss: 0.0006783960852771997\n",
      "Epoch 2346: train loss: 0.0008353934390470386\n",
      "Epoch 2347: train loss: 0.0006732754409313202\n",
      "Epoch 2348: train loss: 0.0006159726763144135\n",
      "Epoch 2349: train loss: 0.0005098888650536537\n",
      "Epoch 2350: train loss: 0.0004925999674014747\n",
      "Epoch 2351: train loss: 0.0006348259630613029\n",
      "Epoch 2352: train loss: 0.0006942929467186332\n",
      "Epoch 2353: train loss: 0.0006281635141931474\n",
      "Epoch 2354: train loss: 0.0005349238053895533\n",
      "Epoch 2355: train loss: 0.0005474149947986007\n",
      "Epoch 2356: train loss: 0.0005591431981883943\n",
      "Epoch 2357: train loss: 0.0005217008292675018\n",
      "Epoch 2358: train loss: 0.0004934094613417983\n",
      "Epoch 2359: train loss: 0.00045274352305568755\n",
      "Epoch 2360: train loss: 0.00047844817163422704\n",
      "Epoch 2361: train loss: 0.0005448766169138253\n",
      "Epoch 2362: train loss: 0.0006047875504009426\n",
      "Epoch 2363: train loss: 0.0006469771615229547\n",
      "Epoch 2364: train loss: 0.0005508369649760425\n",
      "Epoch 2365: train loss: 0.00046398676931858063\n",
      "Epoch 2366: train loss: 0.0004964063409715891\n",
      "Epoch 2367: train loss: 0.0004882085486315191\n",
      "Epoch 2368: train loss: 0.000524762668646872\n",
      "Epoch 2369: train loss: 0.00048361840890720487\n",
      "Epoch 2370: train loss: 0.00045085776946507394\n",
      "Epoch 2371: train loss: 0.00044537248322740197\n",
      "Epoch 2372: train loss: 0.0004722916055470705\n",
      "Epoch 2373: train loss: 0.0005181581364013255\n",
      "Epoch 2374: train loss: 0.00047500283108092844\n",
      "Epoch 2375: train loss: 0.0004321098676882684\n",
      "Epoch 2376: train loss: 0.0003922846226487309\n",
      "Epoch 2377: train loss: 0.0003862010780721903\n",
      "Epoch 2378: train loss: 0.00039467329042963684\n",
      "Epoch 2379: train loss: 0.0003939287271350622\n",
      "Epoch 2380: train loss: 0.0003967105585616082\n",
      "Epoch 2381: train loss: 0.00039049575570970774\n",
      "Epoch 2382: train loss: 0.00038740370655432343\n",
      "Epoch 2383: train loss: 0.0003874218091368675\n",
      "Epoch 2384: train loss: 0.0003940364404115826\n",
      "Epoch 2385: train loss: 0.0003941368777304888\n",
      "Epoch 2386: train loss: 0.0003853951347991824\n",
      "Epoch 2387: train loss: 0.00037592032458633184\n",
      "Epoch 2388: train loss: 0.00036815041676163673\n",
      "Epoch 2389: train loss: 0.0003603784425649792\n",
      "Epoch 2390: train loss: 0.0003562873462215066\n",
      "Epoch 2391: train loss: 0.00035427234251983464\n",
      "Epoch 2392: train loss: 0.00035245774779468775\n",
      "Epoch 2393: train loss: 0.0003484706685412675\n",
      "Epoch 2394: train loss: 0.000344445783412084\n",
      "Epoch 2395: train loss: 0.0003412897349335253\n",
      "Epoch 2396: train loss: 0.0003393910301383585\n",
      "Epoch 2397: train loss: 0.0003379615372978151\n",
      "Epoch 2398: train loss: 0.0003372368519194424\n",
      "Epoch 2399: train loss: 0.00033792704925872386\n",
      "Epoch 2400: train loss: 0.0003381027199793607\n",
      "Epoch 2401: train loss: 0.00033814433845691383\n",
      "Epoch 2402: train loss: 0.000337989506078884\n",
      "Epoch 2403: train loss: 0.0003386533062439412\n",
      "Epoch 2404: train loss: 0.00033903541043400764\n",
      "Epoch 2405: train loss: 0.0003406280593480915\n",
      "Epoch 2406: train loss: 0.00034373928792774677\n",
      "Epoch 2407: train loss: 0.00034978860639967024\n",
      "Epoch 2408: train loss: 0.00035897945053875446\n",
      "Epoch 2409: train loss: 0.00037378043634817004\n",
      "Epoch 2410: train loss: 0.0003954936400987208\n",
      "Epoch 2411: train loss: 0.0004294389800634235\n",
      "Epoch 2412: train loss: 0.0004732068337034434\n",
      "Epoch 2413: train loss: 0.0005383448442444205\n",
      "Epoch 2414: train loss: 0.0006139838951639831\n",
      "Epoch 2415: train loss: 0.0007189253810793161\n",
      "Epoch 2416: train loss: 0.000807262840680778\n",
      "Epoch 2417: train loss: 0.000893235846888274\n",
      "Epoch 2418: train loss: 0.0008826360572129488\n",
      "Epoch 2419: train loss: 0.0008098525577224791\n",
      "Epoch 2420: train loss: 0.0006457280833274126\n",
      "Epoch 2421: train loss: 0.0004927810514345765\n",
      "Epoch 2422: train loss: 0.00037146895192563534\n",
      "Epoch 2423: train loss: 0.00038786910590715706\n",
      "Epoch 2424: train loss: 0.00044809799874201417\n",
      "Epoch 2425: train loss: 0.00048064294969663024\n",
      "Epoch 2426: train loss: 0.00044522195821627975\n",
      "Epoch 2427: train loss: 0.00038708813372068107\n",
      "Epoch 2428: train loss: 0.00039649097016081214\n",
      "Epoch 2429: train loss: 0.00042557567940093577\n",
      "Epoch 2430: train loss: 0.0005711328703910112\n",
      "Epoch 2431: train loss: 0.000511408201418817\n",
      "Epoch 2432: train loss: 0.0003745298308786005\n",
      "Epoch 2433: train loss: 0.0003638242487795651\n",
      "Epoch 2434: train loss: 0.00046806249883957207\n",
      "Epoch 2435: train loss: 0.0007694115629419684\n",
      "Epoch 2436: train loss: 0.000638696423266083\n",
      "Epoch 2437: train loss: 0.0004596300714183599\n",
      "Epoch 2438: train loss: 0.0004173601628281176\n",
      "Epoch 2439: train loss: 0.00046008347999304533\n",
      "Epoch 2440: train loss: 0.0005887406296096742\n",
      "Epoch 2441: train loss: 0.0004546728450804949\n",
      "Epoch 2442: train loss: 0.00035167226451449096\n",
      "Epoch 2443: train loss: 0.00034047855297103524\n",
      "Epoch 2444: train loss: 0.00039713139995001256\n",
      "Epoch 2445: train loss: 0.00044321612222120166\n",
      "Epoch 2446: train loss: 0.00035840398049913347\n",
      "Epoch 2447: train loss: 0.000291347416350618\n",
      "Epoch 2448: train loss: 0.00028300328995101154\n",
      "Epoch 2449: train loss: 0.0003183983499184251\n",
      "Epoch 2450: train loss: 0.0003403781447559595\n",
      "Epoch 2451: train loss: 0.0003246135893277824\n",
      "Epoch 2452: train loss: 0.00031142443185672164\n",
      "Epoch 2453: train loss: 0.0002890600881073624\n",
      "Epoch 2454: train loss: 0.00027788677834905684\n",
      "Epoch 2455: train loss: 0.0002825812844093889\n",
      "Epoch 2456: train loss: 0.0002872166223824024\n",
      "Epoch 2457: train loss: 0.00029154287767596543\n",
      "Epoch 2458: train loss: 0.0002744160592556\n",
      "Epoch 2459: train loss: 0.00026960790273733437\n",
      "Epoch 2460: train loss: 0.000279951753327623\n",
      "Epoch 2461: train loss: 0.000289403076749295\n",
      "Epoch 2462: train loss: 0.0002974167582578957\n",
      "Epoch 2463: train loss: 0.00029157972312532365\n",
      "Epoch 2464: train loss: 0.00027834303909912705\n",
      "Epoch 2465: train loss: 0.00025388936046510935\n",
      "Epoch 2466: train loss: 0.0002430804306641221\n",
      "Epoch 2467: train loss: 0.0002494865329936147\n",
      "Epoch 2468: train loss: 0.00026262333267368376\n",
      "Epoch 2469: train loss: 0.00027832051273435354\n",
      "Epoch 2470: train loss: 0.0002835968043655157\n",
      "Epoch 2471: train loss: 0.0002781973162200302\n",
      "Epoch 2472: train loss: 0.0002575017570052296\n",
      "Epoch 2473: train loss: 0.00024513431708328426\n",
      "Epoch 2474: train loss: 0.00024353092885576189\n",
      "Epoch 2475: train loss: 0.00024674946325831115\n",
      "Epoch 2476: train loss: 0.0002559525310061872\n",
      "Epoch 2477: train loss: 0.0002569508214946836\n",
      "Epoch 2478: train loss: 0.0002572597877588123\n",
      "Epoch 2479: train loss: 0.00024361915711779147\n",
      "Epoch 2480: train loss: 0.0002349021378904581\n",
      "Epoch 2481: train loss: 0.00022980992798693478\n",
      "Epoch 2482: train loss: 0.00022784664179198444\n",
      "Epoch 2483: train loss: 0.0002293676370754838\n",
      "Epoch 2484: train loss: 0.0002318649203516543\n",
      "Epoch 2485: train loss: 0.0002334865857847035\n",
      "Epoch 2486: train loss: 0.00023400412464980036\n",
      "Epoch 2487: train loss: 0.00023554581275675446\n",
      "Epoch 2488: train loss: 0.00023338012397289276\n",
      "Epoch 2489: train loss: 0.00023188367777038366\n",
      "Epoch 2490: train loss: 0.000228152159252204\n",
      "Epoch 2491: train loss: 0.00022503844229504466\n",
      "Epoch 2492: train loss: 0.00022046080266591161\n",
      "Epoch 2493: train loss: 0.00021701557852793485\n",
      "Epoch 2494: train loss: 0.0002139359712600708\n",
      "Epoch 2495: train loss: 0.00021174752328079194\n",
      "Epoch 2496: train loss: 0.0002101856080116704\n",
      "Epoch 2497: train loss: 0.00020892843895126134\n",
      "Epoch 2498: train loss: 0.00020803108054678887\n",
      "Epoch 2499: train loss: 0.00020783900981768966\n",
      "Epoch 2500: train loss: 0.000208174780709669\n",
      "Epoch 2501: train loss: 0.00020897969079669565\n",
      "Epoch 2502: train loss: 0.00021109494264237583\n",
      "Epoch 2503: train loss: 0.00021528193610720336\n",
      "Epoch 2504: train loss: 0.00022303784498944879\n",
      "Epoch 2505: train loss: 0.00023720224271528423\n",
      "Epoch 2506: train loss: 0.0002646773064043373\n",
      "Epoch 2507: train loss: 0.00031684827990829945\n",
      "Epoch 2508: train loss: 0.0004070004797540605\n",
      "Epoch 2509: train loss: 0.0005501747946254909\n",
      "Epoch 2510: train loss: 0.0007135245250537992\n",
      "Epoch 2511: train loss: 0.0007912449073046446\n",
      "Epoch 2512: train loss: 0.0008329810807481408\n",
      "Epoch 2513: train loss: 0.0008183717727661133\n",
      "Epoch 2514: train loss: 0.000997383613139391\n",
      "Epoch 2515: train loss: 0.0008592868107371032\n",
      "Epoch 2516: train loss: 0.0009274533949792385\n",
      "Epoch 2517: train loss: 0.0003775613440666348\n",
      "Epoch 2518: train loss: 0.00038603367283940315\n",
      "Epoch 2519: train loss: 0.000773786858189851\n",
      "Epoch 2520: train loss: 0.000616680015809834\n",
      "Epoch 2521: train loss: 0.000556170241907239\n",
      "Epoch 2522: train loss: 0.0004709101631306112\n",
      "Epoch 2523: train loss: 0.00039640124305151403\n",
      "Epoch 2524: train loss: 0.000461937568616122\n",
      "Epoch 2525: train loss: 0.0005211647367104888\n",
      "Epoch 2526: train loss: 0.0005948051693849266\n",
      "Epoch 2527: train loss: 0.0006038050050847232\n",
      "Epoch 2528: train loss: 0.0005651766550727189\n",
      "Epoch 2529: train loss: 0.00046005836338736117\n",
      "Epoch 2530: train loss: 0.00031952100107446313\n",
      "Epoch 2531: train loss: 0.00035820831544697285\n",
      "Epoch 2532: train loss: 0.0004750738444272429\n",
      "Epoch 2533: train loss: 0.00047152204206213355\n",
      "Epoch 2534: train loss: 0.00029834898305125535\n",
      "Epoch 2535: train loss: 0.00021239688794594258\n",
      "Epoch 2536: train loss: 0.0002905174915213138\n",
      "Epoch 2537: train loss: 0.00034786073956638575\n",
      "Epoch 2538: train loss: 0.0003039128496311605\n",
      "Epoch 2539: train loss: 0.00025486553204245865\n",
      "Epoch 2540: train loss: 0.00023349185357801616\n",
      "Epoch 2541: train loss: 0.00022162847744766623\n",
      "Epoch 2542: train loss: 0.00023393290757667273\n",
      "Epoch 2543: train loss: 0.0002634116099216044\n",
      "Epoch 2544: train loss: 0.00024795328499749303\n",
      "Epoch 2545: train loss: 0.00018914457177743316\n",
      "Epoch 2546: train loss: 0.00018322495452594012\n",
      "Epoch 2547: train loss: 0.00021879146515857428\n",
      "Epoch 2548: train loss: 0.0002234947169199586\n",
      "Epoch 2549: train loss: 0.00020928044978063554\n",
      "Epoch 2550: train loss: 0.00019560055807232857\n",
      "Epoch 2551: train loss: 0.00018471981456968933\n",
      "Epoch 2552: train loss: 0.0001774853008100763\n",
      "Epoch 2553: train loss: 0.00018339285452384502\n",
      "Epoch 2554: train loss: 0.00019906600937247276\n",
      "Epoch 2555: train loss: 0.00019916163000743836\n",
      "Epoch 2556: train loss: 0.00018357821682002395\n",
      "Epoch 2557: train loss: 0.00017188266792800277\n",
      "Epoch 2558: train loss: 0.00016929098637774587\n",
      "Epoch 2559: train loss: 0.00016912288265302777\n",
      "Epoch 2560: train loss: 0.0001727255730656907\n",
      "Epoch 2561: train loss: 0.00018250908760819584\n",
      "Epoch 2562: train loss: 0.00018588297825772315\n",
      "Epoch 2563: train loss: 0.0001795003772713244\n",
      "Epoch 2564: train loss: 0.00016797799617052078\n",
      "Epoch 2565: train loss: 0.00016223192506004125\n",
      "Epoch 2566: train loss: 0.0001611446205060929\n",
      "Epoch 2567: train loss: 0.00016137553029693663\n",
      "Epoch 2568: train loss: 0.00016795295232441276\n",
      "Epoch 2569: train loss: 0.00017531870980747044\n",
      "Epoch 2570: train loss: 0.00017523074347991496\n",
      "Epoch 2571: train loss: 0.00016827664512675256\n",
      "Epoch 2572: train loss: 0.00016324098396580666\n",
      "Epoch 2573: train loss: 0.00015934387920424342\n",
      "Epoch 2574: train loss: 0.00015456436085514724\n",
      "Epoch 2575: train loss: 0.00015123269986361265\n",
      "Epoch 2576: train loss: 0.0001503589010098949\n",
      "Epoch 2577: train loss: 0.0001499210629845038\n",
      "Epoch 2578: train loss: 0.00014962982095312327\n",
      "Epoch 2579: train loss: 0.0001504759566159919\n",
      "Epoch 2580: train loss: 0.0001521548256278038\n",
      "Epoch 2581: train loss: 0.00015375751536339521\n",
      "Epoch 2582: train loss: 0.0001544878468848765\n",
      "Epoch 2583: train loss: 0.000155113244545646\n",
      "Epoch 2584: train loss: 0.00015524706395808607\n",
      "Epoch 2585: train loss: 0.00015585891378577799\n",
      "Epoch 2586: train loss: 0.00015608922694809735\n",
      "Epoch 2587: train loss: 0.00015807567979209125\n",
      "Epoch 2588: train loss: 0.00015886327309999615\n",
      "Epoch 2589: train loss: 0.0001607003214303404\n",
      "Epoch 2590: train loss: 0.00016023140051402152\n",
      "Epoch 2591: train loss: 0.00016316314577125013\n",
      "Epoch 2592: train loss: 0.0001637190580368042\n",
      "Epoch 2593: train loss: 0.00016843403864186257\n",
      "Epoch 2594: train loss: 0.00016898814646992832\n",
      "Epoch 2595: train loss: 0.00017387547995895147\n",
      "Epoch 2596: train loss: 0.00017459766240790486\n",
      "Epoch 2597: train loss: 0.0001797652366803959\n",
      "Epoch 2598: train loss: 0.00018168965470977128\n",
      "Epoch 2599: train loss: 0.00018815095245372504\n",
      "Epoch 2600: train loss: 0.000190556442248635\n",
      "Epoch 2601: train loss: 0.00019619065278675407\n",
      "Epoch 2602: train loss: 0.00019735578098334372\n",
      "Epoch 2603: train loss: 0.00019978579075541347\n",
      "Epoch 2604: train loss: 0.00019888886890839785\n",
      "Epoch 2605: train loss: 0.0001986042916541919\n",
      "Epoch 2606: train loss: 0.00019628652080427855\n",
      "Epoch 2607: train loss: 0.00019439987954683602\n",
      "Epoch 2608: train loss: 0.00018969991651829332\n",
      "Epoch 2609: train loss: 0.0001847304665716365\n",
      "Epoch 2610: train loss: 0.00017732352716848254\n",
      "Epoch 2611: train loss: 0.00017092794587370008\n",
      "Epoch 2612: train loss: 0.00016370548109989613\n",
      "Epoch 2613: train loss: 0.00015887434710748494\n",
      "Epoch 2614: train loss: 0.0001533099275548011\n",
      "Epoch 2615: train loss: 0.000150355976074934\n",
      "Epoch 2616: train loss: 0.00014584050222765654\n",
      "Epoch 2617: train loss: 0.00014409284631256014\n",
      "Epoch 2618: train loss: 0.00014107057359069586\n",
      "Epoch 2619: train loss: 0.00014090081094764173\n",
      "Epoch 2620: train loss: 0.0001392714912071824\n",
      "Epoch 2621: train loss: 0.0001402807392878458\n",
      "Epoch 2622: train loss: 0.0001392838021274656\n",
      "Epoch 2623: train loss: 0.00014123896835371852\n",
      "Epoch 2624: train loss: 0.0001413484278600663\n",
      "Epoch 2625: train loss: 0.00014555788948200643\n",
      "Epoch 2626: train loss: 0.0001481626823078841\n",
      "Epoch 2627: train loss: 0.00015620634076185524\n",
      "Epoch 2628: train loss: 0.0001617789239389822\n",
      "Epoch 2629: train loss: 0.00017455584020353854\n",
      "Epoch 2630: train loss: 0.00018372391059529036\n",
      "Epoch 2631: train loss: 0.0002016224170802161\n",
      "Epoch 2632: train loss: 0.00021421228302642703\n",
      "Epoch 2633: train loss: 0.0002320604253327474\n",
      "Epoch 2634: train loss: 0.00024440628476440907\n",
      "Epoch 2635: train loss: 0.0002550666104070842\n",
      "Epoch 2636: train loss: 0.00026603409787639976\n",
      "Epoch 2637: train loss: 0.00027185719227418303\n",
      "Epoch 2638: train loss: 0.0002804233809001744\n",
      "Epoch 2639: train loss: 0.00027949095238000154\n",
      "Epoch 2640: train loss: 0.0002757349575404078\n",
      "Epoch 2641: train loss: 0.00025845281197689474\n",
      "Epoch 2642: train loss: 0.00023529902682639658\n",
      "Epoch 2643: train loss: 0.0002034537901636213\n",
      "Epoch 2644: train loss: 0.00017177766130771488\n",
      "Epoch 2645: train loss: 0.0001435214508092031\n",
      "Epoch 2646: train loss: 0.0001244565355591476\n",
      "Epoch 2647: train loss: 0.00011623597674770281\n",
      "Epoch 2648: train loss: 0.00011792420991696417\n",
      "Epoch 2649: train loss: 0.00012640967906918377\n",
      "Epoch 2650: train loss: 0.00013671291526407003\n",
      "Epoch 2651: train loss: 0.0001456305180909112\n",
      "Epoch 2652: train loss: 0.00014924161951057613\n",
      "Epoch 2653: train loss: 0.00014846726844552904\n",
      "Epoch 2654: train loss: 0.00014229092630557716\n",
      "Epoch 2655: train loss: 0.0001348796213278547\n",
      "Epoch 2656: train loss: 0.0001263948652194813\n",
      "Epoch 2657: train loss: 0.0001211802737088874\n",
      "Epoch 2658: train loss: 0.00011831196025013924\n",
      "Epoch 2659: train loss: 0.00012114341370761395\n",
      "Epoch 2660: train loss: 0.00012684376270044595\n",
      "Epoch 2661: train loss: 0.00014003957039676607\n",
      "Epoch 2662: train loss: 0.00015454883396159858\n",
      "Epoch 2663: train loss: 0.000181087976670824\n",
      "Epoch 2664: train loss: 0.00019947496184613556\n",
      "Epoch 2665: train loss: 0.0002307052054675296\n",
      "Epoch 2666: train loss: 0.00022816196724306792\n",
      "Epoch 2667: train loss: 0.000224124058149755\n",
      "Epoch 2668: train loss: 0.00019375018018763512\n",
      "Epoch 2669: train loss: 0.00016818572476040572\n",
      "Epoch 2670: train loss: 0.00015026942128315568\n",
      "Epoch 2671: train loss: 0.0001396145235048607\n",
      "Epoch 2672: train loss: 0.00013309475616551936\n",
      "Epoch 2673: train loss: 0.00012783809506800026\n",
      "Epoch 2674: train loss: 0.0001232946669915691\n",
      "Epoch 2675: train loss: 0.00011934380745515227\n",
      "Epoch 2676: train loss: 0.00011660893505904824\n",
      "Epoch 2677: train loss: 0.00011409910803195089\n",
      "Epoch 2678: train loss: 0.00011343054211465642\n",
      "Epoch 2679: train loss: 0.00011353643640177324\n",
      "Epoch 2680: train loss: 0.0001178746169898659\n",
      "Epoch 2681: train loss: 0.00012224689999129623\n",
      "Epoch 2682: train loss: 0.0001359077577944845\n",
      "Epoch 2683: train loss: 0.00014849068247713149\n",
      "Epoch 2684: train loss: 0.0001844446233008057\n",
      "Epoch 2685: train loss: 0.00019813617109321058\n",
      "Epoch 2686: train loss: 0.0002355362958041951\n",
      "Epoch 2687: train loss: 0.00021043560991529375\n",
      "Epoch 2688: train loss: 0.0001907278347061947\n",
      "Epoch 2689: train loss: 0.00015927957429084927\n",
      "Epoch 2690: train loss: 0.0001442075299564749\n",
      "Epoch 2691: train loss: 0.00014254469715524465\n",
      "Epoch 2692: train loss: 0.00014770777488593012\n",
      "Epoch 2693: train loss: 0.00015737322974018753\n",
      "Epoch 2694: train loss: 0.00016818117001093924\n",
      "Epoch 2695: train loss: 0.00018276373157277703\n",
      "Epoch 2696: train loss: 0.00019485241500660777\n",
      "Epoch 2697: train loss: 0.00021543254842981696\n",
      "Epoch 2698: train loss: 0.00021917476260568947\n",
      "Epoch 2699: train loss: 0.00023877440253272653\n",
      "Epoch 2700: train loss: 0.0002274077123729512\n",
      "Epoch 2701: train loss: 0.00023503803822677583\n",
      "Epoch 2702: train loss: 0.00021103040489833802\n",
      "Epoch 2703: train loss: 0.00020469179435167462\n",
      "Epoch 2704: train loss: 0.00017662598111201078\n",
      "Epoch 2705: train loss: 0.0001663301809458062\n",
      "Epoch 2706: train loss: 0.00014146915054880083\n",
      "Epoch 2707: train loss: 0.00013145460980013013\n",
      "Epoch 2708: train loss: 0.00011735058797057718\n",
      "Epoch 2709: train loss: 0.00011685156641760841\n",
      "Epoch 2710: train loss: 0.0001200746264657937\n",
      "Epoch 2711: train loss: 0.0001348905061604455\n",
      "Epoch 2712: train loss: 0.00015428657934535295\n",
      "Epoch 2713: train loss: 0.00018467535846866667\n",
      "Epoch 2714: train loss: 0.0002163094177376479\n",
      "Epoch 2715: train loss: 0.0002539540291763842\n",
      "Epoch 2716: train loss: 0.0002820387890096754\n",
      "Epoch 2717: train loss: 0.0002942793653346598\n",
      "Epoch 2718: train loss: 0.0002912467170972377\n",
      "Epoch 2719: train loss: 0.0002673639974091202\n",
      "Epoch 2720: train loss: 0.00024430546909570694\n",
      "Epoch 2721: train loss: 0.00021660518541466445\n",
      "Epoch 2722: train loss: 0.00018923651077784598\n",
      "Epoch 2723: train loss: 0.00015989942767191678\n",
      "Epoch 2724: train loss: 0.00013434179709292948\n",
      "Epoch 2725: train loss: 0.00011467612785054371\n",
      "Epoch 2726: train loss: 0.0001081770024029538\n",
      "Epoch 2727: train loss: 0.00011077146336901933\n",
      "Epoch 2728: train loss: 0.00012860281276516616\n",
      "Epoch 2729: train loss: 0.00014728560927323997\n",
      "Epoch 2730: train loss: 0.00017032735922839493\n",
      "Epoch 2731: train loss: 0.00017615227261558175\n",
      "Epoch 2732: train loss: 0.00017245419439859688\n",
      "Epoch 2733: train loss: 0.0001538297365186736\n",
      "Epoch 2734: train loss: 0.0001345492637483403\n",
      "Epoch 2735: train loss: 0.00011605002509895712\n",
      "Epoch 2736: train loss: 0.0001045017343130894\n",
      "Epoch 2737: train loss: 9.704117110231891e-05\n",
      "Epoch 2738: train loss: 9.506600326858461e-05\n",
      "Epoch 2739: train loss: 9.545268403599039e-05\n",
      "Epoch 2740: train loss: 9.862413571681827e-05\n",
      "Epoch 2741: train loss: 0.00010178039519814774\n",
      "Epoch 2742: train loss: 0.00010466451203683391\n",
      "Epoch 2743: train loss: 0.00010581980313872918\n",
      "Epoch 2744: train loss: 0.00010653995559550822\n",
      "Epoch 2745: train loss: 0.00010565762931946665\n",
      "Epoch 2746: train loss: 0.00010541945084696636\n",
      "Epoch 2747: train loss: 0.00010385873611085117\n",
      "Epoch 2748: train loss: 0.00010496858158148825\n",
      "Epoch 2749: train loss: 0.0001053681262419559\n",
      "Epoch 2750: train loss: 0.00011255149729549885\n",
      "Epoch 2751: train loss: 0.00011767498654080555\n",
      "Epoch 2752: train loss: 0.00013885828957427293\n",
      "Epoch 2753: train loss: 0.00014994062075857073\n",
      "Epoch 2754: train loss: 0.00019450279069133103\n",
      "Epoch 2755: train loss: 0.00019199027155991644\n",
      "Epoch 2756: train loss: 0.00021636275050695986\n",
      "Epoch 2757: train loss: 0.0001725247857393697\n",
      "Epoch 2758: train loss: 0.00014303553325589746\n",
      "Epoch 2759: train loss: 0.0001181261395686306\n",
      "Epoch 2760: train loss: 0.00011065206490457058\n",
      "Epoch 2761: train loss: 0.00011340441415086389\n",
      "Epoch 2762: train loss: 0.00012248073471710086\n",
      "Epoch 2763: train loss: 0.00013604116975329816\n",
      "Epoch 2764: train loss: 0.0001508107379777357\n",
      "Epoch 2765: train loss: 0.0001721998560242355\n",
      "Epoch 2766: train loss: 0.00018960284069180489\n",
      "Epoch 2767: train loss: 0.00022133314632810652\n",
      "Epoch 2768: train loss: 0.00022988718410488218\n",
      "Epoch 2769: train loss: 0.0002635915589053184\n",
      "Epoch 2770: train loss: 0.00025141736841760576\n",
      "Epoch 2771: train loss: 0.00026599335251376033\n",
      "Epoch 2772: train loss: 0.0002401570527581498\n",
      "Epoch 2773: train loss: 0.0002322741347597912\n",
      "Epoch 2774: train loss: 0.00019729281484615058\n",
      "Epoch 2775: train loss: 0.00017554273654241115\n",
      "Epoch 2776: train loss: 0.00014198543794918805\n",
      "Epoch 2777: train loss: 0.00011872385221067816\n",
      "Epoch 2778: train loss: 9.769776806933805e-05\n",
      "Epoch 2779: train loss: 8.640811574878171e-05\n",
      "Epoch 2780: train loss: 8.262839401140809e-05\n",
      "Epoch 2781: train loss: 8.578709093853831e-05\n",
      "Epoch 2782: train loss: 9.371743362862617e-05\n",
      "Epoch 2783: train loss: 0.0001045144017552957\n",
      "Epoch 2784: train loss: 0.00011780949716921896\n",
      "Epoch 2785: train loss: 0.0001330789818894118\n",
      "Epoch 2786: train loss: 0.00015291007002815604\n",
      "Epoch 2787: train loss: 0.0001797355798771605\n",
      "Epoch 2788: train loss: 0.00021693292364943773\n",
      "Epoch 2789: train loss: 0.0002689011162146926\n",
      "Epoch 2790: train loss: 0.0002990135399159044\n",
      "Epoch 2791: train loss: 0.0003023935714736581\n",
      "Epoch 2792: train loss: 0.00024103866599034518\n",
      "Epoch 2793: train loss: 0.0001661246787989512\n",
      "Epoch 2794: train loss: 0.0001242090220330283\n",
      "Epoch 2795: train loss: 0.00011286139488220215\n",
      "Epoch 2796: train loss: 0.0001278740237466991\n",
      "Epoch 2797: train loss: 0.00015886061009950936\n",
      "Epoch 2798: train loss: 0.0002220210590166971\n",
      "Epoch 2799: train loss: 0.00018621263734530658\n",
      "Epoch 2800: train loss: 0.00014522008132189512\n",
      "Epoch 2801: train loss: 0.0001024817320285365\n",
      "Epoch 2802: train loss: 9.167223470285535e-05\n",
      "Epoch 2803: train loss: 9.716943168314174e-05\n",
      "Epoch 2804: train loss: 0.00011334985902067274\n",
      "Epoch 2805: train loss: 0.00014515795919578522\n",
      "Epoch 2806: train loss: 0.0001485757384216413\n",
      "Epoch 2807: train loss: 0.000162175259902142\n",
      "Epoch 2808: train loss: 0.0001343508338322863\n",
      "Epoch 2809: train loss: 0.00012837792746722698\n",
      "Epoch 2810: train loss: 0.00010549028957029805\n",
      "Epoch 2811: train loss: 9.418654371984303e-05\n",
      "Epoch 2812: train loss: 8.325389353558421e-05\n",
      "Epoch 2813: train loss: 7.868900138419122e-05\n",
      "Epoch 2814: train loss: 7.934885798022151e-05\n",
      "Epoch 2815: train loss: 8.2854472566396e-05\n",
      "Epoch 2816: train loss: 8.805154357105494e-05\n",
      "Epoch 2817: train loss: 9.371562191518024e-05\n",
      "Epoch 2818: train loss: 0.0001018604016280733\n",
      "Epoch 2819: train loss: 0.00010450707486597821\n",
      "Epoch 2820: train loss: 0.00011110153718618676\n",
      "Epoch 2821: train loss: 0.00010875207954086363\n",
      "Epoch 2822: train loss: 0.00011331045971019194\n",
      "Epoch 2823: train loss: 0.00010749555804068223\n",
      "Epoch 2824: train loss: 0.00011011983588105068\n",
      "Epoch 2825: train loss: 0.00010247256432194263\n",
      "Epoch 2826: train loss: 0.00010351737000746652\n",
      "Epoch 2827: train loss: 9.881213918561116e-05\n",
      "Epoch 2828: train loss: 0.00010266077879350632\n",
      "Epoch 2829: train loss: 0.00010242100688628852\n",
      "Epoch 2830: train loss: 0.00011009962327079847\n",
      "Epoch 2831: train loss: 0.00011318797623971477\n",
      "Epoch 2832: train loss: 0.00012435016105882823\n",
      "Epoch 2833: train loss: 0.0001350979582639411\n",
      "Epoch 2834: train loss: 0.00015698342758696526\n",
      "Epoch 2835: train loss: 0.0001837271556723863\n",
      "Epoch 2836: train loss: 0.00021975088748149574\n",
      "Epoch 2837: train loss: 0.00026071059983223677\n",
      "Epoch 2838: train loss: 0.0002997297269757837\n",
      "Epoch 2839: train loss: 0.0003492924151942134\n",
      "Epoch 2840: train loss: 0.0003940603928640485\n",
      "Epoch 2841: train loss: 0.000448281760327518\n",
      "Epoch 2842: train loss: 0.00048383581452071667\n",
      "Epoch 2843: train loss: 0.0005032725748606026\n",
      "Epoch 2844: train loss: 0.0004752206150442362\n",
      "Epoch 2845: train loss: 0.0004258565604686737\n",
      "Epoch 2846: train loss: 0.0003217267512809485\n",
      "Epoch 2847: train loss: 0.000266663555521518\n",
      "Epoch 2848: train loss: 0.00017787367687560618\n",
      "Epoch 2849: train loss: 0.0001777678553480655\n",
      "Epoch 2850: train loss: 0.00015994810382835567\n",
      "Epoch 2851: train loss: 0.00015777266526129097\n",
      "Epoch 2852: train loss: 0.0001652489445405081\n",
      "Epoch 2853: train loss: 0.00017503670824225992\n",
      "Epoch 2854: train loss: 0.00021155716967768967\n",
      "Epoch 2855: train loss: 0.00021065484907012433\n",
      "Epoch 2856: train loss: 0.00024631418637000024\n",
      "Epoch 2857: train loss: 0.0001388520613545552\n",
      "Epoch 2858: train loss: 8.663807966513559e-05\n",
      "Epoch 2859: train loss: 0.0001391743717249483\n",
      "Epoch 2860: train loss: 0.00023440690711140633\n",
      "Epoch 2861: train loss: 0.00047629233449697495\n",
      "Epoch 2862: train loss: 0.00032457782072015107\n",
      "Epoch 2863: train loss: 0.00016682241403032094\n",
      "Epoch 2864: train loss: 0.0002620917512103915\n",
      "Epoch 2865: train loss: 0.00044316862476989627\n",
      "Epoch 2866: train loss: 0.0009991059778258204\n",
      "Epoch 2867: train loss: 0.0007312320522032678\n",
      "Epoch 2868: train loss: 0.0008849790901876986\n",
      "Epoch 2869: train loss: 0.001541890436783433\n",
      "Epoch 2870: train loss: 0.00027252547442913055\n",
      "Epoch 2871: train loss: 0.0025701888371258974\n",
      "Epoch 2872: train loss: 0.009740124456584454\n",
      "Epoch 2873: train loss: 0.008448678068816662\n",
      "Epoch 2874: train loss: 0.003237868193536997\n",
      "Epoch 2875: train loss: 0.011443566530942917\n",
      "Epoch 2876: train loss: 0.007440383546054363\n",
      "Epoch 2877: train loss: 0.022736988961696625\n",
      "Epoch 2878: train loss: 0.07476325333118439\n",
      "Epoch 2879: train loss: 0.023692365735769272\n",
      "Epoch 2880: train loss: 0.07461199909448624\n",
      "Epoch 2881: train loss: 0.04506973549723625\n",
      "Epoch 2882: train loss: 0.035138797014951706\n",
      "Epoch 2883: train loss: 0.03620120510458946\n",
      "Epoch 2884: train loss: 0.03327488899230957\n",
      "Epoch 2885: train loss: 0.01738729141652584\n",
      "Epoch 2886: train loss: 0.020909003913402557\n",
      "Epoch 2887: train loss: 0.02473333850502968\n",
      "Epoch 2888: train loss: 0.013979403302073479\n",
      "Epoch 2889: train loss: 0.013160215690732002\n",
      "Epoch 2890: train loss: 0.017154080793261528\n",
      "Epoch 2891: train loss: 0.012665214948356152\n",
      "Epoch 2892: train loss: 0.01333521492779255\n",
      "Epoch 2893: train loss: 0.013006205670535564\n",
      "Epoch 2894: train loss: 0.008650831878185272\n",
      "Epoch 2895: train loss: 0.00854418147355318\n",
      "Epoch 2896: train loss: 0.011928722262382507\n",
      "Epoch 2897: train loss: 0.009825669229030609\n",
      "Epoch 2898: train loss: 0.00628092885017395\n",
      "Epoch 2899: train loss: 0.007068329025059938\n",
      "Epoch 2900: train loss: 0.008299584500491619\n",
      "Epoch 2901: train loss: 0.0073190960101783276\n",
      "Epoch 2902: train loss: 0.006243074778467417\n",
      "Epoch 2903: train loss: 0.006223049480468035\n",
      "Epoch 2904: train loss: 0.0057441615499556065\n",
      "Epoch 2905: train loss: 0.004911036696285009\n",
      "Epoch 2906: train loss: 0.005513101350516081\n",
      "Epoch 2907: train loss: 0.0062027135863900185\n",
      "Epoch 2908: train loss: 0.004940032958984375\n",
      "Epoch 2909: train loss: 0.0038992848712950945\n",
      "Epoch 2910: train loss: 0.004222365096211433\n",
      "Epoch 2911: train loss: 0.004404708277434111\n",
      "Epoch 2912: train loss: 0.003954662941396236\n",
      "Epoch 2913: train loss: 0.003609794657677412\n",
      "Epoch 2914: train loss: 0.0034810230135917664\n",
      "Epoch 2915: train loss: 0.003244641935452819\n",
      "Epoch 2916: train loss: 0.0030344449914991856\n",
      "Epoch 2917: train loss: 0.003067938145250082\n",
      "Epoch 2918: train loss: 0.003096461296081543\n",
      "Epoch 2919: train loss: 0.002812827005982399\n",
      "Epoch 2920: train loss: 0.002554422477260232\n",
      "Epoch 2921: train loss: 0.0024450235068798065\n",
      "Epoch 2922: train loss: 0.0024755087215453386\n",
      "Epoch 2923: train loss: 0.0024865190498530865\n",
      "Epoch 2924: train loss: 0.0022568583954125643\n",
      "Epoch 2925: train loss: 0.002090378198772669\n",
      "Epoch 2926: train loss: 0.002038553822785616\n",
      "Epoch 2927: train loss: 0.0019731312058866024\n",
      "Epoch 2928: train loss: 0.0019326182082295418\n",
      "Epoch 2929: train loss: 0.001899826806038618\n",
      "Epoch 2930: train loss: 0.0017397438641637564\n",
      "Epoch 2931: train loss: 0.0016512407455593348\n",
      "Epoch 2932: train loss: 0.0016420558094978333\n",
      "Epoch 2933: train loss: 0.001642038463614881\n",
      "Epoch 2934: train loss: 0.0015855201054364443\n",
      "Epoch 2935: train loss: 0.001475796103477478\n",
      "Epoch 2936: train loss: 0.0014277563896030188\n",
      "Epoch 2937: train loss: 0.0014299617614597082\n",
      "Epoch 2938: train loss: 0.0013913812581449747\n",
      "Epoch 2939: train loss: 0.001351911691017449\n",
      "Epoch 2940: train loss: 0.0013129860162734985\n",
      "Epoch 2941: train loss: 0.0012773338239639997\n",
      "Epoch 2942: train loss: 0.0012445520842447877\n",
      "Epoch 2943: train loss: 0.001221293699927628\n",
      "Epoch 2944: train loss: 0.0012122341431677341\n",
      "Epoch 2945: train loss: 0.0011808256385847926\n",
      "Epoch 2946: train loss: 0.0011375179747119546\n",
      "Epoch 2947: train loss: 0.001118574058637023\n",
      "Epoch 2948: train loss: 0.0011087912134826183\n",
      "Epoch 2949: train loss: 0.0010935409227386117\n",
      "Epoch 2950: train loss: 0.0010611939942464232\n",
      "Epoch 2951: train loss: 0.001034783897921443\n",
      "Epoch 2952: train loss: 0.001021787291392684\n",
      "Epoch 2953: train loss: 0.0010058586485683918\n",
      "Epoch 2954: train loss: 0.000988444546237588\n",
      "Epoch 2955: train loss: 0.0009668006096035242\n",
      "Epoch 2956: train loss: 0.0009495093254372478\n",
      "Epoch 2957: train loss: 0.0009328376618213952\n",
      "Epoch 2958: train loss: 0.0009192647994495928\n",
      "Epoch 2959: train loss: 0.000905595428775996\n",
      "Epoch 2960: train loss: 0.0008898794185370207\n",
      "Epoch 2961: train loss: 0.0008738597971387208\n",
      "Epoch 2962: train loss: 0.0008598192362114787\n",
      "Epoch 2963: train loss: 0.0008490288746543229\n",
      "Epoch 2964: train loss: 0.0008368125418201089\n",
      "Epoch 2965: train loss: 0.0008241332834586501\n",
      "Epoch 2966: train loss: 0.0008106432505883276\n",
      "Epoch 2967: train loss: 0.0007996896747499704\n",
      "Epoch 2968: train loss: 0.000789647689089179\n",
      "Epoch 2969: train loss: 0.0007795746787451208\n",
      "Epoch 2970: train loss: 0.0007689551566727459\n",
      "Epoch 2971: train loss: 0.000757936795707792\n",
      "Epoch 2972: train loss: 0.000748313672374934\n",
      "Epoch 2973: train loss: 0.0007395652937702835\n",
      "Epoch 2974: train loss: 0.0007309080101549625\n",
      "Epoch 2975: train loss: 0.0007211920456029475\n",
      "Epoch 2976: train loss: 0.0007122744573280215\n",
      "Epoch 2977: train loss: 0.0007036997703835368\n",
      "Epoch 2978: train loss: 0.0006957428413443267\n",
      "Epoch 2979: train loss: 0.0006875988328829408\n",
      "Epoch 2980: train loss: 0.0006794115761294961\n",
      "Epoch 2981: train loss: 0.0006714221090078354\n",
      "Epoch 2982: train loss: 0.0006637019105255604\n",
      "Epoch 2983: train loss: 0.0006563300848938525\n",
      "Epoch 2984: train loss: 0.0006489840452559292\n",
      "Epoch 2985: train loss: 0.0006417746772058308\n",
      "Epoch 2986: train loss: 0.0006344632711261511\n",
      "Epoch 2987: train loss: 0.0006274742190726101\n",
      "Epoch 2988: train loss: 0.0006206539692357183\n",
      "Epoch 2989: train loss: 0.0006140628829598427\n",
      "Epoch 2990: train loss: 0.0006073206895962358\n",
      "Epoch 2991: train loss: 0.0006007191259413958\n",
      "Epoch 2992: train loss: 0.0005943044670857489\n",
      "Epoch 2993: train loss: 0.0005880826502107084\n",
      "Epoch 2994: train loss: 0.0005819139187224209\n",
      "Epoch 2995: train loss: 0.0005757285980507731\n",
      "Epoch 2996: train loss: 0.0005696853040717542\n",
      "Epoch 2997: train loss: 0.0005638023721985519\n",
      "Epoch 2998: train loss: 0.000558025436475873\n",
      "Epoch 2999: train loss: 0.0005522674764506519\n",
      "Epoch 3000: train loss: 0.0005466167349368334\n",
      "Epoch 3001: train loss: 0.0005410396261140704\n",
      "Epoch 3002: train loss: 0.0005355846951715648\n",
      "Epoch 3003: train loss: 0.000530177669133991\n",
      "Epoch 3004: train loss: 0.0005248778616078198\n",
      "Epoch 3005: train loss: 0.0005196285783313215\n",
      "Epoch 3006: train loss: 0.0005144496099092066\n",
      "Epoch 3007: train loss: 0.0005093536456115544\n",
      "Epoch 3008: train loss: 0.0005043555283918977\n",
      "Epoch 3009: train loss: 0.000499413872603327\n",
      "Epoch 3010: train loss: 0.0004945168038830161\n",
      "Epoch 3011: train loss: 0.0004896937170997262\n",
      "Epoch 3012: train loss: 0.00048496361705474555\n",
      "Epoch 3013: train loss: 0.00048029524623416364\n",
      "Epoch 3014: train loss: 0.0004756667767651379\n",
      "Epoch 3015: train loss: 0.00047110955347307026\n",
      "Epoch 3016: train loss: 0.0004666183958761394\n",
      "Epoch 3017: train loss: 0.0004621880652848631\n",
      "Epoch 3018: train loss: 0.0004578115767799318\n",
      "Epoch 3019: train loss: 0.00045349608990363777\n",
      "Epoch 3020: train loss: 0.00044923287350684404\n",
      "Epoch 3021: train loss: 0.0004450247506611049\n",
      "Epoch 3022: train loss: 0.00044087134301662445\n",
      "Epoch 3023: train loss: 0.0004367814981378615\n",
      "Epoch 3024: train loss: 0.00043273300980217755\n",
      "Epoch 3025: train loss: 0.0004287357733119279\n",
      "Epoch 3026: train loss: 0.0004247901670169085\n",
      "Epoch 3027: train loss: 0.0004208980535622686\n",
      "Epoch 3028: train loss: 0.0004170514293946326\n",
      "Epoch 3029: train loss: 0.00041324866469949484\n",
      "Epoch 3030: train loss: 0.00040949569665826857\n",
      "Epoch 3031: train loss: 0.000405788014177233\n",
      "Epoch 3032: train loss: 0.0004021264030598104\n",
      "Epoch 3033: train loss: 0.00039850815664976835\n",
      "Epoch 3034: train loss: 0.0003949331003241241\n",
      "Epoch 3035: train loss: 0.0003913972759619355\n",
      "Epoch 3036: train loss: 0.00038790664984844625\n",
      "Epoch 3037: train loss: 0.00038445842801593244\n",
      "Epoch 3038: train loss: 0.00038104565464891493\n",
      "Epoch 3039: train loss: 0.0003776720550376922\n",
      "Epoch 3040: train loss: 0.00037433853140100837\n",
      "Epoch 3041: train loss: 0.00037104528746567667\n",
      "Epoch 3042: train loss: 0.00036778629873879254\n",
      "Epoch 3043: train loss: 0.00036456360248848796\n",
      "Epoch 3044: train loss: 0.00036137737333774567\n",
      "Epoch 3045: train loss: 0.00035822586505673826\n",
      "Epoch 3046: train loss: 0.0003551082336343825\n",
      "Epoch 3047: train loss: 0.00035202575963921845\n",
      "Epoch 3048: train loss: 0.00034897535806521773\n",
      "Epoch 3049: train loss: 0.000345958600519225\n",
      "Epoch 3050: train loss: 0.0003429744974710047\n",
      "Epoch 3051: train loss: 0.000340020953444764\n",
      "Epoch 3052: train loss: 0.00033709994750097394\n",
      "Epoch 3053: train loss: 0.0003342068230267614\n",
      "Epoch 3054: train loss: 0.0003313470515422523\n",
      "Epoch 3055: train loss: 0.0003285140555817634\n",
      "Epoch 3056: train loss: 0.00032571208430454135\n",
      "Epoch 3057: train loss: 0.0003229366848245263\n",
      "Epoch 3058: train loss: 0.0003201905929017812\n",
      "Epoch 3059: train loss: 0.0003174749144818634\n",
      "Epoch 3060: train loss: 0.0003147812676616013\n",
      "Epoch 3061: train loss: 0.00031211748137138784\n",
      "Epoch 3062: train loss: 0.00030948017956689\n",
      "Epoch 3063: train loss: 0.0003068704972974956\n",
      "Epoch 3064: train loss: 0.0003042855823878199\n",
      "Epoch 3065: train loss: 0.0003017266863025725\n",
      "Epoch 3066: train loss: 0.0002991936053149402\n",
      "Epoch 3067: train loss: 0.00029668459319509566\n",
      "Epoch 3068: train loss: 0.0002942006103694439\n",
      "Epoch 3069: train loss: 0.0002917409292422235\n",
      "Epoch 3070: train loss: 0.00028930677217431366\n",
      "Epoch 3071: train loss: 0.00028689613100141287\n",
      "Epoch 3072: train loss: 0.00028451011166907847\n",
      "Epoch 3073: train loss: 0.00028214664780534804\n",
      "Epoch 3074: train loss: 0.00027980635059066117\n",
      "Epoch 3075: train loss: 0.0002774900058284402\n",
      "Epoch 3076: train loss: 0.0002751972642727196\n",
      "Epoch 3077: train loss: 0.0002729261468630284\n",
      "Epoch 3078: train loss: 0.0002706779632717371\n",
      "Epoch 3079: train loss: 0.0002684523060452193\n",
      "Epoch 3080: train loss: 0.000266249873675406\n",
      "Epoch 3081: train loss: 0.00026407005498185754\n",
      "Epoch 3082: train loss: 0.00026191104552708566\n",
      "Epoch 3083: train loss: 0.0002597726124804467\n",
      "Epoch 3084: train loss: 0.0002576586848590523\n",
      "Epoch 3085: train loss: 0.0002555651590228081\n",
      "Epoch 3086: train loss: 0.0002534933155402541\n",
      "Epoch 3087: train loss: 0.00025144327082671225\n",
      "Epoch 3088: train loss: 0.00024941383162513375\n",
      "Epoch 3089: train loss: 0.00024740537628531456\n",
      "Epoch 3090: train loss: 0.0002454177592881024\n",
      "Epoch 3091: train loss: 0.0002434516791254282\n",
      "Epoch 3092: train loss: 0.00024150556419044733\n",
      "Epoch 3093: train loss: 0.0002395807532593608\n",
      "Epoch 3094: train loss: 0.0002376746851950884\n",
      "Epoch 3095: train loss: 0.00023579117259941995\n",
      "Epoch 3096: train loss: 0.00023392733419314027\n",
      "Epoch 3097: train loss: 0.00023208199127111584\n",
      "Epoch 3098: train loss: 0.00023025671544019133\n",
      "Epoch 3099: train loss: 0.00022845198691356927\n",
      "Epoch 3100: train loss: 0.0002266659284941852\n",
      "Epoch 3101: train loss: 0.0002249017561553046\n",
      "Epoch 3102: train loss: 0.00022315423120744526\n",
      "Epoch 3103: train loss: 0.00022142723901197314\n",
      "Epoch 3104: train loss: 0.0002197188005084172\n",
      "Epoch 3105: train loss: 0.00021802941046189517\n",
      "Epoch 3106: train loss: 0.00021635858865920454\n",
      "Epoch 3107: train loss: 0.00021470672800205648\n",
      "Epoch 3108: train loss: 0.00021307211136445403\n",
      "Epoch 3109: train loss: 0.00021145645587239414\n",
      "Epoch 3110: train loss: 0.00020985773880966008\n",
      "Epoch 3111: train loss: 0.00020827785192523152\n",
      "Epoch 3112: train loss: 0.00020671547099482268\n",
      "Epoch 3113: train loss: 0.00020516995573416352\n",
      "Epoch 3114: train loss: 0.0002036418445641175\n",
      "Epoch 3115: train loss: 0.00020213279640302062\n",
      "Epoch 3116: train loss: 0.00020063840202055871\n",
      "Epoch 3117: train loss: 0.00019916181918233633\n",
      "Epoch 3118: train loss: 0.00019770211656577885\n",
      "Epoch 3119: train loss: 0.00019625830464065075\n",
      "Epoch 3120: train loss: 0.00019483148935250938\n",
      "Epoch 3121: train loss: 0.00019341972074471414\n",
      "Epoch 3122: train loss: 0.00019202478870283812\n",
      "Epoch 3123: train loss: 0.00019064515072386712\n",
      "Epoch 3124: train loss: 0.00018928047211375087\n",
      "Epoch 3125: train loss: 0.00018793220806401223\n",
      "Epoch 3126: train loss: 0.0001865988306235522\n",
      "Epoch 3127: train loss: 0.00018528076179791242\n",
      "Epoch 3128: train loss: 0.00018397609528619796\n",
      "Epoch 3129: train loss: 0.00018268669373355806\n",
      "Epoch 3130: train loss: 0.0001814122369978577\n",
      "Epoch 3131: train loss: 0.0001801513571990654\n",
      "Epoch 3132: train loss: 0.00017890555318444967\n",
      "Epoch 3133: train loss: 0.00017767456301953644\n",
      "Epoch 3134: train loss: 0.000176454967004247\n",
      "Epoch 3135: train loss: 0.0001752508687786758\n",
      "Epoch 3136: train loss: 0.00017405921244062483\n",
      "Epoch 3137: train loss: 0.00017288088565692306\n",
      "Epoch 3138: train loss: 0.00017171638319268823\n",
      "Epoch 3139: train loss: 0.0001705637259874493\n",
      "Epoch 3140: train loss: 0.00016942527145147324\n",
      "Epoch 3141: train loss: 0.0001682969304965809\n",
      "Epoch 3142: train loss: 0.00016718269034754485\n",
      "Epoch 3143: train loss: 0.00016608125588390976\n",
      "Epoch 3144: train loss: 0.0001649896876187995\n",
      "Epoch 3145: train loss: 0.00016391166718676686\n",
      "Epoch 3146: train loss: 0.0001628449681447819\n",
      "Epoch 3147: train loss: 0.00016179087106138468\n",
      "Epoch 3148: train loss: 0.0001607462763786316\n",
      "Epoch 3149: train loss: 0.00015971461834851652\n",
      "Epoch 3150: train loss: 0.00015869323397055268\n",
      "Epoch 3151: train loss: 0.0001576821377966553\n",
      "Epoch 3152: train loss: 0.0001566831924719736\n",
      "Epoch 3153: train loss: 0.00015569436072837561\n",
      "Epoch 3154: train loss: 0.000154715875396505\n",
      "Epoch 3155: train loss: 0.00015374862414319068\n",
      "Epoch 3156: train loss: 0.00015279039507731795\n",
      "Epoch 3157: train loss: 0.00015184331277851015\n",
      "Epoch 3158: train loss: 0.00015090576198417693\n",
      "Epoch 3159: train loss: 0.00014997783000580966\n",
      "Epoch 3160: train loss: 0.00014905947318766266\n",
      "Epoch 3161: train loss: 0.00014815102622378618\n",
      "Epoch 3162: train loss: 0.00014725272194482386\n",
      "Epoch 3163: train loss: 0.0001463625958422199\n",
      "Epoch 3164: train loss: 0.00014548255421686918\n",
      "Epoch 3165: train loss: 0.00014461055980063975\n",
      "Epoch 3166: train loss: 0.00014374744205269963\n",
      "Epoch 3167: train loss: 0.0001428938121534884\n",
      "Epoch 3168: train loss: 0.00014204756007529795\n",
      "Epoch 3169: train loss: 0.0001412114652339369\n",
      "Epoch 3170: train loss: 0.00014038309745956212\n",
      "Epoch 3171: train loss: 0.00013956328621134162\n",
      "Epoch 3172: train loss: 0.00013875169679522514\n",
      "Epoch 3173: train loss: 0.00013794789265375584\n",
      "Epoch 3174: train loss: 0.00013715236855205148\n",
      "Epoch 3175: train loss: 0.00013636500807479024\n",
      "Epoch 3176: train loss: 0.00013558511273004115\n",
      "Epoch 3177: train loss: 0.0001348123769275844\n",
      "Epoch 3178: train loss: 0.0001340474555036053\n",
      "Epoch 3179: train loss: 0.00013329015928320587\n",
      "Epoch 3180: train loss: 0.00013254021177999675\n",
      "Epoch 3181: train loss: 0.00013179678353480995\n",
      "Epoch 3182: train loss: 0.00013106231926940382\n",
      "Epoch 3183: train loss: 0.0001303332974202931\n",
      "Epoch 3184: train loss: 0.00012961254105903208\n",
      "Epoch 3185: train loss: 0.00012889767822343856\n",
      "Epoch 3186: train loss: 0.0001281901786569506\n",
      "Epoch 3187: train loss: 0.00012748909648507833\n",
      "Epoch 3188: train loss: 0.00012679411156568676\n",
      "Epoch 3189: train loss: 0.00012610644625965506\n",
      "Epoch 3190: train loss: 0.00012542576587293297\n",
      "Epoch 3191: train loss: 0.00012475055700633675\n",
      "Epoch 3192: train loss: 0.00012408156180754304\n",
      "Epoch 3193: train loss: 0.0001234196242876351\n",
      "Epoch 3194: train loss: 0.00012276327470317483\n",
      "Epoch 3195: train loss: 0.0001221132988575846\n",
      "Epoch 3196: train loss: 0.00012146882363595068\n",
      "Epoch 3197: train loss: 0.00012083059118594974\n",
      "Epoch 3198: train loss: 0.0001201981576741673\n",
      "Epoch 3199: train loss: 0.00011957152310060337\n",
      "Epoch 3200: train loss: 0.00011895040370291099\n",
      "Epoch 3201: train loss: 0.00011833557800855488\n",
      "Epoch 3202: train loss: 0.0001177254380309023\n",
      "Epoch 3203: train loss: 0.00011712162813637406\n",
      "Epoch 3204: train loss: 0.00011652255489025265\n",
      "Epoch 3205: train loss: 0.0001159285384346731\n",
      "Epoch 3206: train loss: 0.00011534026998560876\n",
      "Epoch 3207: train loss: 0.00011475803330540657\n",
      "Epoch 3208: train loss: 0.00011417952191550285\n",
      "Epoch 3209: train loss: 0.00011360750067979097\n",
      "Epoch 3210: train loss: 0.00011304011422907934\n",
      "Epoch 3211: train loss: 0.00011247710790485144\n",
      "Epoch 3212: train loss: 0.00011192005331395194\n",
      "Epoch 3213: train loss: 0.00011136772081954405\n",
      "Epoch 3214: train loss: 0.0001108189026126638\n",
      "Epoch 3215: train loss: 0.00011027623986592516\n",
      "Epoch 3216: train loss: 0.00010973784810630605\n",
      "Epoch 3217: train loss: 0.00010920457862084731\n",
      "Epoch 3218: train loss: 0.00010867472883546725\n",
      "Epoch 3219: train loss: 0.00010815019049914554\n",
      "Epoch 3220: train loss: 0.0001076307162293233\n",
      "Epoch 3221: train loss: 0.00010711502545746043\n",
      "Epoch 3222: train loss: 0.00010660357656888664\n",
      "Epoch 3223: train loss: 0.0001060967770172283\n",
      "Epoch 3224: train loss: 0.00010559416114119813\n",
      "Epoch 3225: train loss: 0.00010509636922506616\n",
      "Epoch 3226: train loss: 0.00010460199700901285\n",
      "Epoch 3227: train loss: 0.00010411249240860343\n",
      "Epoch 3228: train loss: 0.00010362668399466202\n",
      "Epoch 3229: train loss: 0.00010314459359506145\n",
      "Epoch 3230: train loss: 0.00010266677418258041\n",
      "Epoch 3231: train loss: 0.00010219321120530367\n",
      "Epoch 3232: train loss: 0.00010172386100748554\n",
      "Epoch 3233: train loss: 0.00010125822882400826\n",
      "Epoch 3234: train loss: 0.00010079576168209314\n",
      "Epoch 3235: train loss: 0.00010033807484433055\n",
      "Epoch 3236: train loss: 9.988331294152886e-05\n",
      "Epoch 3237: train loss: 9.943320765160024e-05\n",
      "Epoch 3238: train loss: 9.898566349875182e-05\n",
      "Epoch 3239: train loss: 9.854264499153942e-05\n",
      "Epoch 3240: train loss: 9.810282790567726e-05\n",
      "Epoch 3241: train loss: 9.766688162926584e-05\n",
      "Epoch 3242: train loss: 9.723478433443233e-05\n",
      "Epoch 3243: train loss: 9.680545190349221e-05\n",
      "Epoch 3244: train loss: 9.637997573008761e-05\n",
      "Epoch 3245: train loss: 9.595759183866903e-05\n",
      "Epoch 3246: train loss: 9.553886775393039e-05\n",
      "Epoch 3247: train loss: 9.512400720268488e-05\n",
      "Epoch 3248: train loss: 9.471189696341753e-05\n",
      "Epoch 3249: train loss: 9.430352656636387e-05\n",
      "Epoch 3250: train loss: 9.3897404440213e-05\n",
      "Epoch 3251: train loss: 9.349499305244535e-05\n",
      "Epoch 3252: train loss: 9.309594315709546e-05\n",
      "Epoch 3253: train loss: 9.27001892705448e-05\n",
      "Epoch 3254: train loss: 9.230730211129412e-05\n",
      "Epoch 3255: train loss: 9.191766730509698e-05\n",
      "Epoch 3256: train loss: 9.153108112514019e-05\n",
      "Epoch 3257: train loss: 9.114733984461054e-05\n",
      "Epoch 3258: train loss: 9.076685091713443e-05\n",
      "Epoch 3259: train loss: 9.038883581524715e-05\n",
      "Epoch 3260: train loss: 9.001469879876822e-05\n",
      "Epoch 3261: train loss: 8.964256994659081e-05\n",
      "Epoch 3262: train loss: 8.927371527533978e-05\n",
      "Epoch 3263: train loss: 8.890802564565092e-05\n",
      "Epoch 3264: train loss: 8.854403131408617e-05\n",
      "Epoch 3265: train loss: 8.818410424282774e-05\n",
      "Epoch 3266: train loss: 8.782619261182845e-05\n",
      "Epoch 3267: train loss: 8.747127867536619e-05\n",
      "Epoch 3268: train loss: 8.71193697093986e-05\n",
      "Epoch 3269: train loss: 8.676952711539343e-05\n",
      "Epoch 3270: train loss: 8.642254397273064e-05\n",
      "Epoch 3271: train loss: 8.60778454807587e-05\n",
      "Epoch 3272: train loss: 8.573671220801771e-05\n",
      "Epoch 3273: train loss: 8.539753616787493e-05\n",
      "Epoch 3274: train loss: 8.506078302161768e-05\n",
      "Epoch 3275: train loss: 8.472733316011727e-05\n",
      "Epoch 3276: train loss: 8.439536031801254e-05\n",
      "Epoch 3277: train loss: 8.406605775235221e-05\n",
      "Epoch 3278: train loss: 8.373979653697461e-05\n",
      "Epoch 3279: train loss: 8.341548527823761e-05\n",
      "Epoch 3280: train loss: 8.309365512104705e-05\n",
      "Epoch 3281: train loss: 8.277479355456308e-05\n",
      "Epoch 3282: train loss: 8.24576272862032e-05\n",
      "Epoch 3283: train loss: 8.21423382149078e-05\n",
      "Epoch 3284: train loss: 8.18300322862342e-05\n",
      "Epoch 3285: train loss: 8.152025839081034e-05\n",
      "Epoch 3286: train loss: 8.121252176351845e-05\n",
      "Epoch 3287: train loss: 8.090707706287503e-05\n",
      "Epoch 3288: train loss: 8.060340769588947e-05\n",
      "Epoch 3289: train loss: 8.030220487853512e-05\n",
      "Epoch 3290: train loss: 8.000351226655766e-05\n",
      "Epoch 3291: train loss: 7.970704609761015e-05\n",
      "Epoch 3292: train loss: 7.941204967210069e-05\n",
      "Epoch 3293: train loss: 7.911968714324757e-05\n",
      "Epoch 3294: train loss: 7.882902718847618e-05\n",
      "Epoch 3295: train loss: 7.854076102375984e-05\n",
      "Epoch 3296: train loss: 7.82548522693105e-05\n",
      "Epoch 3297: train loss: 7.796994032105431e-05\n",
      "Epoch 3298: train loss: 7.768808427499607e-05\n",
      "Epoch 3299: train loss: 7.74079016991891e-05\n",
      "Epoch 3300: train loss: 7.712929073022678e-05\n",
      "Epoch 3301: train loss: 7.6853517384734e-05\n",
      "Epoch 3302: train loss: 7.657904643565416e-05\n",
      "Epoch 3303: train loss: 7.63068746891804e-05\n",
      "Epoch 3304: train loss: 7.603624544572085e-05\n",
      "Epoch 3305: train loss: 7.576755888294429e-05\n",
      "Epoch 3306: train loss: 7.550074224127457e-05\n",
      "Epoch 3307: train loss: 7.523629028582945e-05\n",
      "Epoch 3308: train loss: 7.497326441807672e-05\n",
      "Epoch 3309: train loss: 7.471205026376992e-05\n",
      "Epoch 3310: train loss: 7.445253140758723e-05\n",
      "Epoch 3311: train loss: 7.419487519655377e-05\n",
      "Epoch 3312: train loss: 7.393884152406827e-05\n",
      "Epoch 3313: train loss: 7.368478691205382e-05\n",
      "Epoch 3314: train loss: 7.343222387135029e-05\n",
      "Epoch 3315: train loss: 7.318241114262491e-05\n",
      "Epoch 3316: train loss: 7.293331145774573e-05\n",
      "Epoch 3317: train loss: 7.2686088969931e-05\n",
      "Epoch 3318: train loss: 7.24404671927914e-05\n",
      "Epoch 3319: train loss: 7.21966425771825e-05\n",
      "Epoch 3320: train loss: 7.19537420081906e-05\n",
      "Epoch 3321: train loss: 7.171372271841392e-05\n",
      "Epoch 3322: train loss: 7.147482392610982e-05\n",
      "Epoch 3323: train loss: 7.12375549483113e-05\n",
      "Epoch 3324: train loss: 7.100163929862902e-05\n",
      "Epoch 3325: train loss: 7.076738256728277e-05\n",
      "Epoch 3326: train loss: 7.053490116959438e-05\n",
      "Epoch 3327: train loss: 7.030404958641157e-05\n",
      "Epoch 3328: train loss: 7.007420208537951e-05\n",
      "Epoch 3329: train loss: 6.984598439885303e-05\n",
      "Epoch 3330: train loss: 6.961990584386513e-05\n",
      "Epoch 3331: train loss: 6.939446029718965e-05\n",
      "Epoch 3332: train loss: 6.917055725352839e-05\n",
      "Epoch 3333: train loss: 6.894853140693158e-05\n",
      "Epoch 3334: train loss: 6.872737139929086e-05\n",
      "Epoch 3335: train loss: 6.85084451106377e-05\n",
      "Epoch 3336: train loss: 6.829106132499874e-05\n",
      "Epoch 3337: train loss: 6.807463796576485e-05\n",
      "Epoch 3338: train loss: 6.785942241549492e-05\n",
      "Epoch 3339: train loss: 6.764564022887498e-05\n",
      "Epoch 3340: train loss: 6.743382255081087e-05\n",
      "Epoch 3341: train loss: 6.722252874169499e-05\n",
      "Epoch 3342: train loss: 6.70134904794395e-05\n",
      "Epoch 3343: train loss: 6.68051143293269e-05\n",
      "Epoch 3344: train loss: 6.659774953732267e-05\n",
      "Epoch 3345: train loss: 6.639240746153519e-05\n",
      "Epoch 3346: train loss: 6.618791667278856e-05\n",
      "Epoch 3347: train loss: 6.598519394174218e-05\n",
      "Epoch 3348: train loss: 6.57829295960255e-05\n",
      "Epoch 3349: train loss: 6.558285531355068e-05\n",
      "Epoch 3350: train loss: 6.538334127981216e-05\n",
      "Epoch 3351: train loss: 6.518542795674875e-05\n",
      "Epoch 3352: train loss: 6.498884613392875e-05\n",
      "Epoch 3353: train loss: 6.479329022113234e-05\n",
      "Epoch 3354: train loss: 6.459908763645217e-05\n",
      "Epoch 3355: train loss: 6.440651486627758e-05\n",
      "Epoch 3356: train loss: 6.421429134206846e-05\n",
      "Epoch 3357: train loss: 6.402354483725503e-05\n",
      "Epoch 3358: train loss: 6.383370782714337e-05\n",
      "Epoch 3359: train loss: 6.364610453601927e-05\n",
      "Epoch 3360: train loss: 6.345813017105684e-05\n",
      "Epoch 3361: train loss: 6.327190203592181e-05\n",
      "Epoch 3362: train loss: 6.30874274065718e-05\n",
      "Epoch 3363: train loss: 6.290324381552637e-05\n",
      "Epoch 3364: train loss: 6.272087921388447e-05\n",
      "Epoch 3365: train loss: 6.253932224353775e-05\n",
      "Epoch 3366: train loss: 6.23588275630027e-05\n",
      "Epoch 3367: train loss: 6.217926420504227e-05\n",
      "Epoch 3368: train loss: 6.20005521341227e-05\n",
      "Epoch 3369: train loss: 6.182333163451403e-05\n",
      "Epoch 3370: train loss: 6.164701335364953e-05\n",
      "Epoch 3371: train loss: 6.147247040644288e-05\n",
      "Epoch 3372: train loss: 6.129789835540578e-05\n",
      "Epoch 3373: train loss: 6.11246723565273e-05\n",
      "Epoch 3374: train loss: 6.095251956139691e-05\n",
      "Epoch 3375: train loss: 6.078118894947693e-05\n",
      "Epoch 3376: train loss: 6.061138992663473e-05\n",
      "Epoch 3377: train loss: 6.044224573997781e-05\n",
      "Epoch 3378: train loss: 6.027436756994575e-05\n",
      "Epoch 3379: train loss: 6.010681681800634e-05\n",
      "Epoch 3380: train loss: 5.9941110521322116e-05\n",
      "Epoch 3381: train loss: 5.9776066336780787e-05\n",
      "Epoch 3382: train loss: 5.9611447795759887e-05\n",
      "Epoch 3383: train loss: 5.944834992988035e-05\n",
      "Epoch 3384: train loss: 5.9286077885190025e-05\n",
      "Epoch 3385: train loss: 5.912462074775249e-05\n",
      "Epoch 3386: train loss: 5.896378934266977e-05\n",
      "Epoch 3387: train loss: 5.880443859496154e-05\n",
      "Epoch 3388: train loss: 5.864582271897234e-05\n",
      "Epoch 3389: train loss: 5.848812361364253e-05\n",
      "Epoch 3390: train loss: 5.833134491695091e-05\n",
      "Epoch 3391: train loss: 5.817541750730015e-05\n",
      "Epoch 3392: train loss: 5.8020217693410814e-05\n",
      "Epoch 3393: train loss: 5.7865556300384924e-05\n",
      "Epoch 3394: train loss: 5.7712481066118926e-05\n",
      "Epoch 3395: train loss: 5.755998063250445e-05\n",
      "Epoch 3396: train loss: 5.740812775911763e-05\n",
      "Epoch 3397: train loss: 5.725757000618614e-05\n",
      "Epoch 3398: train loss: 5.710791810997762e-05\n",
      "Epoch 3399: train loss: 5.695848085451871e-05\n",
      "Epoch 3400: train loss: 5.681020047632046e-05\n",
      "Epoch 3401: train loss: 5.666236029355787e-05\n",
      "Epoch 3402: train loss: 5.651571700582281e-05\n",
      "Epoch 3403: train loss: 5.637023423332721e-05\n",
      "Epoch 3404: train loss: 5.622528260573745e-05\n",
      "Epoch 3405: train loss: 5.6080731155816466e-05\n",
      "Epoch 3406: train loss: 5.5937114666448906e-05\n",
      "Epoch 3407: train loss: 5.579460776061751e-05\n",
      "Epoch 3408: train loss: 5.565256651607342e-05\n",
      "Epoch 3409: train loss: 5.551124922931194e-05\n",
      "Epoch 3410: train loss: 5.537104880204424e-05\n",
      "Epoch 3411: train loss: 5.5231255828402936e-05\n",
      "Epoch 3412: train loss: 5.5092008551582694e-05\n",
      "Epoch 3413: train loss: 5.4954158258624375e-05\n",
      "Epoch 3414: train loss: 5.481667540152557e-05\n",
      "Epoch 3415: train loss: 5.468004383146763e-05\n",
      "Epoch 3416: train loss: 5.454400888993405e-05\n",
      "Epoch 3417: train loss: 5.440859968075529e-05\n",
      "Epoch 3418: train loss: 5.427408905234188e-05\n",
      "Epoch 3419: train loss: 5.41405170224607e-05\n",
      "Epoch 3420: train loss: 5.400730879046023e-05\n",
      "Epoch 3421: train loss: 5.387448982219212e-05\n",
      "Epoch 3422: train loss: 5.374312604544684e-05\n",
      "Epoch 3423: train loss: 5.36116749572102e-05\n",
      "Epoch 3424: train loss: 5.3481733630178496e-05\n",
      "Epoch 3425: train loss: 5.3351755923358724e-05\n",
      "Epoch 3426: train loss: 5.3222775022732094e-05\n",
      "Epoch 3427: train loss: 5.30946854269132e-05\n",
      "Epoch 3428: train loss: 5.29671051481273e-05\n",
      "Epoch 3429: train loss: 5.2840321586700156e-05\n",
      "Epoch 3430: train loss: 5.2713356126332656e-05\n",
      "Epoch 3431: train loss: 5.25875948369503e-05\n",
      "Epoch 3432: train loss: 5.246271757641807e-05\n",
      "Epoch 3433: train loss: 5.233795673120767e-05\n",
      "Epoch 3434: train loss: 5.221429455559701e-05\n",
      "Epoch 3435: train loss: 5.2091294492129236e-05\n",
      "Epoch 3436: train loss: 5.1968567277072e-05\n",
      "Epoch 3437: train loss: 5.1846731366822496e-05\n",
      "Epoch 3438: train loss: 5.1725168304983526e-05\n",
      "Epoch 3439: train loss: 5.160434011486359e-05\n",
      "Epoch 3440: train loss: 5.148411946720444e-05\n",
      "Epoch 3441: train loss: 5.136515392223373e-05\n",
      "Epoch 3442: train loss: 5.124625749886036e-05\n",
      "Epoch 3443: train loss: 5.112765575177036e-05\n",
      "Epoch 3444: train loss: 5.101008719066158e-05\n",
      "Epoch 3445: train loss: 5.0892671424662694e-05\n",
      "Epoch 3446: train loss: 5.077567038824782e-05\n",
      "Epoch 3447: train loss: 5.0659979024203494e-05\n",
      "Epoch 3448: train loss: 5.054476059740409e-05\n",
      "Epoch 3449: train loss: 5.0429411203367636e-05\n",
      "Epoch 3450: train loss: 5.031514592701569e-05\n",
      "Epoch 3451: train loss: 5.0201433623442426e-05\n",
      "Epoch 3452: train loss: 5.0088252464774996e-05\n",
      "Epoch 3453: train loss: 4.9975758884102106e-05\n",
      "Epoch 3454: train loss: 4.9863730964716524e-05\n",
      "Epoch 3455: train loss: 4.9752237828215584e-05\n",
      "Epoch 3456: train loss: 4.9641395889921114e-05\n",
      "Epoch 3457: train loss: 4.9530488468008116e-05\n",
      "Epoch 3458: train loss: 4.942030500387773e-05\n",
      "Epoch 3459: train loss: 4.9310932809021324e-05\n",
      "Epoch 3460: train loss: 4.920213541481644e-05\n",
      "Epoch 3461: train loss: 4.909384733764455e-05\n",
      "Epoch 3462: train loss: 4.898575207334943e-05\n",
      "Epoch 3463: train loss: 4.887852628598921e-05\n",
      "Epoch 3464: train loss: 4.877126048086211e-05\n",
      "Epoch 3465: train loss: 4.8665016947779804e-05\n",
      "Epoch 3466: train loss: 4.85597483930178e-05\n",
      "Epoch 3467: train loss: 4.845407966058701e-05\n",
      "Epoch 3468: train loss: 4.83493713545613e-05\n",
      "Epoch 3469: train loss: 4.8244837671518326e-05\n",
      "Epoch 3470: train loss: 4.8140540457097813e-05\n",
      "Epoch 3471: train loss: 4.803739648195915e-05\n",
      "Epoch 3472: train loss: 4.793454718310386e-05\n",
      "Epoch 3473: train loss: 4.783190888701938e-05\n",
      "Epoch 3474: train loss: 4.773002001456916e-05\n",
      "Epoch 3475: train loss: 4.762860044138506e-05\n",
      "Epoch 3476: train loss: 4.752716631628573e-05\n",
      "Epoch 3477: train loss: 4.7426961828023195e-05\n",
      "Epoch 3478: train loss: 4.7326666390290484e-05\n",
      "Epoch 3479: train loss: 4.7226880269590765e-05\n",
      "Epoch 3480: train loss: 4.712761437986046e-05\n",
      "Epoch 3481: train loss: 4.7029061533976346e-05\n",
      "Epoch 3482: train loss: 4.6930916141718626e-05\n",
      "Epoch 3483: train loss: 4.6832858060952276e-05\n",
      "Epoch 3484: train loss: 4.673545845435001e-05\n",
      "Epoch 3485: train loss: 4.663817890104838e-05\n",
      "Epoch 3486: train loss: 4.654168151319027e-05\n",
      "Epoch 3487: train loss: 4.644611544790678e-05\n",
      "Epoch 3488: train loss: 4.6349923650268465e-05\n",
      "Epoch 3489: train loss: 4.625483779818751e-05\n",
      "Epoch 3490: train loss: 4.616024307324551e-05\n",
      "Epoch 3491: train loss: 4.6065757487667724e-05\n",
      "Epoch 3492: train loss: 4.597153383656405e-05\n",
      "Epoch 3493: train loss: 4.5877895900048316e-05\n",
      "Epoch 3494: train loss: 4.5784752728650346e-05\n",
      "Epoch 3495: train loss: 4.5691787818213925e-05\n",
      "Epoch 3496: train loss: 4.5599834265885875e-05\n",
      "Epoch 3497: train loss: 4.55075605714228e-05\n",
      "Epoch 3498: train loss: 4.541607268038206e-05\n",
      "Epoch 3499: train loss: 4.5324861275730655e-05\n",
      "Epoch 3500: train loss: 4.523432289715856e-05\n",
      "Epoch 3501: train loss: 4.5143766328692436e-05\n",
      "Epoch 3502: train loss: 4.5054028305457905e-05\n",
      "Epoch 3503: train loss: 4.496391920838505e-05\n",
      "Epoch 3504: train loss: 4.487497062655166e-05\n",
      "Epoch 3505: train loss: 4.478650589589961e-05\n",
      "Epoch 3506: train loss: 4.469794293981977e-05\n",
      "Epoch 3507: train loss: 4.460950003704056e-05\n",
      "Epoch 3508: train loss: 4.4522093958221376e-05\n",
      "Epoch 3509: train loss: 4.443473881110549e-05\n",
      "Epoch 3510: train loss: 4.434760921867564e-05\n",
      "Epoch 3511: train loss: 4.4261145376367494e-05\n",
      "Epoch 3512: train loss: 4.41749652964063e-05\n",
      "Epoch 3513: train loss: 4.408917448017746e-05\n",
      "Epoch 3514: train loss: 4.4003507355228066e-05\n",
      "Epoch 3515: train loss: 4.391888796817511e-05\n",
      "Epoch 3516: train loss: 4.383379564387724e-05\n",
      "Epoch 3517: train loss: 4.374933268991299e-05\n",
      "Epoch 3518: train loss: 4.366517532616854e-05\n",
      "Epoch 3519: train loss: 4.35819492849987e-05\n",
      "Epoch 3520: train loss: 4.349821028881706e-05\n",
      "Epoch 3521: train loss: 4.341552630648948e-05\n",
      "Epoch 3522: train loss: 4.3332940549589694e-05\n",
      "Epoch 3523: train loss: 4.325067129684612e-05\n",
      "Epoch 3524: train loss: 4.316847480367869e-05\n",
      "Epoch 3525: train loss: 4.308684583520517e-05\n",
      "Epoch 3526: train loss: 4.300556975067593e-05\n",
      "Epoch 3527: train loss: 4.292479934520088e-05\n",
      "Epoch 3528: train loss: 4.284373426344246e-05\n",
      "Epoch 3529: train loss: 4.2763767851283774e-05\n",
      "Epoch 3530: train loss: 4.26834485551808e-05\n",
      "Epoch 3531: train loss: 4.2604038753779605e-05\n",
      "Epoch 3532: train loss: 4.252469079801813e-05\n",
      "Epoch 3533: train loss: 4.244559022481553e-05\n",
      "Epoch 3534: train loss: 4.236697350279428e-05\n",
      "Epoch 3535: train loss: 4.228862962918356e-05\n",
      "Epoch 3536: train loss: 4.2210180254187435e-05\n",
      "Epoch 3537: train loss: 4.213290958432481e-05\n",
      "Epoch 3538: train loss: 4.205500954412855e-05\n",
      "Epoch 3539: train loss: 4.197789166937582e-05\n",
      "Epoch 3540: train loss: 4.1900970245478675e-05\n",
      "Epoch 3541: train loss: 4.182468182989396e-05\n",
      "Epoch 3542: train loss: 4.174821879132651e-05\n",
      "Epoch 3543: train loss: 4.1672316001495346e-05\n",
      "Epoch 3544: train loss: 4.1596911614760756e-05\n",
      "Epoch 3545: train loss: 4.152151450398378e-05\n",
      "Epoch 3546: train loss: 4.144632475799881e-05\n",
      "Epoch 3547: train loss: 4.137156065553427e-05\n",
      "Epoch 3548: train loss: 4.129701846977696e-05\n",
      "Epoch 3549: train loss: 4.1222920117434114e-05\n",
      "Epoch 3550: train loss: 4.11492474086117e-05\n",
      "Epoch 3551: train loss: 4.107590575586073e-05\n",
      "Epoch 3552: train loss: 4.100228034076281e-05\n",
      "Epoch 3553: train loss: 4.0929382521426305e-05\n",
      "Epoch 3554: train loss: 4.0856568375602365e-05\n",
      "Epoch 3555: train loss: 4.078403799212538e-05\n",
      "Epoch 3556: train loss: 4.071158036822453e-05\n",
      "Epoch 3557: train loss: 4.063989399583079e-05\n",
      "Epoch 3558: train loss: 4.0568171243648976e-05\n",
      "Epoch 3559: train loss: 4.049708877573721e-05\n",
      "Epoch 3560: train loss: 4.042596629005857e-05\n",
      "Epoch 3561: train loss: 4.035521124023944e-05\n",
      "Epoch 3562: train loss: 4.0284481656271964e-05\n",
      "Epoch 3563: train loss: 4.021427594125271e-05\n",
      "Epoch 3564: train loss: 4.0144030208466575e-05\n",
      "Epoch 3565: train loss: 4.007444658782333e-05\n",
      "Epoch 3566: train loss: 4.000476110377349e-05\n",
      "Epoch 3567: train loss: 3.993558493675664e-05\n",
      "Epoch 3568: train loss: 3.986654701293446e-05\n",
      "Epoch 3569: train loss: 3.97979783883784e-05\n",
      "Epoch 3570: train loss: 3.972947524744086e-05\n",
      "Epoch 3571: train loss: 3.96611139876768e-05\n",
      "Epoch 3572: train loss: 3.959325113100931e-05\n",
      "Epoch 3573: train loss: 3.9525435568066314e-05\n",
      "Epoch 3574: train loss: 3.9457972889067605e-05\n",
      "Epoch 3575: train loss: 3.9390801248373464e-05\n",
      "Epoch 3576: train loss: 3.932376057491638e-05\n",
      "Epoch 3577: train loss: 3.925712007912807e-05\n",
      "Epoch 3578: train loss: 3.919048685929738e-05\n",
      "Epoch 3579: train loss: 3.912434112862684e-05\n",
      "Epoch 3580: train loss: 3.905817720806226e-05\n",
      "Epoch 3581: train loss: 3.899259536410682e-05\n",
      "Epoch 3582: train loss: 3.8927166315261275e-05\n",
      "Epoch 3583: train loss: 3.886235208483413e-05\n",
      "Epoch 3584: train loss: 3.879693758790381e-05\n",
      "Epoch 3585: train loss: 3.873235982609913e-05\n",
      "Epoch 3586: train loss: 3.866753468173556e-05\n",
      "Epoch 3587: train loss: 3.860293509205803e-05\n",
      "Epoch 3588: train loss: 3.8538611988769844e-05\n",
      "Epoch 3589: train loss: 3.8474838220281526e-05\n",
      "Epoch 3590: train loss: 3.841139186988585e-05\n",
      "Epoch 3591: train loss: 3.8347836380125955e-05\n",
      "Epoch 3592: train loss: 3.828459739452228e-05\n",
      "Epoch 3593: train loss: 3.822176222456619e-05\n",
      "Epoch 3594: train loss: 3.8158799725351855e-05\n",
      "Epoch 3595: train loss: 3.809631380136125e-05\n",
      "Epoch 3596: train loss: 3.803376603173092e-05\n",
      "Epoch 3597: train loss: 3.797188037424348e-05\n",
      "Epoch 3598: train loss: 3.791000563069247e-05\n",
      "Epoch 3599: train loss: 3.7848250940442085e-05\n",
      "Epoch 3600: train loss: 3.7786918255733326e-05\n",
      "Epoch 3601: train loss: 3.772564741666429e-05\n",
      "Epoch 3602: train loss: 3.7664492992917076e-05\n",
      "Epoch 3603: train loss: 3.760349136427976e-05\n",
      "Epoch 3604: train loss: 3.754294812097214e-05\n",
      "Epoch 3605: train loss: 3.7482586776604876e-05\n",
      "Epoch 3606: train loss: 3.742223634617403e-05\n",
      "Epoch 3607: train loss: 3.7362213333835825e-05\n",
      "Epoch 3608: train loss: 3.730281241587363e-05\n",
      "Epoch 3609: train loss: 3.7242876715026796e-05\n",
      "Epoch 3610: train loss: 3.718381776707247e-05\n",
      "Epoch 3611: train loss: 3.7124529626453295e-05\n",
      "Epoch 3612: train loss: 3.7065376091049984e-05\n",
      "Epoch 3613: train loss: 3.7006881029810756e-05\n",
      "Epoch 3614: train loss: 3.694788392749615e-05\n",
      "Epoch 3615: train loss: 3.688947617774829e-05\n",
      "Epoch 3616: train loss: 3.683153408928774e-05\n",
      "Epoch 3617: train loss: 3.677354834508151e-05\n",
      "Epoch 3618: train loss: 3.67153188562952e-05\n",
      "Epoch 3619: train loss: 3.665775875560939e-05\n",
      "Epoch 3620: train loss: 3.6600478779291734e-05\n",
      "Epoch 3621: train loss: 3.654347165138461e-05\n",
      "Epoch 3622: train loss: 3.6486097087617964e-05\n",
      "Epoch 3623: train loss: 3.6429453757591546e-05\n",
      "Epoch 3624: train loss: 3.6372712202137336e-05\n",
      "Epoch 3625: train loss: 3.6316421756055206e-05\n",
      "Epoch 3626: train loss: 3.626008765422739e-05\n",
      "Epoch 3627: train loss: 3.620422285166569e-05\n",
      "Epoch 3628: train loss: 3.614804518292658e-05\n",
      "Epoch 3629: train loss: 3.6092511436436325e-05\n",
      "Epoch 3630: train loss: 3.603717777878046e-05\n",
      "Epoch 3631: train loss: 3.598192779463716e-05\n",
      "Epoch 3632: train loss: 3.5927190765505657e-05\n",
      "Epoch 3633: train loss: 3.587289393180981e-05\n",
      "Epoch 3634: train loss: 3.581966302590445e-05\n",
      "Epoch 3635: train loss: 3.576702147256583e-05\n",
      "Epoch 3636: train loss: 3.571576235117391e-05\n",
      "Epoch 3637: train loss: 3.566675150068477e-05\n",
      "Epoch 3638: train loss: 3.562122583389282e-05\n",
      "Epoch 3639: train loss: 3.5582521377364174e-05\n",
      "Epoch 3640: train loss: 3.555434159352444e-05\n",
      "Epoch 3641: train loss: 3.554603608790785e-05\n",
      "Epoch 3642: train loss: 3.557048330549151e-05\n",
      "Epoch 3643: train loss: 3.5656823456520215e-05\n",
      "Epoch 3644: train loss: 3.5846798709826544e-05\n",
      "Epoch 3645: train loss: 3.624230157583952e-05\n",
      "Epoch 3646: train loss: 3.6972207453800365e-05\n",
      "Epoch 3647: train loss: 3.840774661512114e-05\n",
      "Epoch 3648: train loss: 4.0902465116232634e-05\n",
      "Epoch 3649: train loss: 4.582755718729459e-05\n",
      "Epoch 3650: train loss: 5.369255450204946e-05\n",
      "Epoch 3651: train loss: 6.902789027662948e-05\n",
      "Epoch 3652: train loss: 8.871064346749336e-05\n",
      "Epoch 3653: train loss: 0.00012137079465901479\n",
      "Epoch 3654: train loss: 0.00014292969717644155\n",
      "Epoch 3655: train loss: 0.00015358280506916344\n",
      "Epoch 3656: train loss: 0.00012844381853938103\n",
      "Epoch 3657: train loss: 8.684961358085275e-05\n",
      "Epoch 3658: train loss: 5.296177914715372e-05\n",
      "Epoch 3659: train loss: 3.768980968743563e-05\n",
      "Epoch 3660: train loss: 3.6494762753136456e-05\n",
      "Epoch 3661: train loss: 4.6141612983774394e-05\n",
      "Epoch 3662: train loss: 6.324271089397371e-05\n",
      "Epoch 3663: train loss: 8.190391963580623e-05\n",
      "Epoch 3664: train loss: 0.00010425779328215867\n",
      "Epoch 3665: train loss: 0.00011784211528720334\n",
      "Epoch 3666: train loss: 0.00012580501788761467\n",
      "Epoch 3667: train loss: 0.00010770409426186234\n",
      "Epoch 3668: train loss: 7.780797750456259e-05\n",
      "Epoch 3669: train loss: 5.12696205987595e-05\n",
      "Epoch 3670: train loss: 3.776467201532796e-05\n",
      "Epoch 3671: train loss: 3.513185220072046e-05\n",
      "Epoch 3672: train loss: 4.100569640286267e-05\n",
      "Epoch 3673: train loss: 5.09496676386334e-05\n",
      "Epoch 3674: train loss: 5.9740843425970525e-05\n",
      "Epoch 3675: train loss: 6.518656300613657e-05\n",
      "Epoch 3676: train loss: 6.608260446228087e-05\n",
      "Epoch 3677: train loss: 6.324704736471176e-05\n",
      "Epoch 3678: train loss: 5.596975461230613e-05\n",
      "Epoch 3679: train loss: 4.772985994350165e-05\n",
      "Epoch 3680: train loss: 4.073474701726809e-05\n",
      "Epoch 3681: train loss: 3.6343066312838346e-05\n",
      "Epoch 3682: train loss: 3.415106039028615e-05\n",
      "Epoch 3683: train loss: 3.393577935639769e-05\n",
      "Epoch 3684: train loss: 3.5129720345139503e-05\n",
      "Epoch 3685: train loss: 3.70583547919523e-05\n",
      "Epoch 3686: train loss: 3.92507718061097e-05\n",
      "Epoch 3687: train loss: 4.1093924664892256e-05\n",
      "Epoch 3688: train loss: 4.225614611641504e-05\n",
      "Epoch 3689: train loss: 4.2319246858824044e-05\n",
      "Epoch 3690: train loss: 4.146663923165761e-05\n",
      "Epoch 3691: train loss: 4.006012386525981e-05\n",
      "Epoch 3692: train loss: 3.855304021271877e-05\n",
      "Epoch 3693: train loss: 3.702103640534915e-05\n",
      "Epoch 3694: train loss: 3.57032404281199e-05\n",
      "Epoch 3695: train loss: 3.4607081033755094e-05\n",
      "Epoch 3696: train loss: 3.383978400961496e-05\n",
      "Epoch 3697: train loss: 3.3379183150827885e-05\n",
      "Epoch 3698: train loss: 3.314247805974446e-05\n",
      "Epoch 3699: train loss: 3.311039836262353e-05\n",
      "Epoch 3700: train loss: 3.320942778373137e-05\n",
      "Epoch 3701: train loss: 3.339885006425902e-05\n",
      "Epoch 3702: train loss: 3.366445889696479e-05\n",
      "Epoch 3703: train loss: 3.396651300136e-05\n",
      "Epoch 3704: train loss: 3.4307075111428276e-05\n",
      "Epoch 3705: train loss: 3.468315117061138e-05\n",
      "Epoch 3706: train loss: 3.506486245896667e-05\n",
      "Epoch 3707: train loss: 3.5511318856151775e-05\n",
      "Epoch 3708: train loss: 3.600894706323743e-05\n",
      "Epoch 3709: train loss: 3.6647335946327075e-05\n",
      "Epoch 3710: train loss: 3.739235762623139e-05\n",
      "Epoch 3711: train loss: 3.83470905944705e-05\n",
      "Epoch 3712: train loss: 3.947405275539495e-05\n",
      "Epoch 3713: train loss: 4.102908860659227e-05\n",
      "Epoch 3714: train loss: 4.29386600444559e-05\n",
      "Epoch 3715: train loss: 4.55613735539373e-05\n",
      "Epoch 3716: train loss: 4.8575449909549206e-05\n",
      "Epoch 3717: train loss: 5.2470819355221465e-05\n",
      "Epoch 3718: train loss: 5.666887591360137e-05\n",
      "Epoch 3719: train loss: 6.185346865095198e-05\n",
      "Epoch 3720: train loss: 6.671771552646533e-05\n",
      "Epoch 3721: train loss: 7.155096682254225e-05\n",
      "Epoch 3722: train loss: 7.436134183080867e-05\n",
      "Epoch 3723: train loss: 7.555439515272155e-05\n",
      "Epoch 3724: train loss: 7.40951145417057e-05\n",
      "Epoch 3725: train loss: 7.086803088895977e-05\n",
      "Epoch 3726: train loss: 6.58887583995238e-05\n",
      "Epoch 3727: train loss: 6.021864101057872e-05\n",
      "Epoch 3728: train loss: 5.4482155974255875e-05\n",
      "Epoch 3729: train loss: 4.950377478962764e-05\n",
      "Epoch 3730: train loss: 4.539241854217835e-05\n",
      "Epoch 3731: train loss: 4.228339093970135e-05\n",
      "Epoch 3732: train loss: 3.992521305917762e-05\n",
      "Epoch 3733: train loss: 3.828796252491884e-05\n",
      "Epoch 3734: train loss: 3.716616265592165e-05\n",
      "Epoch 3735: train loss: 3.652332452475093e-05\n",
      "Epoch 3736: train loss: 3.620163261075504e-05\n",
      "Epoch 3737: train loss: 3.61949932994321e-05\n",
      "Epoch 3738: train loss: 3.642635783762671e-05\n",
      "Epoch 3739: train loss: 3.697016654768959e-05\n",
      "Epoch 3740: train loss: 3.781218038056977e-05\n",
      "Epoch 3741: train loss: 3.907155405613594e-05\n",
      "Epoch 3742: train loss: 4.072835508850403e-05\n",
      "Epoch 3743: train loss: 4.293695019441657e-05\n",
      "Epoch 3744: train loss: 4.569082739180885e-05\n",
      "Epoch 3745: train loss: 4.923681990476325e-05\n",
      "Epoch 3746: train loss: 5.347448677639477e-05\n",
      "Epoch 3747: train loss: 5.858097210875712e-05\n",
      "Epoch 3748: train loss: 6.408069748431444e-05\n",
      "Epoch 3749: train loss: 6.991496775299311e-05\n",
      "Epoch 3750: train loss: 7.512907905038446e-05\n",
      "Epoch 3751: train loss: 7.918783376226202e-05\n",
      "Epoch 3752: train loss: 8.09696939541027e-05\n",
      "Epoch 3753: train loss: 7.98086985014379e-05\n",
      "Epoch 3754: train loss: 7.599269883939996e-05\n",
      "Epoch 3755: train loss: 6.985261279623955e-05\n",
      "Epoch 3756: train loss: 6.302067777141929e-05\n",
      "Epoch 3757: train loss: 5.6078668421832845e-05\n",
      "Epoch 3758: train loss: 5.004124977858737e-05\n",
      "Epoch 3759: train loss: 4.5000466343481094e-05\n",
      "Epoch 3760: train loss: 4.1124611016130075e-05\n",
      "Epoch 3761: train loss: 3.825862950179726e-05\n",
      "Epoch 3762: train loss: 3.6234137951396406e-05\n",
      "Epoch 3763: train loss: 3.485910565359518e-05\n",
      "Epoch 3764: train loss: 3.394597297301516e-05\n",
      "Epoch 3765: train loss: 3.3372667530784383e-05\n",
      "Epoch 3766: train loss: 3.30375405610539e-05\n",
      "Epoch 3767: train loss: 3.2897285564104095e-05\n",
      "Epoch 3768: train loss: 3.291838220320642e-05\n",
      "Epoch 3769: train loss: 3.310563988634385e-05\n",
      "Epoch 3770: train loss: 3.346688026795164e-05\n",
      "Epoch 3771: train loss: 3.40424885507673e-05\n",
      "Epoch 3772: train loss: 3.48814828612376e-05\n",
      "Epoch 3773: train loss: 3.60684898623731e-05\n",
      "Epoch 3774: train loss: 3.7713740312028676e-05\n",
      "Epoch 3775: train loss: 3.997327439719811e-05\n",
      "Epoch 3776: train loss: 4.304878166294657e-05\n",
      "Epoch 3777: train loss: 4.7207002353388816e-05\n",
      "Epoch 3778: train loss: 5.273024726193398e-05\n",
      "Epoch 3779: train loss: 5.9895581216551363e-05\n",
      "Epoch 3780: train loss: 6.87171341269277e-05\n",
      "Epoch 3781: train loss: 7.87695826147683e-05\n",
      "Epoch 3782: train loss: 8.898084342945367e-05\n",
      "Epoch 3783: train loss: 9.682349627837539e-05\n",
      "Epoch 3784: train loss: 0.00010076026956085116\n",
      "Epoch 3785: train loss: 9.727248834678903e-05\n",
      "Epoch 3786: train loss: 8.893058111425489e-05\n",
      "Epoch 3787: train loss: 7.574768824269995e-05\n",
      "Epoch 3788: train loss: 6.316130020422861e-05\n",
      "Epoch 3789: train loss: 5.190269075683318e-05\n",
      "Epoch 3790: train loss: 4.362912295619026e-05\n",
      "Epoch 3791: train loss: 3.771202318603173e-05\n",
      "Epoch 3792: train loss: 3.387518154340796e-05\n",
      "Epoch 3793: train loss: 3.1561598007101566e-05\n",
      "Epoch 3794: train loss: 3.034699875570368e-05\n",
      "Epoch 3795: train loss: 2.9853496016585268e-05\n",
      "Epoch 3796: train loss: 2.9831680876668543e-05\n",
      "Epoch 3797: train loss: 3.014131470990833e-05\n",
      "Epoch 3798: train loss: 3.07301597786136e-05\n",
      "Epoch 3799: train loss: 3.16210716846399e-05\n",
      "Epoch 3800: train loss: 3.289538653916679e-05\n",
      "Epoch 3801: train loss: 3.466567432042211e-05\n",
      "Epoch 3802: train loss: 3.704071423271671e-05\n",
      "Epoch 3803: train loss: 4.016359162051231e-05\n",
      "Epoch 3804: train loss: 4.410965266288258e-05\n",
      "Epoch 3805: train loss: 4.9151793064083904e-05\n",
      "Epoch 3806: train loss: 5.511040217243135e-05\n",
      "Epoch 3807: train loss: 6.223571108421311e-05\n",
      "Epoch 3808: train loss: 6.946329085621983e-05\n",
      "Epoch 3809: train loss: 7.696312968619168e-05\n",
      "Epoch 3810: train loss: 8.263599738711491e-05\n",
      "Epoch 3811: train loss: 8.687387889949605e-05\n",
      "Epoch 3812: train loss: 8.697048906469718e-05\n",
      "Epoch 3813: train loss: 8.445823914371431e-05\n",
      "Epoch 3814: train loss: 7.784179615555331e-05\n",
      "Epoch 3815: train loss: 7.034802547423169e-05\n",
      "Epoch 3816: train loss: 6.180768104968593e-05\n",
      "Epoch 3817: train loss: 5.453723861137405e-05\n",
      "Epoch 3818: train loss: 4.8104673624038696e-05\n",
      "Epoch 3819: train loss: 4.319244544603862e-05\n",
      "Epoch 3820: train loss: 3.9337985072052106e-05\n",
      "Epoch 3821: train loss: 3.659200592665002e-05\n",
      "Epoch 3822: train loss: 3.463399116299115e-05\n",
      "Epoch 3823: train loss: 3.332821142976172e-05\n",
      "Epoch 3824: train loss: 3.24644279316999e-05\n",
      "Epoch 3825: train loss: 3.193654629285447e-05\n",
      "Epoch 3826: train loss: 3.1650899472879246e-05\n",
      "Epoch 3827: train loss: 3.157441824441776e-05\n",
      "Epoch 3828: train loss: 3.168217517668381e-05\n",
      "Epoch 3829: train loss: 3.198687045369297e-05\n",
      "Epoch 3830: train loss: 3.2501677196705714e-05\n",
      "Epoch 3831: train loss: 3.3296597393928096e-05\n",
      "Epoch 3832: train loss: 3.442729212110862e-05\n",
      "Epoch 3833: train loss: 3.604951416491531e-05\n",
      "Epoch 3834: train loss: 3.824972009169869e-05\n",
      "Epoch 3835: train loss: 4.1310042433906347e-05\n",
      "Epoch 3836: train loss: 4.531099693849683e-05\n",
      "Epoch 3837: train loss: 5.0759550504153594e-05\n",
      "Epoch 3838: train loss: 5.756865721195936e-05\n",
      "Epoch 3839: train loss: 6.63587125018239e-05\n",
      "Epoch 3840: train loss: 7.601561810588464e-05\n",
      "Epoch 3841: train loss: 8.674620039528236e-05\n",
      "Epoch 3842: train loss: 9.49383174884133e-05\n",
      "Epoch 3843: train loss: 0.00010075057070935145\n",
      "Epoch 3844: train loss: 9.884285100270063e-05\n",
      "Epoch 3845: train loss: 9.273020987166092e-05\n",
      "Epoch 3846: train loss: 8.022795373108238e-05\n",
      "Epoch 3847: train loss: 6.789881445001811e-05\n",
      "Epoch 3848: train loss: 5.5724125559208915e-05\n",
      "Epoch 3849: train loss: 4.6583078074036166e-05\n",
      "Epoch 3850: train loss: 3.9602564356755465e-05\n",
      "Epoch 3851: train loss: 3.4877488360507414e-05\n",
      "Epoch 3852: train loss: 3.174097219016403e-05\n",
      "Epoch 3853: train loss: 2.9854190870537423e-05\n",
      "Epoch 3854: train loss: 2.880547799577471e-05\n",
      "Epoch 3855: train loss: 2.830514858942479e-05\n",
      "Epoch 3856: train loss: 2.8150889193057083e-05\n",
      "Epoch 3857: train loss: 2.822339411068242e-05\n",
      "Epoch 3858: train loss: 2.84716716123512e-05\n",
      "Epoch 3859: train loss: 2.8894673960166983e-05\n",
      "Epoch 3860: train loss: 2.9526574508054182e-05\n",
      "Epoch 3861: train loss: 3.0418892492889427e-05\n",
      "Epoch 3862: train loss: 3.165672023897059e-05\n",
      "Epoch 3863: train loss: 3.331106199766509e-05\n",
      "Epoch 3864: train loss: 3.5584020224632695e-05\n",
      "Epoch 3865: train loss: 3.8583479181397706e-05\n",
      "Epoch 3866: train loss: 4.2717270844150335e-05\n",
      "Epoch 3867: train loss: 4.7959096264094114e-05\n",
      "Epoch 3868: train loss: 5.501318446476944e-05\n",
      "Epoch 3869: train loss: 6.341651169350371e-05\n",
      "Epoch 3870: train loss: 7.415612344630063e-05\n",
      "Epoch 3871: train loss: 8.510165935149416e-05\n",
      "Epoch 3872: train loss: 9.66408260865137e-05\n",
      "Epoch 3873: train loss: 0.00010317275882698596\n",
      "Epoch 3874: train loss: 0.00010578335059108213\n",
      "Epoch 3875: train loss: 9.867522021522745e-05\n",
      "Epoch 3876: train loss: 8.809973951429129e-05\n",
      "Epoch 3877: train loss: 7.293053204193711e-05\n",
      "Epoch 3878: train loss: 5.994728417135775e-05\n",
      "Epoch 3879: train loss: 4.8615693231113255e-05\n",
      "Epoch 3880: train loss: 4.061897561769001e-05\n",
      "Epoch 3881: train loss: 3.489387745503336e-05\n",
      "Epoch 3882: train loss: 3.123341593891382e-05\n",
      "Epoch 3883: train loss: 2.902755659306422e-05\n",
      "Epoch 3884: train loss: 2.7886042516911402e-05\n",
      "Epoch 3885: train loss: 2.7447802494862117e-05\n",
      "Epoch 3886: train loss: 2.7480362405185588e-05\n",
      "Epoch 3887: train loss: 2.784906064334791e-05\n",
      "Epoch 3888: train loss: 2.8504206056823023e-05\n",
      "Epoch 3889: train loss: 2.9472210371750407e-05\n",
      "Epoch 3890: train loss: 3.082212424487807e-05\n",
      "Epoch 3891: train loss: 3.268930595368147e-05\n",
      "Epoch 3892: train loss: 3.5154098441125825e-05\n",
      "Epoch 3893: train loss: 3.845464016194455e-05\n",
      "Epoch 3894: train loss: 4.2577157728374004e-05\n",
      "Epoch 3895: train loss: 4.801607065019198e-05\n",
      "Epoch 3896: train loss: 5.42768175364472e-05\n",
      "Epoch 3897: train loss: 6.20766804786399e-05\n",
      "Epoch 3898: train loss: 6.960427708690986e-05\n",
      "Epoch 3899: train loss: 7.794709381414577e-05\n",
      "Epoch 3900: train loss: 8.364029054064304e-05\n",
      "Epoch 3901: train loss: 8.848441211739555e-05\n",
      "Epoch 3902: train loss: 8.79412837093696e-05\n",
      "Epoch 3903: train loss: 8.530641935067251e-05\n",
      "Epoch 3904: train loss: 7.761329470667988e-05\n",
      "Epoch 3905: train loss: 6.970826507313177e-05\n",
      "Epoch 3906: train loss: 6.050566298654303e-05\n",
      "Epoch 3907: train loss: 5.308696927386336e-05\n",
      "Epoch 3908: train loss: 4.642744534066878e-05\n",
      "Epoch 3909: train loss: 4.14845890190918e-05\n",
      "Epoch 3910: train loss: 3.753083728952333e-05\n",
      "Epoch 3911: train loss: 3.4754950320348144e-05\n",
      "Epoch 3912: train loss: 3.2737832952989265e-05\n",
      "Epoch 3913: train loss: 3.140779153909534e-05\n",
      "Epoch 3914: train loss: 3.051089333894197e-05\n",
      "Epoch 3915: train loss: 2.9969693059683777e-05\n",
      "Epoch 3916: train loss: 2.9668119168491103e-05\n",
      "Epoch 3917: train loss: 2.959184166684281e-05\n",
      "Epoch 3918: train loss: 2.9697894206037745e-05\n",
      "Epoch 3919: train loss: 3.0020666599739343e-05\n",
      "Epoch 3920: train loss: 3.055182969546877e-05\n",
      "Epoch 3921: train loss: 3.139019463560544e-05\n",
      "Epoch 3922: train loss: 3.256361742387526e-05\n",
      "Epoch 3923: train loss: 3.428067429922521e-05\n",
      "Epoch 3924: train loss: 3.657100023701787e-05\n",
      "Epoch 3925: train loss: 3.982098860433325e-05\n",
      "Epoch 3926: train loss: 4.3986070522805676e-05\n",
      "Epoch 3927: train loss: 4.980530866305344e-05\n",
      "Epoch 3928: train loss: 5.68989671592135e-05\n",
      "Epoch 3929: train loss: 6.638474587816745e-05\n",
      "Epoch 3930: train loss: 7.642257696716115e-05\n",
      "Epoch 3931: train loss: 8.810272993287072e-05\n",
      "Epoch 3932: train loss: 9.633577428758144e-05\n",
      "Epoch 3933: train loss: 0.00010267821926390752\n",
      "Epoch 3934: train loss: 9.995528671424836e-05\n",
      "Epoch 3935: train loss: 9.345455328002572e-05\n",
      "Epoch 3936: train loss: 7.974391337484121e-05\n",
      "Epoch 3937: train loss: 6.680452497676015e-05\n",
      "Epoch 3938: train loss: 5.397294444264844e-05\n",
      "Epoch 3939: train loss: 4.458334660739638e-05\n",
      "Epoch 3940: train loss: 3.741152977454476e-05\n",
      "Epoch 3941: train loss: 3.261598976678215e-05\n",
      "Epoch 3942: train loss: 2.9440270736813545e-05\n",
      "Epoch 3943: train loss: 2.756063622655347e-05\n",
      "Epoch 3944: train loss: 2.655166645126883e-05\n",
      "Epoch 3945: train loss: 2.6122885174117982e-05\n",
      "Epoch 3946: train loss: 2.6070998501381837e-05\n",
      "Epoch 3947: train loss: 2.6278821678715758e-05\n",
      "Epoch 3948: train loss: 2.6704710762714967e-05\n",
      "Epoch 3949: train loss: 2.7356154532753862e-05\n",
      "Epoch 3950: train loss: 2.829008190019522e-05\n",
      "Epoch 3951: train loss: 2.956681782961823e-05\n",
      "Epoch 3952: train loss: 3.131675111944787e-05\n",
      "Epoch 3953: train loss: 3.3596788853174075e-05\n",
      "Epoch 3954: train loss: 3.672664024634287e-05\n",
      "Epoch 3955: train loss: 4.0703183913137764e-05\n",
      "Epoch 3956: train loss: 4.614137651515193e-05\n",
      "Epoch 3957: train loss: 5.254901043372229e-05\n",
      "Epoch 3958: train loss: 6.096275319578126e-05\n",
      "Epoch 3959: train loss: 6.978601595619693e-05\n",
      "Epoch 3960: train loss: 8.05716190370731e-05\n",
      "Epoch 3961: train loss: 8.921454718802124e-05\n",
      "Epoch 3962: train loss: 9.734780178405344e-05\n",
      "Epoch 3963: train loss: 9.81494813458994e-05\n",
      "Epoch 3964: train loss: 9.559849422657862e-05\n",
      "Epoch 3965: train loss: 8.518366666976362e-05\n",
      "Epoch 3966: train loss: 7.423963688779622e-05\n",
      "Epoch 3967: train loss: 6.148080865386873e-05\n",
      "Epoch 3968: train loss: 5.147016418050043e-05\n",
      "Epoch 3969: train loss: 4.2951138311764225e-05\n",
      "Epoch 3970: train loss: 3.691761594382115e-05\n",
      "Epoch 3971: train loss: 3.247567292419262e-05\n",
      "Epoch 3972: train loss: 2.9548507882282138e-05\n",
      "Epoch 3973: train loss: 2.765116914815735e-05\n",
      "Epoch 3974: train loss: 2.6515908757573925e-05\n",
      "Epoch 3975: train loss: 2.5868001102935523e-05\n",
      "Epoch 3976: train loss: 2.5532101062708534e-05\n",
      "Epoch 3977: train loss: 2.5390778318978846e-05\n",
      "Epoch 3978: train loss: 2.537852014938835e-05\n",
      "Epoch 3979: train loss: 2.546303039707709e-05\n",
      "Epoch 3980: train loss: 2.563859015936032e-05\n",
      "Epoch 3981: train loss: 2.5916880986187607e-05\n",
      "Epoch 3982: train loss: 2.631740971992258e-05\n",
      "Epoch 3983: train loss: 2.6887677449849434e-05\n",
      "Epoch 3984: train loss: 2.767427395156119e-05\n",
      "Epoch 3985: train loss: 2.8792845114367083e-05\n",
      "Epoch 3986: train loss: 3.0327813874464482e-05\n",
      "Epoch 3987: train loss: 3.255624687881209e-05\n",
      "Epoch 3988: train loss: 3.564335565897636e-05\n",
      "Epoch 3989: train loss: 4.026478563901037e-05\n",
      "Epoch 3990: train loss: 4.66292149212677e-05\n",
      "Epoch 3991: train loss: 5.604603575193323e-05\n",
      "Epoch 3992: train loss: 6.804059376008809e-05\n",
      "Epoch 3993: train loss: 8.44206806505099e-05\n",
      "Epoch 3994: train loss: 0.0001011892527458258\n",
      "Epoch 3995: train loss: 0.00011930621258215979\n",
      "Epoch 3996: train loss: 0.00012663440429605544\n",
      "Epoch 3997: train loss: 0.0001259306591236964\n",
      "Epoch 3998: train loss: 0.00010673106589820236\n",
      "Epoch 3999: train loss: 8.427958528045565e-05\n",
      "Epoch 4000: train loss: 6.04252636549063e-05\n",
      "Epoch 4001: train loss: 4.420096593094058e-05\n",
      "Epoch 4002: train loss: 3.3591186365811154e-05\n",
      "Epoch 4003: train loss: 2.775114080577623e-05\n",
      "Epoch 4004: train loss: 2.517040047678165e-05\n",
      "Epoch 4005: train loss: 2.5010253011714667e-05\n",
      "Epoch 4006: train loss: 2.6663516109692864e-05\n",
      "Epoch 4007: train loss: 2.9762319172732532e-05\n",
      "Epoch 4008: train loss: 3.4353495721006766e-05\n",
      "Epoch 4009: train loss: 4.038507540826686e-05\n",
      "Epoch 4010: train loss: 4.796995926881209e-05\n",
      "Epoch 4011: train loss: 5.668607263942249e-05\n",
      "Epoch 4012: train loss: 6.695915362797678e-05\n",
      "Epoch 4013: train loss: 7.610962347825989e-05\n",
      "Epoch 4014: train loss: 8.382111263927072e-05\n",
      "Epoch 4015: train loss: 8.38213600218296e-05\n",
      "Epoch 4016: train loss: 7.950262806843966e-05\n",
      "Epoch 4017: train loss: 6.820333510404453e-05\n",
      "Epoch 4018: train loss: 5.70516858715564e-05\n",
      "Epoch 4019: train loss: 4.550995436147787e-05\n",
      "Epoch 4020: train loss: 3.68005785276182e-05\n",
      "Epoch 4021: train loss: 3.043624928977806e-05\n",
      "Epoch 4022: train loss: 2.6643139790394343e-05\n",
      "Epoch 4023: train loss: 2.4831777409417555e-05\n",
      "Epoch 4024: train loss: 2.451715226925444e-05\n",
      "Epoch 4025: train loss: 2.5288609322160482e-05\n",
      "Epoch 4026: train loss: 2.6875000912696123e-05\n",
      "Epoch 4027: train loss: 2.916719859058503e-05\n",
      "Epoch 4028: train loss: 3.2158051908481866e-05\n",
      "Epoch 4029: train loss: 3.606805330491625e-05\n",
      "Epoch 4030: train loss: 4.08278065151535e-05\n",
      "Epoch 4031: train loss: 4.669575355364941e-05\n",
      "Epoch 4032: train loss: 5.267691085464321e-05\n",
      "Epoch 4033: train loss: 5.927516031078994e-05\n",
      "Epoch 4034: train loss: 6.409308116417378e-05\n",
      "Epoch 4035: train loss: 6.84836704749614e-05\n",
      "Epoch 4036: train loss: 6.861804286018014e-05\n",
      "Epoch 4037: train loss: 6.73582180752419e-05\n",
      "Epoch 4038: train loss: 6.185134407132864e-05\n",
      "Epoch 4039: train loss: 5.624600453302264e-05\n",
      "Epoch 4040: train loss: 4.9424226745031774e-05\n",
      "Epoch 4041: train loss: 4.3977990571875125e-05\n",
      "Epoch 4042: train loss: 3.910463419742882e-05\n",
      "Epoch 4043: train loss: 3.5583183489507064e-05\n",
      "Epoch 4044: train loss: 3.284843114670366e-05\n",
      "Epoch 4045: train loss: 3.101667607552372e-05\n",
      "Epoch 4046: train loss: 2.973759183078073e-05\n",
      "Epoch 4047: train loss: 2.898021375585813e-05\n",
      "Epoch 4048: train loss: 2.855178536265157e-05\n",
      "Epoch 4049: train loss: 2.8460721296141855e-05\n",
      "Epoch 4050: train loss: 2.861892426153645e-05\n",
      "Epoch 4051: train loss: 2.9108858143445104e-05\n",
      "Epoch 4052: train loss: 2.9871875085518695e-05\n",
      "Epoch 4053: train loss: 3.106547592324205e-05\n",
      "Epoch 4054: train loss: 3.261961319367401e-05\n",
      "Epoch 4055: train loss: 3.485847992124036e-05\n",
      "Epoch 4056: train loss: 3.765599467442371e-05\n",
      "Epoch 4057: train loss: 4.16170951211825e-05\n",
      "Epoch 4058: train loss: 4.63317810499575e-05\n",
      "Epoch 4059: train loss: 5.280514233163558e-05\n",
      "Epoch 4060: train loss: 5.9881345805479214e-05\n",
      "Epoch 4061: train loss: 6.910099182277918e-05\n",
      "Epoch 4062: train loss: 7.757150888210163e-05\n",
      "Epoch 4063: train loss: 8.712426642887294e-05\n",
      "Epoch 4064: train loss: 9.196816972689703e-05\n",
      "Epoch 4065: train loss: 9.492693789070472e-05\n",
      "Epoch 4066: train loss: 8.965126471593976e-05\n",
      "Epoch 4067: train loss: 8.225238707382232e-05\n",
      "Epoch 4068: train loss: 6.98722287779674e-05\n",
      "Epoch 4069: train loss: 5.901239637751132e-05\n",
      "Epoch 4070: train loss: 4.841331610805355e-05\n",
      "Epoch 4071: train loss: 4.060003630002029e-05\n",
      "Epoch 4072: train loss: 3.4429864172125235e-05\n",
      "Epoch 4073: train loss: 3.0201310437405482e-05\n",
      "Epoch 4074: train loss: 2.7281832444714382e-05\n",
      "Epoch 4075: train loss: 2.545974166423548e-05\n",
      "Epoch 4076: train loss: 2.4374026907025836e-05\n",
      "Epoch 4077: train loss: 2.379140096309129e-05\n",
      "Epoch 4078: train loss: 2.3529541067546234e-05\n",
      "Epoch 4079: train loss: 2.347688678128179e-05\n",
      "Epoch 4080: train loss: 2.3572931240778416e-05\n",
      "Epoch 4081: train loss: 2.379252327955328e-05\n",
      "Epoch 4082: train loss: 2.414220398350153e-05\n",
      "Epoch 4083: train loss: 2.464134195179213e-05\n",
      "Epoch 4084: train loss: 2.5339842977700755e-05\n",
      "Epoch 4085: train loss: 2.6280968086211942e-05\n",
      "Epoch 4086: train loss: 2.7597316147875972e-05\n",
      "Epoch 4087: train loss: 2.9352509955060668e-05\n",
      "Epoch 4088: train loss: 3.184833985869773e-05\n",
      "Epoch 4089: train loss: 3.5152384953107685e-05\n",
      "Epoch 4090: train loss: 3.995817314716987e-05\n",
      "Epoch 4091: train loss: 4.6229135477915406e-05\n",
      "Epoch 4092: train loss: 5.533466173801571e-05\n",
      "Epoch 4093: train loss: 6.630353891523555e-05\n",
      "Epoch 4094: train loss: 8.100964623736218e-05\n",
      "Epoch 4095: train loss: 9.501394379185513e-05\n",
      "Epoch 4096: train loss: 0.00010997879144269973\n",
      "Epoch 4097: train loss: 0.00011538619583006948\n",
      "Epoch 4098: train loss: 0.0001149765303125605\n",
      "Epoch 4099: train loss: 9.965091157937422e-05\n",
      "Epoch 4100: train loss: 8.163748134393245e-05\n",
      "Epoch 4101: train loss: 6.097629739088006e-05\n",
      "Epoch 4102: train loss: 4.597307997755706e-05\n",
      "Epoch 4103: train loss: 3.511286558932625e-05\n",
      "Epoch 4104: train loss: 2.8476408260758035e-05\n",
      "Epoch 4105: train loss: 2.4700200810912065e-05\n",
      "Epoch 4106: train loss: 2.3135256924433634e-05\n",
      "Epoch 4107: train loss: 2.3184413294075057e-05\n",
      "Epoch 4108: train loss: 2.442029472149443e-05\n",
      "Epoch 4109: train loss: 2.6650681320461445e-05\n",
      "Epoch 4110: train loss: 2.9786497179884464e-05\n",
      "Epoch 4111: train loss: 3.390780329937115e-05\n",
      "Epoch 4112: train loss: 3.8974438211880624e-05\n",
      "Epoch 4113: train loss: 4.539990550256334e-05\n",
      "Epoch 4114: train loss: 5.2485054766293615e-05\n",
      "Epoch 4115: train loss: 6.056418351363391e-05\n",
      "Epoch 4116: train loss: 6.682240928057581e-05\n",
      "Epoch 4117: train loss: 7.229121547425166e-05\n",
      "Epoch 4118: train loss: 7.275941607076675e-05\n",
      "Epoch 4119: train loss: 7.143963739508763e-05\n",
      "Epoch 4120: train loss: 6.461107113864273e-05\n",
      "Epoch 4121: train loss: 5.7138306146953255e-05\n",
      "Epoch 4122: train loss: 4.8011745093390346e-05\n",
      "Epoch 4123: train loss: 4.0743423596723005e-05\n",
      "Epoch 4124: train loss: 3.469763760222122e-05\n",
      "Epoch 4125: train loss: 3.0482518923236057e-05\n",
      "Epoch 4126: train loss: 2.744460289250128e-05\n",
      "Epoch 4127: train loss: 2.5448012820561416e-05\n",
      "Epoch 4128: train loss: 2.416746247035917e-05\n",
      "Epoch 4129: train loss: 2.340631908737123e-05\n",
      "Epoch 4130: train loss: 2.296155616932083e-05\n",
      "Epoch 4131: train loss: 2.271129415021278e-05\n",
      "Epoch 4132: train loss: 2.2581432858714834e-05\n",
      "Epoch 4133: train loss: 2.2537495169672184e-05\n",
      "Epoch 4134: train loss: 2.256234256492462e-05\n",
      "Epoch 4135: train loss: 2.2649213860859163e-05\n",
      "Epoch 4136: train loss: 2.2802276362199336e-05\n",
      "Epoch 4137: train loss: 2.3024462279863656e-05\n",
      "Epoch 4138: train loss: 2.333406882826239e-05\n",
      "Epoch 4139: train loss: 2.3762078853906132e-05\n",
      "Epoch 4140: train loss: 2.4379785827477463e-05\n",
      "Epoch 4141: train loss: 2.5259359972551465e-05\n",
      "Epoch 4142: train loss: 2.6587995307636447e-05\n",
      "Epoch 4143: train loss: 2.853152182069607e-05\n",
      "Epoch 4144: train loss: 3.158519757562317e-05\n",
      "Epoch 4145: train loss: 3.6136869312031195e-05\n",
      "Epoch 4146: train loss: 4.335642370278947e-05\n",
      "Epoch 4147: train loss: 5.3664352890336886e-05\n",
      "Epoch 4148: train loss: 6.930007657501847e-05\n",
      "Epoch 4149: train loss: 8.887765579856932e-05\n",
      "Epoch 4150: train loss: 0.00011478516535134986\n",
      "Epoch 4151: train loss: 0.00013574633339885622\n",
      "Epoch 4152: train loss: 0.00015087838983163238\n",
      "Epoch 4153: train loss: 0.00013808062067255378\n",
      "Epoch 4154: train loss: 0.00011144179006805643\n",
      "Epoch 4155: train loss: 7.341687887674198e-05\n",
      "Epoch 4156: train loss: 4.650419214158319e-05\n",
      "Epoch 4157: train loss: 3.0623035854659975e-05\n",
      "Epoch 4158: train loss: 2.3614853489561938e-05\n",
      "Epoch 4159: train loss: 2.2568337953998707e-05\n",
      "Epoch 4160: train loss: 2.5866938813123852e-05\n",
      "Epoch 4161: train loss: 3.23198146361392e-05\n",
      "Epoch 4162: train loss: 4.1081904782913625e-05\n",
      "Epoch 4163: train loss: 5.273472561384551e-05\n",
      "Epoch 4164: train loss: 6.631790893152356e-05\n",
      "Epoch 4165: train loss: 8.026979776332155e-05\n",
      "Epoch 4166: train loss: 8.929904288379475e-05\n",
      "Epoch 4167: train loss: 9.290192247135565e-05\n",
      "Epoch 4168: train loss: 8.617918501840904e-05\n",
      "Epoch 4169: train loss: 7.431528501911089e-05\n",
      "Epoch 4170: train loss: 5.6844470236683264e-05\n",
      "Epoch 4171: train loss: 4.18043855461292e-05\n",
      "Epoch 4172: train loss: 3.060105154872872e-05\n",
      "Epoch 4173: train loss: 2.442278309899848e-05\n",
      "Epoch 4174: train loss: 2.2266143787419423e-05\n",
      "Epoch 4175: train loss: 2.3304344722419046e-05\n",
      "Epoch 4176: train loss: 2.6535622964729555e-05\n",
      "Epoch 4177: train loss: 3.103444760199636e-05\n",
      "Epoch 4178: train loss: 3.650266080512665e-05\n",
      "Epoch 4179: train loss: 4.222397546982393e-05\n",
      "Epoch 4180: train loss: 4.7969191655283794e-05\n",
      "Epoch 4181: train loss: 5.1863080443581566e-05\n",
      "Epoch 4182: train loss: 5.413017061073333e-05\n",
      "Epoch 4183: train loss: 5.3894906159257516e-05\n",
      "Epoch 4184: train loss: 5.282337951939553e-05\n",
      "Epoch 4185: train loss: 4.953617826686241e-05\n",
      "Epoch 4186: train loss: 4.5501550630433485e-05\n",
      "Epoch 4187: train loss: 3.991647099610418e-05\n",
      "Epoch 4188: train loss: 3.480804662103765e-05\n",
      "Epoch 4189: train loss: 3.0137865905999206e-05\n",
      "Epoch 4190: train loss: 2.66462466242956e-05\n",
      "Epoch 4191: train loss: 2.4123990442603827e-05\n",
      "Epoch 4192: train loss: 2.2565562176168896e-05\n",
      "Epoch 4193: train loss: 2.180819501518272e-05\n",
      "Epoch 4194: train loss: 2.1682833903469145e-05\n",
      "Epoch 4195: train loss: 2.2017571609467268e-05\n",
      "Epoch 4196: train loss: 2.270084951305762e-05\n",
      "Epoch 4197: train loss: 2.3664402760914527e-05\n",
      "Epoch 4198: train loss: 2.4878649128368124e-05\n",
      "Epoch 4199: train loss: 2.6435989639139734e-05\n",
      "Epoch 4200: train loss: 2.837032479874324e-05\n",
      "Epoch 4201: train loss: 3.087489676545374e-05\n",
      "Epoch 4202: train loss: 3.380176713108085e-05\n",
      "Epoch 4203: train loss: 3.7533187423832715e-05\n",
      "Epoch 4204: train loss: 4.1681425500428304e-05\n",
      "Epoch 4205: train loss: 4.7069141146494076e-05\n",
      "Epoch 4206: train loss: 5.238081939751282e-05\n",
      "Epoch 4207: train loss: 5.868893276783638e-05\n",
      "Epoch 4208: train loss: 6.318915984593332e-05\n",
      "Epoch 4209: train loss: 6.796044908696786e-05\n",
      "Epoch 4210: train loss: 6.954364653211087e-05\n",
      "Epoch 4211: train loss: 7.109133002813905e-05\n",
      "Epoch 4212: train loss: 6.889840005896986e-05\n",
      "Epoch 4213: train loss: 6.650768773397431e-05\n",
      "Epoch 4214: train loss: 6.127229426056147e-05\n",
      "Epoch 4215: train loss: 5.668665835401043e-05\n",
      "Epoch 4216: train loss: 5.112913277116604e-05\n",
      "Epoch 4217: train loss: 4.6810269850539044e-05\n",
      "Epoch 4218: train loss: 4.2483701690798625e-05\n",
      "Epoch 4219: train loss: 3.9263682992896065e-05\n",
      "Epoch 4220: train loss: 3.633882442954928e-05\n",
      "Epoch 4221: train loss: 3.429246135056019e-05\n",
      "Epoch 4222: train loss: 3.2574567740084603e-05\n",
      "Epoch 4223: train loss: 3.148708492517471e-05\n",
      "Epoch 4224: train loss: 3.065109558519907e-05\n",
      "Epoch 4225: train loss: 3.029780236829538e-05\n",
      "Epoch 4226: train loss: 3.0158189474605024e-05\n",
      "Epoch 4227: train loss: 3.045450648642145e-05\n",
      "Epoch 4228: train loss: 3.094210478593595e-05\n",
      "Epoch 4229: train loss: 3.190241113770753e-05\n",
      "Epoch 4230: train loss: 3.308898885734379e-05\n",
      "Epoch 4231: train loss: 3.492034011287615e-05\n",
      "Epoch 4232: train loss: 3.7052654079161584e-05\n",
      "Epoch 4233: train loss: 4.0077193261822686e-05\n",
      "Epoch 4234: train loss: 4.333650940679945e-05\n",
      "Epoch 4235: train loss: 4.770840678247623e-05\n",
      "Epoch 4236: train loss: 5.199683801038191e-05\n",
      "Epoch 4237: train loss: 5.749913543695584e-05\n",
      "Epoch 4238: train loss: 6.197328912094235e-05\n",
      "Epoch 4239: train loss: 6.710785964969546e-05\n",
      "Epoch 4240: train loss: 6.941017636563629e-05\n",
      "Epoch 4241: train loss: 7.136230851756409e-05\n",
      "Epoch 4242: train loss: 6.923946784809232e-05\n",
      "Epoch 4243: train loss: 6.65980187477544e-05\n",
      "Epoch 4244: train loss: 6.0721820773324e-05\n",
      "Epoch 4245: train loss: 5.52598612557631e-05\n",
      "Epoch 4246: train loss: 4.8638648877386004e-05\n",
      "Epoch 4247: train loss: 4.326151974964887e-05\n",
      "Epoch 4248: train loss: 3.8132555346237496e-05\n",
      "Epoch 4249: train loss: 3.426488910918124e-05\n",
      "Epoch 4250: train loss: 3.102311893599108e-05\n",
      "Epoch 4251: train loss: 2.8700846087303944e-05\n",
      "Epoch 4252: train loss: 2.6915620765066706e-05\n",
      "Epoch 4253: train loss: 2.5701572667458095e-05\n",
      "Epoch 4254: train loss: 2.4829843823681585e-05\n",
      "Epoch 4255: train loss: 2.4286906409543008e-05\n",
      "Epoch 4256: train loss: 2.394967305008322e-05\n",
      "Epoch 4257: train loss: 2.383101309533231e-05\n",
      "Epoch 4258: train loss: 2.386838423262816e-05\n",
      "Epoch 4259: train loss: 2.4112114260788076e-05\n",
      "Epoch 4260: train loss: 2.453049637551885e-05\n",
      "Epoch 4261: train loss: 2.5228355298168026e-05\n",
      "Epoch 4262: train loss: 2.6193551093456335e-05\n",
      "Epoch 4263: train loss: 2.7637090170173906e-05\n",
      "Epoch 4264: train loss: 2.9552420528489165e-05\n",
      "Epoch 4265: train loss: 3.2359737815568224e-05\n",
      "Epoch 4266: train loss: 3.5997982195112854e-05\n",
      "Epoch 4267: train loss: 4.1288691136287525e-05\n",
      "Epoch 4268: train loss: 4.785888813785277e-05\n",
      "Epoch 4269: train loss: 5.712849088013172e-05\n",
      "Epoch 4270: train loss: 6.739737000316381e-05\n",
      "Epoch 4271: train loss: 8.049481402849779e-05\n",
      "Epoch 4272: train loss: 9.113811393035576e-05\n",
      "Epoch 4273: train loss: 0.00010126680717803538\n",
      "Epoch 4274: train loss: 0.00010161710088141263\n",
      "Epoch 4275: train loss: 9.721694368636236e-05\n",
      "Epoch 4276: train loss: 8.244760101661086e-05\n",
      "Epoch 4277: train loss: 6.731931352987885e-05\n",
      "Epoch 4278: train loss: 5.144374881638214e-05\n",
      "Epoch 4279: train loss: 3.988167372881435e-05\n",
      "Epoch 4280: train loss: 3.1253988709067926e-05\n",
      "Epoch 4281: train loss: 2.5724139049998485e-05\n",
      "Epoch 4282: train loss: 2.237285480077844e-05\n",
      "Epoch 4283: train loss: 2.0754838260472752e-05\n",
      "Epoch 4284: train loss: 2.0387476979522035e-05\n",
      "Epoch 4285: train loss: 2.0925161152263172e-05\n",
      "Epoch 4286: train loss: 2.2157955754664727e-05\n",
      "Epoch 4287: train loss: 2.397714342805557e-05\n",
      "Epoch 4288: train loss: 2.6409601559862494e-05\n",
      "Epoch 4289: train loss: 2.9447564884321764e-05\n",
      "Epoch 4290: train loss: 3.332753840368241e-05\n",
      "Epoch 4291: train loss: 3.781671330216341e-05\n",
      "Epoch 4292: train loss: 4.331584568717517e-05\n",
      "Epoch 4293: train loss: 4.8800575314089656e-05\n",
      "Epoch 4294: train loss: 5.508567483047955e-05\n",
      "Epoch 4295: train loss: 5.9715650422731414e-05\n",
      "Epoch 4296: train loss: 6.412713992176577e-05\n",
      "Epoch 4297: train loss: 6.464350008172914e-05\n",
      "Epoch 4298: train loss: 6.417618715204298e-05\n",
      "Epoch 4299: train loss: 6.000488065183163e-05\n",
      "Epoch 4300: train loss: 5.5930660892045125e-05\n",
      "Epoch 4301: train loss: 5.023087578592822e-05\n",
      "Epoch 4302: train loss: 4.5492804929381236e-05\n",
      "Epoch 4303: train loss: 4.058781632920727e-05\n",
      "Epoch 4304: train loss: 3.686917625600472e-05\n",
      "Epoch 4305: train loss: 3.3658496249699965e-05\n",
      "Epoch 4306: train loss: 3.14444478135556e-05\n",
      "Epoch 4307: train loss: 2.970668901980389e-05\n",
      "Epoch 4308: train loss: 2.8595728508662432e-05\n",
      "Epoch 4309: train loss: 2.779357964755036e-05\n",
      "Epoch 4310: train loss: 2.743729237408843e-05\n",
      "Epoch 4311: train loss: 2.73283185379114e-05\n",
      "Epoch 4312: train loss: 2.7608120944933034e-05\n",
      "Epoch 4313: train loss: 2.8090224077459425e-05\n",
      "Epoch 4314: train loss: 2.8977021429454908e-05\n",
      "Epoch 4315: train loss: 3.0089197025517933e-05\n",
      "Epoch 4316: train loss: 3.176521568093449e-05\n",
      "Epoch 4317: train loss: 3.3747772249625996e-05\n",
      "Epoch 4318: train loss: 3.655273758340627e-05\n",
      "Epoch 4319: train loss: 3.970664693042636e-05\n",
      "Epoch 4320: train loss: 4.4030766730429605e-05\n",
      "Epoch 4321: train loss: 4.861807974521071e-05\n",
      "Epoch 4322: train loss: 5.4668289521941915e-05\n",
      "Epoch 4323: train loss: 6.027225754223764e-05\n",
      "Epoch 4324: train loss: 6.698133802274242e-05\n",
      "Epoch 4325: train loss: 7.139807712519541e-05\n",
      "Epoch 4326: train loss: 7.568191358586773e-05\n",
      "Epoch 4327: train loss: 7.536116754636168e-05\n",
      "Epoch 4328: train loss: 7.379380258498713e-05\n",
      "Epoch 4329: train loss: 6.742424739059061e-05\n",
      "Epoch 4330: train loss: 6.079195736674592e-05\n",
      "Epoch 4331: train loss: 5.2206465625204146e-05\n",
      "Epoch 4332: train loss: 4.504328535404056e-05\n",
      "Epoch 4333: train loss: 3.825471503660083e-05\n",
      "Epoch 4334: train loss: 3.311779801151715e-05\n",
      "Epoch 4335: train loss: 2.8941876735188998e-05\n",
      "Epoch 4336: train loss: 2.5974641175707802e-05\n",
      "Epoch 4337: train loss: 2.381655212957412e-05\n",
      "Epoch 4338: train loss: 2.2370732040144503e-05\n",
      "Epoch 4339: train loss: 2.1389963876572438e-05\n",
      "Epoch 4340: train loss: 2.0749484974658117e-05\n",
      "Epoch 4341: train loss: 2.032475094893016e-05\n",
      "Epoch 4342: train loss: 2.0046127247042023e-05\n",
      "Epoch 4343: train loss: 1.985875678656157e-05\n",
      "Epoch 4344: train loss: 1.9732508008019067e-05\n",
      "Epoch 4345: train loss: 1.9646058717626147e-05\n",
      "Epoch 4346: train loss: 1.9586550479289144e-05\n",
      "Epoch 4347: train loss: 1.9543824237189256e-05\n",
      "Epoch 4348: train loss: 1.951076410477981e-05\n",
      "Epoch 4349: train loss: 1.9482440620777197e-05\n",
      "Epoch 4350: train loss: 1.945617259480059e-05\n",
      "Epoch 4351: train loss: 1.943104507518001e-05\n",
      "Epoch 4352: train loss: 1.9407671061344445e-05\n",
      "Epoch 4353: train loss: 1.938740570039954e-05\n",
      "Epoch 4354: train loss: 1.9372873794054613e-05\n",
      "Epoch 4355: train loss: 1.9366585547686554e-05\n",
      "Epoch 4356: train loss: 1.9372884707991034e-05\n",
      "Epoch 4357: train loss: 1.939844514708966e-05\n",
      "Epoch 4358: train loss: 1.9456876543699764e-05\n",
      "Epoch 4359: train loss: 1.9570285076042637e-05\n",
      "Epoch 4360: train loss: 1.9782029994530603e-05\n",
      "Epoch 4361: train loss: 2.016134931182023e-05\n",
      "Epoch 4362: train loss: 2.0844488972215913e-05\n",
      "Epoch 4363: train loss: 2.2048257960705087e-05\n",
      "Epoch 4364: train loss: 2.4219478291342966e-05\n",
      "Epoch 4365: train loss: 2.8016707801725715e-05\n",
      "Epoch 4366: train loss: 3.4903805499197915e-05\n",
      "Epoch 4367: train loss: 4.663687650463544e-05\n",
      "Epoch 4368: train loss: 6.748198211425915e-05\n",
      "Epoch 4369: train loss: 9.938381117535755e-05\n",
      "Epoch 4370: train loss: 0.0001487727276980877\n",
      "Epoch 4371: train loss: 0.00019869982497766614\n",
      "Epoch 4372: train loss: 0.00023608646006323397\n",
      "Epoch 4373: train loss: 0.000204148018383421\n",
      "Epoch 4374: train loss: 0.0001269975327886641\n",
      "Epoch 4375: train loss: 4.911230644211173e-05\n",
      "Epoch 4376: train loss: 2.2392570826923475e-05\n",
      "Epoch 4377: train loss: 3.289271262474358e-05\n",
      "Epoch 4378: train loss: 7.216753147076815e-05\n",
      "Epoch 4379: train loss: 0.0001376831205561757\n",
      "Epoch 4380: train loss: 0.0001733776298351586\n",
      "Epoch 4381: train loss: 0.00014436618948820978\n",
      "Epoch 4382: train loss: 7.30469764675945e-05\n",
      "Epoch 4383: train loss: 2.8448233933886513e-05\n",
      "Epoch 4384: train loss: 2.329299422854092e-05\n",
      "Epoch 4385: train loss: 4.161780088907108e-05\n",
      "Epoch 4386: train loss: 8.111867646221071e-05\n",
      "Epoch 4387: train loss: 0.00011775970051530749\n",
      "Epoch 4388: train loss: 0.0001139114101533778\n",
      "Epoch 4389: train loss: 6.774128996767104e-05\n",
      "Epoch 4390: train loss: 3.0234201403800398e-05\n",
      "Epoch 4391: train loss: 2.2023996280040592e-05\n",
      "Epoch 4392: train loss: 3.347324309288524e-05\n",
      "Epoch 4393: train loss: 5.954427251708694e-05\n",
      "Epoch 4394: train loss: 8.279540634248406e-05\n",
      "Epoch 4395: train loss: 7.891264249337837e-05\n",
      "Epoch 4396: train loss: 4.835997606278397e-05\n",
      "Epoch 4397: train loss: 2.5626861315686256e-05\n",
      "Epoch 4398: train loss: 2.1399959223344922e-05\n",
      "Epoch 4399: train loss: 2.934776057372801e-05\n",
      "Epoch 4400: train loss: 4.601042019203305e-05\n",
      "Epoch 4401: train loss: 5.912625783821568e-05\n",
      "Epoch 4402: train loss: 5.629311635857448e-05\n",
      "Epoch 4403: train loss: 4.049910421599634e-05\n",
      "Epoch 4404: train loss: 2.723875513765961e-05\n",
      "Epoch 4405: train loss: 2.080041122098919e-05\n",
      "Epoch 4406: train loss: 2.0526045773294754e-05\n",
      "Epoch 4407: train loss: 2.5539771741023287e-05\n",
      "Epoch 4408: train loss: 3.164120062137954e-05\n",
      "Epoch 4409: train loss: 3.441693479544483e-05\n",
      "Epoch 4410: train loss: 3.300290700281039e-05\n",
      "Epoch 4411: train loss: 2.9480155717465095e-05\n",
      "Epoch 4412: train loss: 2.4873643269529566e-05\n",
      "Epoch 4413: train loss: 2.136271541530732e-05\n",
      "Epoch 4414: train loss: 1.969743061636109e-05\n",
      "Epoch 4415: train loss: 1.9537861589924432e-05\n",
      "Epoch 4416: train loss: 2.069962465611752e-05\n",
      "Epoch 4417: train loss: 2.2637939764535986e-05\n",
      "Epoch 4418: train loss: 2.425291677354835e-05\n",
      "Epoch 4419: train loss: 2.5113507945206948e-05\n",
      "Epoch 4420: train loss: 2.533408951421734e-05\n",
      "Epoch 4421: train loss: 2.4354780180146918e-05\n",
      "Epoch 4422: train loss: 2.266756746394094e-05\n",
      "Epoch 4423: train loss: 2.10880498343613e-05\n",
      "Epoch 4424: train loss: 1.9937542674597353e-05\n",
      "Epoch 4425: train loss: 1.9162604075972922e-05\n",
      "Epoch 4426: train loss: 1.891562897071708e-05\n",
      "Epoch 4427: train loss: 1.9092060028924607e-05\n",
      "Epoch 4428: train loss: 1.9424214769969694e-05\n",
      "Epoch 4429: train loss: 1.9890814655809663e-05\n",
      "Epoch 4430: train loss: 2.040493200183846e-05\n",
      "Epoch 4431: train loss: 2.0852427041972987e-05\n",
      "Epoch 4432: train loss: 2.1209611077210866e-05\n",
      "Epoch 4433: train loss: 2.1448473489726894e-05\n",
      "Epoch 4434: train loss: 2.1481546355062164e-05\n",
      "Epoch 4435: train loss: 2.1448218831210397e-05\n",
      "Epoch 4436: train loss: 2.13971270568436e-05\n",
      "Epoch 4437: train loss: 2.136944567610044e-05\n",
      "Epoch 4438: train loss: 2.1345558707253076e-05\n",
      "Epoch 4439: train loss: 2.13757339224685e-05\n",
      "Epoch 4440: train loss: 2.1379757527029142e-05\n",
      "Epoch 4441: train loss: 2.1452156943269074e-05\n",
      "Epoch 4442: train loss: 2.1615951482090168e-05\n",
      "Epoch 4443: train loss: 2.194046464865096e-05\n",
      "Epoch 4444: train loss: 2.2424756025429815e-05\n",
      "Epoch 4445: train loss: 2.3187643819255754e-05\n",
      "Epoch 4446: train loss: 2.4165487047866918e-05\n",
      "Epoch 4447: train loss: 2.5557012122590095e-05\n",
      "Epoch 4448: train loss: 2.741593198152259e-05\n",
      "Epoch 4449: train loss: 3.024847865162883e-05\n",
      "Epoch 4450: train loss: 3.4126933314837515e-05\n",
      "Epoch 4451: train loss: 3.987365562352352e-05\n",
      "Epoch 4452: train loss: 4.699493365478702e-05\n",
      "Epoch 4453: train loss: 5.671681719832122e-05\n",
      "Epoch 4454: train loss: 6.73442191327922e-05\n",
      "Epoch 4455: train loss: 8.085398440016434e-05\n",
      "Epoch 4456: train loss: 9.201362263411283e-05\n",
      "Epoch 4457: train loss: 0.00010223792196484283\n",
      "Epoch 4458: train loss: 0.00010140648373635486\n",
      "Epoch 4459: train loss: 9.419507841812447e-05\n",
      "Epoch 4460: train loss: 7.66977173043415e-05\n",
      "Epoch 4461: train loss: 5.976553075015545e-05\n",
      "Epoch 4462: train loss: 4.406536027090624e-05\n",
      "Epoch 4463: train loss: 3.327221565996297e-05\n",
      "Epoch 4464: train loss: 2.5788818675209768e-05\n",
      "Epoch 4465: train loss: 2.1265715986373834e-05\n",
      "Epoch 4466: train loss: 1.8909895516117103e-05\n",
      "Epoch 4467: train loss: 1.822566264308989e-05\n",
      "Epoch 4468: train loss: 1.8760447346721776e-05\n",
      "Epoch 4469: train loss: 2.0211269657011144e-05\n",
      "Epoch 4470: train loss: 2.2432650439441204e-05\n",
      "Epoch 4471: train loss: 2.532665712351445e-05\n",
      "Epoch 4472: train loss: 2.8954998924746178e-05\n",
      "Epoch 4473: train loss: 3.3171058021252975e-05\n",
      "Epoch 4474: train loss: 3.826348256552592e-05\n",
      "Epoch 4475: train loss: 4.349489972810261e-05\n",
      "Epoch 4476: train loss: 4.926666588289663e-05\n",
      "Epoch 4477: train loss: 5.36568186362274e-05\n",
      "Epoch 4478: train loss: 5.762104410678148e-05\n",
      "Epoch 4479: train loss: 5.812607196276076e-05\n",
      "Epoch 4480: train loss: 5.734787191613577e-05\n",
      "Epoch 4481: train loss: 5.308942127157934e-05\n",
      "Epoch 4482: train loss: 4.866695235250518e-05\n",
      "Epoch 4483: train loss: 4.303899549995549e-05\n",
      "Epoch 4484: train loss: 3.8353591662598774e-05\n",
      "Epoch 4485: train loss: 3.37937890435569e-05\n",
      "Epoch 4486: train loss: 3.0266015528468415e-05\n",
      "Epoch 4487: train loss: 2.73485820798669e-05\n",
      "Epoch 4488: train loss: 2.5302277208538726e-05\n",
      "Epoch 4489: train loss: 2.3788381440681405e-05\n",
      "Epoch 4490: train loss: 2.2779087885282934e-05\n",
      "Epoch 4491: train loss: 2.20592428377131e-05\n",
      "Epoch 4492: train loss: 2.1624889996019192e-05\n",
      "Epoch 4493: train loss: 2.1388124878285453e-05\n",
      "Epoch 4494: train loss: 2.1376357835833915e-05\n",
      "Epoch 4495: train loss: 2.1523897885344923e-05\n",
      "Epoch 4496: train loss: 2.1882617147639394e-05\n",
      "Epoch 4497: train loss: 2.2411941245081834e-05\n",
      "Epoch 4498: train loss: 2.3242879251483828e-05\n",
      "Epoch 4499: train loss: 2.4362187104998156e-05\n",
      "Epoch 4500: train loss: 2.6018209609901533e-05\n",
      "Epoch 4501: train loss: 2.8183674658066593e-05\n",
      "Epoch 4502: train loss: 3.133575228275731e-05\n",
      "Epoch 4503: train loss: 3.54068361048121e-05\n",
      "Epoch 4504: train loss: 4.130608431296423e-05\n",
      "Epoch 4505: train loss: 4.8587586206849664e-05\n",
      "Epoch 4506: train loss: 5.862898979103193e-05\n",
      "Epoch 4507: train loss: 6.947456859052181e-05\n",
      "Epoch 4508: train loss: 8.258509478764609e-05\n",
      "Epoch 4509: train loss: 9.220949141308665e-05\n",
      "Epoch 4510: train loss: 9.951333777280524e-05\n",
      "Epoch 4511: train loss: 9.604533988749608e-05\n",
      "Epoch 4512: train loss: 8.712400449439883e-05\n",
      "Epoch 4513: train loss: 7.023736543487757e-05\n",
      "Epoch 4514: train loss: 5.456321014207788e-05\n",
      "Epoch 4515: train loss: 4.0347902540815994e-05\n",
      "Epoch 4516: train loss: 3.04639197565848e-05\n",
      "Epoch 4517: train loss: 2.3711229005130008e-05\n",
      "Epoch 4518: train loss: 1.9786441043834202e-05\n",
      "Epoch 4519: train loss: 1.7985674276133068e-05\n",
      "Epoch 4520: train loss: 1.779314516170416e-05\n",
      "Epoch 4521: train loss: 1.8780714526656084e-05\n",
      "Epoch 4522: train loss: 2.0666031559812836e-05\n",
      "Epoch 4523: train loss: 2.3324817448155954e-05\n",
      "Epoch 4524: train loss: 2.6651152438716963e-05\n",
      "Epoch 4525: train loss: 3.071917308261618e-05\n",
      "Epoch 4526: train loss: 3.526766522554681e-05\n",
      "Epoch 4527: train loss: 4.0517435991205275e-05\n",
      "Epoch 4528: train loss: 4.551519305096008e-05\n",
      "Epoch 4529: train loss: 5.0676739192567766e-05\n",
      "Epoch 4530: train loss: 5.392451203078963e-05\n",
      "Epoch 4531: train loss: 5.6300788855878636e-05\n",
      "Epoch 4532: train loss: 5.512008647201583e-05\n",
      "Epoch 4533: train loss: 5.276154479361139e-05\n",
      "Epoch 4534: train loss: 4.776547575602308e-05\n",
      "Epoch 4535: train loss: 4.3030671804444864e-05\n",
      "Epoch 4536: train loss: 3.7793317460455e-05\n",
      "Epoch 4537: train loss: 3.356130764586851e-05\n",
      "Epoch 4538: train loss: 2.9740758691332303e-05\n",
      "Epoch 4539: train loss: 2.687661981326528e-05\n",
      "Epoch 4540: train loss: 2.4634662622702308e-05\n",
      "Epoch 4541: train loss: 2.3099313693819568e-05\n",
      "Epoch 4542: train loss: 2.20012243516976e-05\n",
      "Epoch 4543: train loss: 2.1288591597112827e-05\n",
      "Epoch 4544: train loss: 2.081351340166293e-05\n",
      "Epoch 4545: train loss: 2.0573213987518102e-05\n",
      "Epoch 4546: train loss: 2.050340481218882e-05\n",
      "Epoch 4547: train loss: 2.0632736777770333e-05\n",
      "Epoch 4548: train loss: 2.0921946997987106e-05\n",
      "Epoch 4549: train loss: 2.1438147086882964e-05\n",
      "Epoch 4550: train loss: 2.2164553229231387e-05\n",
      "Epoch 4551: train loss: 2.325974674022291e-05\n",
      "Epoch 4552: train loss: 2.473131644364912e-05\n",
      "Epoch 4553: train loss: 2.6891642846749164e-05\n",
      "Epoch 4554: train loss: 2.9747832741122693e-05\n",
      "Epoch 4555: train loss: 3.39317848556675e-05\n",
      "Epoch 4556: train loss: 3.936531356885098e-05\n",
      "Epoch 4557: train loss: 4.718402124126442e-05\n",
      "Epoch 4558: train loss: 5.663318370352499e-05\n",
      "Epoch 4559: train loss: 6.922992179170251e-05\n",
      "Epoch 4560: train loss: 8.173137030098587e-05\n",
      "Epoch 4561: train loss: 9.511692769592628e-05\n",
      "Epoch 4562: train loss: 0.00010131492308573797\n",
      "Epoch 4563: train loss: 0.000101865480246488\n",
      "Epoch 4564: train loss: 8.978008554549888e-05\n",
      "Epoch 4565: train loss: 7.376798748737201e-05\n",
      "Epoch 4566: train loss: 5.4735737649025396e-05\n",
      "Epoch 4567: train loss: 4.007594543509185e-05\n",
      "Epoch 4568: train loss: 2.9170412744861096e-05\n",
      "Epoch 4569: train loss: 2.238385241071228e-05\n",
      "Epoch 4570: train loss: 1.8622771676746197e-05\n",
      "Epoch 4571: train loss: 1.72658110386692e-05\n",
      "Epoch 4572: train loss: 1.7700100215733983e-05\n",
      "Epoch 4573: train loss: 1.9461833289824426e-05\n",
      "Epoch 4574: train loss: 2.2284597434918396e-05\n",
      "Epoch 4575: train loss: 2.5976034521590918e-05\n",
      "Epoch 4576: train loss: 3.051365456485655e-05\n",
      "Epoch 4577: train loss: 3.556711817509495e-05\n",
      "Epoch 4578: train loss: 4.120120138395578e-05\n",
      "Epoch 4579: train loss: 4.64579788967967e-05\n",
      "Epoch 4580: train loss: 5.1532380894059315e-05\n",
      "Epoch 4581: train loss: 5.436730134533718e-05\n",
      "Epoch 4582: train loss: 5.589535794570111e-05\n",
      "Epoch 4583: train loss: 5.364528260543011e-05\n",
      "Epoch 4584: train loss: 4.994249320589006e-05\n",
      "Epoch 4585: train loss: 4.3715688661905006e-05\n",
      "Epoch 4586: train loss: 3.785286025959067e-05\n",
      "Epoch 4587: train loss: 3.2095656933961436e-05\n",
      "Epoch 4588: train loss: 2.76500650215894e-05\n",
      "Epoch 4589: train loss: 2.408193176961504e-05\n",
      "Epoch 4590: train loss: 2.1533736799028702e-05\n",
      "Epoch 4591: train loss: 1.9737817638088018e-05\n",
      "Epoch 4592: train loss: 1.858171890489757e-05\n",
      "Epoch 4593: train loss: 1.7851927623269148e-05\n",
      "Epoch 4594: train loss: 1.7407579434802756e-05\n",
      "Epoch 4595: train loss: 1.7141799617093056e-05\n",
      "Epoch 4596: train loss: 1.699257336440496e-05\n",
      "Epoch 4597: train loss: 1.6925210729823448e-05\n",
      "Epoch 4598: train loss: 1.691946636128705e-05\n",
      "Epoch 4599: train loss: 1.6964289898169227e-05\n",
      "Epoch 4600: train loss: 1.7053414921974763e-05\n",
      "Epoch 4601: train loss: 1.718842213449534e-05\n",
      "Epoch 4602: train loss: 1.7375721654389054e-05\n",
      "Epoch 4603: train loss: 1.7639187717577443e-05\n",
      "Epoch 4604: train loss: 1.801374128262978e-05\n",
      "Epoch 4605: train loss: 1.8572045519249514e-05\n",
      "Epoch 4606: train loss: 1.9401228200877085e-05\n",
      "Epoch 4607: train loss: 2.0697205400210805e-05\n",
      "Epoch 4608: train loss: 2.2675694708595984e-05\n",
      "Epoch 4609: train loss: 2.5851208192761987e-05\n",
      "Epoch 4610: train loss: 3.072707477258518e-05\n",
      "Epoch 4611: train loss: 3.861626100842841e-05\n",
      "Epoch 4612: train loss: 5.035746289649978e-05\n",
      "Epoch 4613: train loss: 6.85246050124988e-05\n",
      "Epoch 4614: train loss: 9.182199573842809e-05\n",
      "Epoch 4615: train loss: 0.00012143084313720465\n",
      "Epoch 4616: train loss: 0.00014389697753358632\n",
      "Epoch 4617: train loss: 0.0001539131044410169\n",
      "Epoch 4618: train loss: 0.00013160816160961986\n",
      "Epoch 4619: train loss: 9.3331269454211e-05\n",
      "Epoch 4620: train loss: 5.26976800756529e-05\n",
      "Epoch 4621: train loss: 2.886167749238666e-05\n",
      "Epoch 4622: train loss: 1.8686838302528486e-05\n",
      "Epoch 4623: train loss: 1.788655026757624e-05\n",
      "Epoch 4624: train loss: 2.425102866254747e-05\n",
      "Epoch 4625: train loss: 3.4866541682276875e-05\n",
      "Epoch 4626: train loss: 4.729274951387197e-05\n",
      "Epoch 4627: train loss: 5.985260213492438e-05\n",
      "Epoch 4628: train loss: 7.246219320222735e-05\n",
      "Epoch 4629: train loss: 8.103762957034633e-05\n",
      "Epoch 4630: train loss: 8.186433115042746e-05\n",
      "Epoch 4631: train loss: 7.184183778008446e-05\n",
      "Epoch 4632: train loss: 5.56657905690372e-05\n",
      "Epoch 4633: train loss: 3.847212792607024e-05\n",
      "Epoch 4634: train loss: 2.6142230126424693e-05\n",
      "Epoch 4635: train loss: 1.911735671455972e-05\n",
      "Epoch 4636: train loss: 1.7038806618074887e-05\n",
      "Epoch 4637: train loss: 1.8863938748836517e-05\n",
      "Epoch 4638: train loss: 2.2973805243964307e-05\n",
      "Epoch 4639: train loss: 2.7851043341797777e-05\n",
      "Epoch 4640: train loss: 3.2195104722632095e-05\n",
      "Epoch 4641: train loss: 3.587687388062477e-05\n",
      "Epoch 4642: train loss: 3.802035280386917e-05\n",
      "Epoch 4643: train loss: 3.848633787129074e-05\n",
      "Epoch 4644: train loss: 3.627108162618242e-05\n",
      "Epoch 4645: train loss: 3.2730029488448054e-05\n",
      "Epoch 4646: train loss: 2.8764556191163138e-05\n",
      "Epoch 4647: train loss: 2.5318642656202428e-05\n",
      "Epoch 4648: train loss: 2.232246879430022e-05\n",
      "Epoch 4649: train loss: 1.9963592421845533e-05\n",
      "Epoch 4650: train loss: 1.8185770386480726e-05\n",
      "Epoch 4651: train loss: 1.7099868273362517e-05\n",
      "Epoch 4652: train loss: 1.6654097635182552e-05\n",
      "Epoch 4653: train loss: 1.6681282431818545e-05\n",
      "Epoch 4654: train loss: 1.7058315279427916e-05\n",
      "Epoch 4655: train loss: 1.7693673726171255e-05\n",
      "Epoch 4656: train loss: 1.8468803318683058e-05\n",
      "Epoch 4657: train loss: 1.931486440298613e-05\n",
      "Epoch 4658: train loss: 2.0287527149775997e-05\n",
      "Epoch 4659: train loss: 2.1356412617024034e-05\n",
      "Epoch 4660: train loss: 2.2629612431046553e-05\n",
      "Epoch 4661: train loss: 2.4067900085356086e-05\n",
      "Epoch 4662: train loss: 2.5806341000134125e-05\n",
      "Epoch 4663: train loss: 2.7728945497074164e-05\n",
      "Epoch 4664: train loss: 3.0194802093319595e-05\n",
      "Epoch 4665: train loss: 3.304602796561085e-05\n",
      "Epoch 4666: train loss: 3.681780435726978e-05\n",
      "Epoch 4667: train loss: 4.0818478737492114e-05\n",
      "Epoch 4668: train loss: 4.562143294606358e-05\n",
      "Epoch 4669: train loss: 4.9878977733897045e-05\n",
      "Epoch 4670: train loss: 5.480302934302017e-05\n",
      "Epoch 4671: train loss: 5.857052747160196e-05\n",
      "Epoch 4672: train loss: 6.266949640121311e-05\n",
      "Epoch 4673: train loss: 6.398832192644477e-05\n",
      "Epoch 4674: train loss: 6.421733269235119e-05\n",
      "Epoch 4675: train loss: 6.09105518378783e-05\n",
      "Epoch 4676: train loss: 5.702202906832099e-05\n",
      "Epoch 4677: train loss: 5.133746162755415e-05\n",
      "Epoch 4678: train loss: 4.620547770173289e-05\n",
      "Epoch 4679: train loss: 4.0572864236310124e-05\n",
      "Epoch 4680: train loss: 3.577113602659665e-05\n",
      "Epoch 4681: train loss: 3.1309853511629626e-05\n",
      "Epoch 4682: train loss: 2.7848052923218347e-05\n",
      "Epoch 4683: train loss: 2.503701762179844e-05\n",
      "Epoch 4684: train loss: 2.298722574778367e-05\n",
      "Epoch 4685: train loss: 2.141497316188179e-05\n",
      "Epoch 4686: train loss: 2.0294563000788912e-05\n",
      "Epoch 4687: train loss: 1.94738240679726e-05\n",
      "Epoch 4688: train loss: 1.8925213225884363e-05\n",
      "Epoch 4689: train loss: 1.8562615878181532e-05\n",
      "Epoch 4690: train loss: 1.8372866179561242e-05\n",
      "Epoch 4691: train loss: 1.83092852239497e-05\n",
      "Epoch 4692: train loss: 1.8383701899438165e-05\n",
      "Epoch 4693: train loss: 1.8579639800009318e-05\n",
      "Epoch 4694: train loss: 1.895003515528515e-05\n",
      "Epoch 4695: train loss: 1.9510782294673845e-05\n",
      "Epoch 4696: train loss: 2.0379808120196685e-05\n",
      "Epoch 4697: train loss: 2.1609737814287655e-05\n",
      "Epoch 4698: train loss: 2.3443297322955914e-05\n",
      "Epoch 4699: train loss: 2.5984281819546595e-05\n",
      "Epoch 4700: train loss: 2.9767794330837205e-05\n",
      "Epoch 4701: train loss: 3.495422060950659e-05\n",
      "Epoch 4702: train loss: 4.260166679159738e-05\n",
      "Epoch 4703: train loss: 5.245306238066405e-05\n",
      "Epoch 4704: train loss: 6.589731492567807e-05\n",
      "Epoch 4705: train loss: 8.030848402995616e-05\n",
      "Epoch 4706: train loss: 9.616976603865623e-05\n",
      "Epoch 4707: train loss: 0.00010542871314100921\n",
      "Epoch 4708: train loss: 0.0001078801869880408\n",
      "Epoch 4709: train loss: 9.568082168698311e-05\n",
      "Epoch 4710: train loss: 7.70835904404521e-05\n",
      "Epoch 4711: train loss: 5.485042856889777e-05\n",
      "Epoch 4712: train loss: 3.7885714846197516e-05\n",
      "Epoch 4713: train loss: 2.610152841953095e-05\n",
      "Epoch 4714: train loss: 1.937379602168221e-05\n",
      "Epoch 4715: train loss: 1.6392315956181847e-05\n",
      "Epoch 4716: train loss: 1.6289179257000796e-05\n",
      "Epoch 4717: train loss: 1.8304659533896483e-05\n",
      "Epoch 4718: train loss: 2.18876484723296e-05\n",
      "Epoch 4719: train loss: 2.677148768270854e-05\n",
      "Epoch 4720: train loss: 3.2624440791551024e-05\n",
      "Epoch 4721: train loss: 3.9169615774881095e-05\n",
      "Epoch 4722: train loss: 4.538889697869308e-05\n",
      "Epoch 4723: train loss: 5.0858481699833646e-05\n",
      "Epoch 4724: train loss: 5.394831168814562e-05\n",
      "Epoch 4725: train loss: 5.4989370255498216e-05\n",
      "Epoch 4726: train loss: 5.225857603363693e-05\n",
      "Epoch 4727: train loss: 4.7705925680929795e-05\n",
      "Epoch 4728: train loss: 4.057730620843358e-05\n",
      "Epoch 4729: train loss: 3.3564727345947176e-05\n",
      "Epoch 4730: train loss: 2.7003794457414187e-05\n",
      "Epoch 4731: train loss: 2.2136249754112214e-05\n",
      "Epoch 4732: train loss: 1.878116381703876e-05\n",
      "Epoch 4733: train loss: 1.6831651009852067e-05\n",
      "Epoch 4734: train loss: 1.596591573616024e-05\n",
      "Epoch 4735: train loss: 1.5918092685751617e-05\n",
      "Epoch 4736: train loss: 1.646777309360914e-05\n",
      "Epoch 4737: train loss: 1.746157431625761e-05\n",
      "Epoch 4738: train loss: 1.8845232261810452e-05\n",
      "Epoch 4739: train loss: 2.059032158285845e-05\n",
      "Epoch 4740: train loss: 2.27243854169501e-05\n",
      "Epoch 4741: train loss: 2.520753514545504e-05\n",
      "Epoch 4742: train loss: 2.8243915949133225e-05\n",
      "Epoch 4743: train loss: 3.1646221032133326e-05\n",
      "Epoch 4744: train loss: 3.568183092284016e-05\n",
      "Epoch 4745: train loss: 3.95920142182149e-05\n",
      "Epoch 4746: train loss: 4.3856449337909e-05\n",
      "Epoch 4747: train loss: 4.7190605982905254e-05\n",
      "Epoch 4748: train loss: 5.064066863269545e-05\n",
      "Epoch 4749: train loss: 5.2515963034238666e-05\n",
      "Epoch 4750: train loss: 5.421742025646381e-05\n",
      "Epoch 4751: train loss: 5.3929496061755344e-05\n",
      "Epoch 4752: train loss: 5.3304644097806886e-05\n",
      "Epoch 4753: train loss: 5.09337151015643e-05\n",
      "Epoch 4754: train loss: 4.853878999711014e-05\n",
      "Epoch 4755: train loss: 4.522665767581202e-05\n",
      "Epoch 4756: train loss: 4.2299918277421966e-05\n",
      "Epoch 4757: train loss: 3.9007874875096604e-05\n",
      "Epoch 4758: train loss: 3.6163964978186414e-05\n",
      "Epoch 4759: train loss: 3.3270742278546095e-05\n",
      "Epoch 4760: train loss: 3.09181195916608e-05\n",
      "Epoch 4761: train loss: 2.8778240448446013e-05\n",
      "Epoch 4762: train loss: 2.7154634153703228e-05\n",
      "Epoch 4763: train loss: 2.574215795903001e-05\n",
      "Epoch 4764: train loss: 2.4717652195249684e-05\n",
      "Epoch 4765: train loss: 2.3891529053798877e-05\n",
      "Epoch 4766: train loss: 2.3417069314746186e-05\n",
      "Epoch 4767: train loss: 2.314990524610039e-05\n",
      "Epoch 4768: train loss: 2.3207767299027182e-05\n",
      "Epoch 4769: train loss: 2.3462165700038895e-05\n",
      "Epoch 4770: train loss: 2.40709341596812e-05\n",
      "Epoch 4771: train loss: 2.4936565750977024e-05\n",
      "Epoch 4772: train loss: 2.6295298084733076e-05\n",
      "Epoch 4773: train loss: 2.803372080961708e-05\n",
      "Epoch 4774: train loss: 3.0500534194288775e-05\n",
      "Epoch 4775: train loss: 3.347547317389399e-05\n",
      "Epoch 4776: train loss: 3.747555092559196e-05\n",
      "Epoch 4777: train loss: 4.198241367703304e-05\n",
      "Epoch 4778: train loss: 4.76991044706665e-05\n",
      "Epoch 4779: train loss: 5.339030030881986e-05\n",
      "Epoch 4780: train loss: 5.982253424008377e-05\n",
      "Epoch 4781: train loss: 6.451304216170684e-05\n",
      "Epoch 4782: train loss: 6.831959035480395e-05\n",
      "Epoch 4783: train loss: 6.809018668718636e-05\n",
      "Epoch 4784: train loss: 6.575429870281368e-05\n",
      "Epoch 4785: train loss: 5.945225711911917e-05\n",
      "Epoch 4786: train loss: 5.2341478294692934e-05\n",
      "Epoch 4787: train loss: 4.3988962715957314e-05\n",
      "Epoch 4788: train loss: 3.670990190585144e-05\n",
      "Epoch 4789: train loss: 3.022908276761882e-05\n",
      "Epoch 4790: train loss: 2.5278350221924484e-05\n",
      "Epoch 4791: train loss: 2.1520872905966826e-05\n",
      "Epoch 4792: train loss: 1.8926737539004534e-05\n",
      "Epoch 4793: train loss: 1.7208876670338213e-05\n",
      "Epoch 4794: train loss: 1.616870576981455e-05\n",
      "Epoch 4795: train loss: 1.5603958672727458e-05\n",
      "Epoch 4796: train loss: 1.5369854736491106e-05\n",
      "Epoch 4797: train loss: 1.536713716632221e-05\n",
      "Epoch 4798: train loss: 1.553501351736486e-05\n",
      "Epoch 4799: train loss: 1.5845555026317015e-05\n",
      "Epoch 4800: train loss: 1.6292462532874197e-05\n",
      "Epoch 4801: train loss: 1.6895452063181438e-05\n",
      "Epoch 4802: train loss: 1.768027686921414e-05\n",
      "Epoch 4803: train loss: 1.8723325410974212e-05\n",
      "Epoch 4804: train loss: 2.0080360627616756e-05\n",
      "Epoch 4805: train loss: 2.1949215806671418e-05\n",
      "Epoch 4806: train loss: 2.4434128135908395e-05\n",
      "Epoch 4807: train loss: 2.7978812795481645e-05\n",
      "Epoch 4808: train loss: 3.270108572905883e-05\n",
      "Epoch 4809: train loss: 3.945360367652029e-05\n",
      "Epoch 4810: train loss: 4.807353252544999e-05\n",
      "Epoch 4811: train loss: 5.9758916904684156e-05\n",
      "Epoch 4812: train loss: 7.274426752701402e-05\n",
      "Epoch 4813: train loss: 8.738278120290488e-05\n",
      "Epoch 4814: train loss: 9.767832671059296e-05\n",
      "Epoch 4815: train loss: 0.00010248974285786971\n",
      "Epoch 4816: train loss: 9.493115794612095e-05\n",
      "Epoch 4817: train loss: 7.990097947185859e-05\n",
      "Epoch 4818: train loss: 5.9408841480035335e-05\n",
      "Epoch 4819: train loss: 4.202376658213325e-05\n",
      "Epoch 4820: train loss: 2.8860449674539268e-05\n",
      "Epoch 4821: train loss: 2.0725481590488926e-05\n",
      "Epoch 4822: train loss: 1.644269286771305e-05\n",
      "Epoch 4823: train loss: 1.5235073988151271e-05\n",
      "Epoch 4824: train loss: 1.6332181985490024e-05\n",
      "Epoch 4825: train loss: 1.911258186737541e-05\n",
      "Epoch 4826: train loss: 2.3193319066194817e-05\n",
      "Epoch 4827: train loss: 2.8265831133467145e-05\n",
      "Epoch 4828: train loss: 3.40583901561331e-05\n",
      "Epoch 4829: train loss: 3.9895461668493226e-05\n",
      "Epoch 4830: train loss: 4.530925798462704e-05\n",
      "Epoch 4831: train loss: 4.9139194743474945e-05\n",
      "Epoch 4832: train loss: 5.133325976203196e-05\n",
      "Epoch 4833: train loss: 5.047010927228257e-05\n",
      "Epoch 4834: train loss: 4.7599936806363985e-05\n",
      "Epoch 4835: train loss: 4.188896855339408e-05\n",
      "Epoch 4836: train loss: 3.561959238140844e-05\n",
      "Epoch 4837: train loss: 2.9140028345864266e-05\n",
      "Epoch 4838: train loss: 2.395837873336859e-05\n",
      "Epoch 4839: train loss: 2.00247759494232e-05\n",
      "Epoch 4840: train loss: 1.7419988580513746e-05\n",
      "Epoch 4841: train loss: 1.5872610674705356e-05\n",
      "Epoch 4842: train loss: 1.5170107872108929e-05\n",
      "Epoch 4843: train loss: 1.5090181477717124e-05\n",
      "Epoch 4844: train loss: 1.5452804291271605e-05\n",
      "Epoch 4845: train loss: 1.6153247997863218e-05\n",
      "Epoch 4846: train loss: 1.7142248907475732e-05\n",
      "Epoch 4847: train loss: 1.8414009900880046e-05\n",
      "Epoch 4848: train loss: 1.998386505874805e-05\n",
      "Epoch 4849: train loss: 2.195936940552201e-05\n",
      "Epoch 4850: train loss: 2.434635280224029e-05\n",
      "Epoch 4851: train loss: 2.7321573725203052e-05\n",
      "Epoch 4852: train loss: 3.06868132611271e-05\n",
      "Epoch 4853: train loss: 3.476702477200888e-05\n",
      "Epoch 4854: train loss: 3.908656071871519e-05\n",
      "Epoch 4855: train loss: 4.428542160894722e-05\n",
      "Epoch 4856: train loss: 4.933449235977605e-05\n",
      "Epoch 4857: train loss: 5.505201988853514e-05\n",
      "Epoch 4858: train loss: 5.954746302450076e-05\n",
      "Epoch 4859: train loss: 6.366768502630293e-05\n",
      "Epoch 4860: train loss: 6.506596400868148e-05\n",
      "Epoch 4861: train loss: 6.501121242763475e-05\n",
      "Epoch 4862: train loss: 6.176998431328684e-05\n",
      "Epoch 4863: train loss: 5.7269426179118454e-05\n",
      "Epoch 4864: train loss: 5.070313636679202e-05\n",
      "Epoch 4865: train loss: 4.409465691423975e-05\n",
      "Epoch 4866: train loss: 3.7297930248314515e-05\n",
      "Epoch 4867: train loss: 3.156394450343214e-05\n",
      "Epoch 4868: train loss: 2.6679605070967227e-05\n",
      "Epoch 4869: train loss: 2.2950400307308882e-05\n",
      "Epoch 4870: train loss: 2.0111934645683505e-05\n",
      "Epoch 4871: train loss: 1.8108132280758582e-05\n",
      "Epoch 4872: train loss: 1.673248152656015e-05\n",
      "Epoch 4873: train loss: 1.5841333151911385e-05\n",
      "Epoch 4874: train loss: 1.52852080645971e-05\n",
      "Epoch 4875: train loss: 1.4961380657041445e-05\n",
      "Epoch 4876: train loss: 1.479433831264032e-05\n",
      "Epoch 4877: train loss: 1.4735833246959373e-05\n",
      "Epoch 4878: train loss: 1.4755288248124998e-05\n",
      "Epoch 4879: train loss: 1.4836574337095954e-05\n",
      "Epoch 4880: train loss: 1.4974383702792693e-05\n",
      "Epoch 4881: train loss: 1.5172102393989917e-05\n",
      "Epoch 4882: train loss: 1.5446277757291682e-05\n",
      "Epoch 4883: train loss: 1.582483855600003e-05\n",
      "Epoch 4884: train loss: 1.6365051124012098e-05\n",
      "Epoch 4885: train loss: 1.7139393094112165e-05\n",
      "Epoch 4886: train loss: 1.8297518181498162e-05\n",
      "Epoch 4887: train loss: 2.0010846128570847e-05\n",
      "Epoch 4888: train loss: 2.2651107428828254e-05\n",
      "Epoch 4889: train loss: 2.6602896468830295e-05\n",
      "Epoch 4890: train loss: 3.276033021393232e-05\n",
      "Epoch 4891: train loss: 4.1792667616391554e-05\n",
      "Epoch 4892: train loss: 5.536234675673768e-05\n",
      "Epoch 4893: train loss: 7.330323569476604e-05\n",
      "Epoch 4894: train loss: 9.637510811444372e-05\n",
      "Epoch 4895: train loss: 0.00011813740275101736\n",
      "Epoch 4896: train loss: 0.000133111811010167\n",
      "Epoch 4897: train loss: 0.0001276308175874874\n",
      "Epoch 4898: train loss: 0.00010338261199649423\n",
      "Epoch 4899: train loss: 6.791952182538807e-05\n",
      "Epoch 4900: train loss: 3.9677230233792216e-05\n",
      "Epoch 4901: train loss: 2.2846366846351884e-05\n",
      "Epoch 4902: train loss: 1.57370504894061e-05\n",
      "Epoch 4903: train loss: 1.5592440831824206e-05\n",
      "Epoch 4904: train loss: 2.0630608560168184e-05\n",
      "Epoch 4905: train loss: 2.8753946025972255e-05\n",
      "Epoch 4906: train loss: 3.8368325476767495e-05\n",
      "Epoch 4907: train loss: 4.9099766329163685e-05\n",
      "Epoch 4908: train loss: 5.9678925026673824e-05\n",
      "Epoch 4909: train loss: 6.696696800645441e-05\n",
      "Epoch 4910: train loss: 6.789156032027677e-05\n",
      "Epoch 4911: train loss: 6.0888618463650346e-05\n",
      "Epoch 4912: train loss: 4.833473212784156e-05\n",
      "Epoch 4913: train loss: 3.551236295606941e-05\n",
      "Epoch 4914: train loss: 2.505267366359476e-05\n",
      "Epoch 4915: train loss: 1.835696275520604e-05\n",
      "Epoch 4916: train loss: 1.520817204436753e-05\n",
      "Epoch 4917: train loss: 1.5187412827799562e-05\n",
      "Epoch 4918: train loss: 1.7471435057814233e-05\n",
      "Epoch 4919: train loss: 2.1105493942741305e-05\n",
      "Epoch 4920: train loss: 2.5396409910172224e-05\n",
      "Epoch 4921: train loss: 2.949731788248755e-05\n",
      "Epoch 4922: train loss: 3.266707790317014e-05\n",
      "Epoch 4923: train loss: 3.4101416531484574e-05\n",
      "Epoch 4924: train loss: 3.408931297599338e-05\n",
      "Epoch 4925: train loss: 3.292818655609153e-05\n",
      "Epoch 4926: train loss: 3.107991142314859e-05\n",
      "Epoch 4927: train loss: 2.841739114955999e-05\n",
      "Epoch 4928: train loss: 2.539251909183804e-05\n",
      "Epoch 4929: train loss: 2.2254638679441996e-05\n",
      "Epoch 4930: train loss: 1.954840263351798e-05\n",
      "Epoch 4931: train loss: 1.7396117982571013e-05\n",
      "Epoch 4932: train loss: 1.588127634022385e-05\n",
      "Epoch 4933: train loss: 1.4950265722291078e-05\n",
      "Epoch 4934: train loss: 1.4515673683490604e-05\n",
      "Epoch 4935: train loss: 1.4472930160991382e-05\n",
      "Epoch 4936: train loss: 1.4723322237841785e-05\n",
      "Epoch 4937: train loss: 1.5180887203314342e-05\n",
      "Epoch 4938: train loss: 1.5810943295946345e-05\n",
      "Epoch 4939: train loss: 1.6613155821687542e-05\n",
      "Epoch 4940: train loss: 1.7590757124708034e-05\n",
      "Epoch 4941: train loss: 1.880372474261094e-05\n",
      "Epoch 4942: train loss: 2.032382872130256e-05\n",
      "Epoch 4943: train loss: 2.2334344976115972e-05\n",
      "Epoch 4944: train loss: 2.4907052647904493e-05\n",
      "Epoch 4945: train loss: 2.8284015570534393e-05\n",
      "Epoch 4946: train loss: 3.2391872082371265e-05\n",
      "Epoch 4947: train loss: 3.771443516598083e-05\n",
      "Epoch 4948: train loss: 4.4025313400197774e-05\n",
      "Epoch 4949: train loss: 5.204110129852779e-05\n",
      "Epoch 4950: train loss: 6.0342950746417046e-05\n",
      "Epoch 4951: train loss: 6.915398262208328e-05\n",
      "Epoch 4952: train loss: 7.540846854681149e-05\n",
      "Epoch 4953: train loss: 7.92899591033347e-05\n",
      "Epoch 4954: train loss: 7.764451584080234e-05\n",
      "Epoch 4955: train loss: 7.208348688436672e-05\n",
      "Epoch 4956: train loss: 6.195312744239345e-05\n",
      "Epoch 4957: train loss: 5.091833372716792e-05\n",
      "Epoch 4958: train loss: 3.992266647401266e-05\n",
      "Epoch 4959: train loss: 3.103866401943378e-05\n",
      "Epoch 4960: train loss: 2.4176524675567634e-05\n",
      "Epoch 4961: train loss: 1.94360673049232e-05\n",
      "Epoch 4962: train loss: 1.6422734915977344e-05\n",
      "Epoch 4963: train loss: 1.4812624613114167e-05\n",
      "Epoch 4964: train loss: 1.4255003407015465e-05\n",
      "Epoch 4965: train loss: 1.4457815268542618e-05\n",
      "Epoch 4966: train loss: 1.521519516245462e-05\n",
      "Epoch 4967: train loss: 1.6404872440034524e-05\n",
      "Epoch 4968: train loss: 1.796792639652267e-05\n",
      "Epoch 4969: train loss: 1.987663381441962e-05\n",
      "Epoch 4970: train loss: 2.2179307052283548e-05\n",
      "Epoch 4971: train loss: 2.485744516889099e-05\n",
      "Epoch 4972: train loss: 2.8026248401147313e-05\n",
      "Epoch 4973: train loss: 3.1453531846636906e-05\n",
      "Epoch 4974: train loss: 3.529261448420584e-05\n",
      "Epoch 4975: train loss: 3.900926822097972e-05\n",
      "Epoch 4976: train loss: 4.300992804928683e-05\n",
      "Epoch 4977: train loss: 4.642353451345116e-05\n",
      "Epoch 4978: train loss: 4.9770798796089366e-05\n",
      "Epoch 4979: train loss: 5.176305057830177e-05\n",
      "Epoch 4980: train loss: 5.302259887685068e-05\n",
      "Epoch 4981: train loss: 5.248884917818941e-05\n",
      "Epoch 4982: train loss: 5.110534038976766e-05\n",
      "Epoch 4983: train loss: 4.827072189073078e-05\n",
      "Epoch 4984: train loss: 4.492766674957238e-05\n",
      "Epoch 4985: train loss: 4.079833524883725e-05\n",
      "Epoch 4986: train loss: 3.66928506991826e-05\n",
      "Epoch 4987: train loss: 3.257896605646238e-05\n",
      "Epoch 4988: train loss: 2.895697980420664e-05\n",
      "Epoch 4989: train loss: 2.5767691113287583e-05\n",
      "Epoch 4990: train loss: 2.316834797966294e-05\n",
      "Epoch 4991: train loss: 2.1049187125754543e-05\n",
      "Epoch 4992: train loss: 1.9415838323766366e-05\n",
      "Epoch 4993: train loss: 1.816742224036716e-05\n",
      "Epoch 4994: train loss: 1.7259366359212436e-05\n",
      "Epoch 4995: train loss: 1.6607755242148414e-05\n",
      "Epoch 4996: train loss: 1.6165446140803397e-05\n",
      "Epoch 4997: train loss: 1.5877754776738584e-05\n",
      "Epoch 4998: train loss: 1.5722898751846515e-05\n",
      "Epoch 4999: train loss: 1.5676780094509013e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "epoch = 5000\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train_tensor)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    \n",
    "    # Zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after training 0.0017063660779967904\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X_val_tensor)\n",
    "after_train = criterion(y_pred.squeeze(), y_val_tensor)\n",
    "print('Test loss after training' , after_train.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a test using predefined values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.7, 3.3, 5.7, 2.5]), 'virginica')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input_indx = rd.randint(0, len(X)-1)\n",
    "\n",
    "(X[X_input_indx], iris.target_names[y[X_input_indx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('setosa', 0.09996476583182812),\n",
       " ('versicolor', 0.27641793712973595),\n",
       " ('virginica', 99.62361454963684))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "output = (model(torch.from_numpy(X[X_input_indx]).float()) ** 2).sqrt()\n",
    "\n",
    "((iris.target_names[0], (output / output.sum())[0].item() * 100),\n",
    " (iris.target_names[1], (output / output.sum())[1].item() * 100),\n",
    " (iris.target_names[2], (output / output.sum())[2].item() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
