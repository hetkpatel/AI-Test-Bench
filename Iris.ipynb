{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "y_val_tensor = torch.from_numpy(y_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_tensor.shape[0], X_val_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP architecture\n",
    "class Iris_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Iris_MLP, self).__init__()\n",
    "        self.ly1 = nn.Linear(4, 10)\n",
    "        self.ly2 = nn.Linear(10, 10)\n",
    "        self.final = nn.Linear(10, 1)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ly1(x)\n",
    "        out = self.sig(out)\n",
    "        out = self.ly2(out)\n",
    "        out = self.sig(out)\n",
    "        out = self.final(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MLP model\n",
    "model = Iris_MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 1.58488929271698\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X_val_tensor)\n",
    "before_train = criterion(y_pred.squeeze(), y_val_tensor)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 1.735119104385376\n",
      "Epoch 1: train loss: 1.2892640829086304\n",
      "Epoch 2: train loss: 0.9648243188858032\n",
      "Epoch 3: train loss: 0.7640462517738342\n",
      "Epoch 4: train loss: 0.6804783940315247\n",
      "Epoch 5: train loss: 0.6898640990257263\n",
      "Epoch 6: train loss: 0.7447771430015564\n",
      "Epoch 7: train loss: 0.7912411093711853\n",
      "Epoch 8: train loss: 0.7982454895973206\n",
      "Epoch 9: train loss: 0.7644335627555847\n",
      "Epoch 10: train loss: 0.7050159573554993\n",
      "Epoch 11: train loss: 0.6392527222633362\n",
      "Epoch 12: train loss: 0.583544135093689\n",
      "Epoch 13: train loss: 0.5477458834648132\n",
      "Epoch 14: train loss: 0.5328104496002197\n",
      "Epoch 15: train loss: 0.530807375907898\n",
      "Epoch 16: train loss: 0.5295795798301697\n",
      "Epoch 17: train loss: 0.519149661064148\n",
      "Epoch 18: train loss: 0.49507445096969604\n",
      "Epoch 19: train loss: 0.4596394896507263\n",
      "Epoch 20: train loss: 0.41931605339050293\n",
      "Epoch 21: train loss: 0.38069015741348267\n",
      "Epoch 22: train loss: 0.3472769856452942\n",
      "Epoch 23: train loss: 0.3186226189136505\n",
      "Epoch 24: train loss: 0.2916875183582306\n",
      "Epoch 25: train loss: 0.2632227838039398\n",
      "Epoch 26: train loss: 0.23159880936145782\n",
      "Epoch 27: train loss: 0.19761055707931519\n",
      "Epoch 28: train loss: 0.16439223289489746\n",
      "Epoch 29: train loss: 0.13654601573944092\n",
      "Epoch 30: train loss: 0.11738109588623047\n",
      "Epoch 31: train loss: 0.10397963970899582\n",
      "Epoch 32: train loss: 0.08982686698436737\n",
      "Epoch 33: train loss: 0.0736234188079834\n",
      "Epoch 34: train loss: 0.06098685786128044\n",
      "Epoch 35: train loss: 0.05550912022590637\n",
      "Epoch 36: train loss: 0.053431134670972824\n",
      "Epoch 37: train loss: 0.04928036034107208\n",
      "Epoch 38: train loss: 0.0435524508357048\n",
      "Epoch 39: train loss: 0.041906606405973434\n",
      "Epoch 40: train loss: 0.04441295191645622\n",
      "Epoch 41: train loss: 0.04313395544886589\n",
      "Epoch 42: train loss: 0.039261020720005035\n",
      "Epoch 43: train loss: 0.039355840533971786\n",
      "Epoch 44: train loss: 0.04046628624200821\n",
      "Epoch 45: train loss: 0.03799520432949066\n",
      "Epoch 46: train loss: 0.035630326718091965\n",
      "Epoch 47: train loss: 0.03646610304713249\n",
      "Epoch 48: train loss: 0.035911817103624344\n",
      "Epoch 49: train loss: 0.033344365656375885\n",
      "Epoch 50: train loss: 0.03315286338329315\n",
      "Epoch 51: train loss: 0.03390787914395332\n",
      "Epoch 52: train loss: 0.032852016389369965\n",
      "Epoch 53: train loss: 0.03242548927664757\n",
      "Epoch 54: train loss: 0.03362203389406204\n",
      "Epoch 55: train loss: 0.03369229659438133\n",
      "Epoch 56: train loss: 0.03317059949040413\n",
      "Epoch 57: train loss: 0.03388803452253342\n",
      "Epoch 58: train loss: 0.03430180251598358\n",
      "Epoch 59: train loss: 0.03366381675004959\n",
      "Epoch 60: train loss: 0.03358587622642517\n",
      "Epoch 61: train loss: 0.03381069377064705\n",
      "Epoch 62: train loss: 0.03319308161735535\n",
      "Epoch 63: train loss: 0.03265949338674545\n",
      "Epoch 64: train loss: 0.03267302364110947\n",
      "Epoch 65: train loss: 0.03228812664747238\n",
      "Epoch 66: train loss: 0.031720854341983795\n",
      "Epoch 67: train loss: 0.03166661411523819\n",
      "Epoch 68: train loss: 0.0315825417637825\n",
      "Epoch 69: train loss: 0.031224265694618225\n",
      "Epoch 70: train loss: 0.031168704852461815\n",
      "Epoch 71: train loss: 0.03125891461968422\n",
      "Epoch 72: train loss: 0.031092766672372818\n",
      "Epoch 73: train loss: 0.030991243198513985\n",
      "Epoch 74: train loss: 0.031075427308678627\n",
      "Epoch 75: train loss: 0.03099752403795719\n",
      "Epoch 76: train loss: 0.030848979949951172\n",
      "Epoch 77: train loss: 0.03085305541753769\n",
      "Epoch 78: train loss: 0.030807318165898323\n",
      "Epoch 79: train loss: 0.030650891363620758\n",
      "Epoch 80: train loss: 0.03059173934161663\n",
      "Epoch 81: train loss: 0.03057253733277321\n",
      "Epoch 82: train loss: 0.030464941635727882\n",
      "Epoch 83: train loss: 0.030397316440939903\n",
      "Epoch 84: train loss: 0.030400244519114494\n",
      "Epoch 85: train loss: 0.030350495129823685\n",
      "Epoch 86: train loss: 0.03029346466064453\n",
      "Epoch 87: train loss: 0.030298249796032906\n",
      "Epoch 88: train loss: 0.030279085040092468\n",
      "Epoch 89: train loss: 0.030227109789848328\n",
      "Epoch 90: train loss: 0.03021402284502983\n",
      "Epoch 91: train loss: 0.030198050662875175\n",
      "Epoch 92: train loss: 0.03014681115746498\n",
      "Epoch 93: train loss: 0.030116857960820198\n",
      "Epoch 94: train loss: 0.03009883314371109\n",
      "Epoch 95: train loss: 0.030054841190576553\n",
      "Epoch 96: train loss: 0.030021851882338524\n",
      "Epoch 97: train loss: 0.03000757098197937\n",
      "Epoch 98: train loss: 0.029978223145008087\n",
      "Epoch 99: train loss: 0.029953034594655037\n",
      "Epoch 100: train loss: 0.02994559332728386\n",
      "Epoch 101: train loss: 0.02992876246571541\n",
      "Epoch 102: train loss: 0.029910458251833916\n",
      "Epoch 103: train loss: 0.029905205592513084\n",
      "Epoch 104: train loss: 0.029893511906266212\n",
      "Epoch 105: train loss: 0.02987733855843544\n",
      "Epoch 106: train loss: 0.02987000159919262\n",
      "Epoch 107: train loss: 0.029858486726880074\n",
      "Epoch 108: train loss: 0.029842376708984375\n",
      "Epoch 109: train loss: 0.02983299270272255\n",
      "Epoch 110: train loss: 0.029821738600730896\n",
      "Epoch 111: train loss: 0.02980717644095421\n",
      "Epoch 112: train loss: 0.029798144474625587\n",
      "Epoch 113: train loss: 0.029788611456751823\n",
      "Epoch 114: train loss: 0.029776742681860924\n",
      "Epoch 115: train loss: 0.02976907044649124\n",
      "Epoch 116: train loss: 0.02976112626492977\n",
      "Epoch 117: train loss: 0.029751215130090714\n",
      "Epoch 118: train loss: 0.029744144529104233\n",
      "Epoch 119: train loss: 0.02973662130534649\n",
      "Epoch 120: train loss: 0.029727479442954063\n",
      "Epoch 121: train loss: 0.029720356687903404\n",
      "Epoch 122: train loss: 0.029712814837694168\n",
      "Epoch 123: train loss: 0.029704248532652855\n",
      "Epoch 124: train loss: 0.02969735488295555\n",
      "Epoch 125: train loss: 0.029690265655517578\n",
      "Epoch 126: train loss: 0.029682686552405357\n",
      "Epoch 127: train loss: 0.029676528647542\n",
      "Epoch 128: train loss: 0.029670221731066704\n",
      "Epoch 129: train loss: 0.029663696885108948\n",
      "Epoch 130: train loss: 0.02965821884572506\n",
      "Epoch 131: train loss: 0.02965247631072998\n",
      "Epoch 132: train loss: 0.029646608978509903\n",
      "Epoch 133: train loss: 0.029641440138220787\n",
      "Epoch 134: train loss: 0.029635921120643616\n",
      "Epoch 135: train loss: 0.0296303890645504\n",
      "Epoch 136: train loss: 0.02962535060942173\n",
      "Epoch 137: train loss: 0.02962002158164978\n",
      "Epoch 138: train loss: 0.029614819213747978\n",
      "Epoch 139: train loss: 0.029609987512230873\n",
      "Epoch 140: train loss: 0.02960497885942459\n",
      "Epoch 141: train loss: 0.02960018813610077\n",
      "Epoch 142: train loss: 0.02959563583135605\n",
      "Epoch 143: train loss: 0.029590964317321777\n",
      "Epoch 144: train loss: 0.02958652190864086\n",
      "Epoch 145: train loss: 0.0295821875333786\n",
      "Epoch 146: train loss: 0.02957776188850403\n",
      "Epoch 147: train loss: 0.029573552310466766\n",
      "Epoch 148: train loss: 0.0295693501830101\n",
      "Epoch 149: train loss: 0.029565127566456795\n",
      "Epoch 150: train loss: 0.02956107072532177\n",
      "Epoch 151: train loss: 0.029557012021541595\n",
      "Epoch 152: train loss: 0.029552984982728958\n",
      "Epoch 153: train loss: 0.029549092054367065\n",
      "Epoch 154: train loss: 0.029545193538069725\n",
      "Epoch 155: train loss: 0.0295413751155138\n",
      "Epoch 156: train loss: 0.029537631198763847\n",
      "Epoch 157: train loss: 0.029533883556723595\n",
      "Epoch 158: train loss: 0.02953021228313446\n",
      "Epoch 159: train loss: 0.029526570811867714\n",
      "Epoch 160: train loss: 0.029522934928536415\n",
      "Epoch 161: train loss: 0.029519373551011086\n",
      "Epoch 162: train loss: 0.02951582334935665\n",
      "Epoch 163: train loss: 0.02951228804886341\n",
      "Epoch 164: train loss: 0.029508816078305244\n",
      "Epoch 165: train loss: 0.029505355283617973\n",
      "Epoch 166: train loss: 0.029501931741833687\n",
      "Epoch 167: train loss: 0.029498538002371788\n",
      "Epoch 168: train loss: 0.029495153576135635\n",
      "Epoch 169: train loss: 0.029491819441318512\n",
      "Epoch 170: train loss: 0.02948850207030773\n",
      "Epoch 171: train loss: 0.029485201463103294\n",
      "Epoch 172: train loss: 0.029481923207640648\n",
      "Epoch 173: train loss: 0.029478667303919792\n",
      "Epoch 174: train loss: 0.029475416988134384\n",
      "Epoch 175: train loss: 0.02947220206260681\n",
      "Epoch 176: train loss: 0.029469003900885582\n",
      "Epoch 177: train loss: 0.02946581318974495\n",
      "Epoch 178: train loss: 0.029462644830346107\n",
      "Epoch 179: train loss: 0.029459483921527863\n",
      "Epoch 180: train loss: 0.02945634536445141\n",
      "Epoch 181: train loss: 0.02945321425795555\n",
      "Epoch 182: train loss: 0.029450081288814545\n",
      "Epoch 183: train loss: 0.029446981847286224\n",
      "Epoch 184: train loss: 0.029443887993693352\n",
      "Epoch 185: train loss: 0.02944079414010048\n",
      "Epoch 186: train loss: 0.02943771705031395\n",
      "Epoch 187: train loss: 0.02943463996052742\n",
      "Epoch 188: train loss: 0.029431570321321487\n",
      "Epoch 189: train loss: 0.02942850813269615\n",
      "Epoch 190: train loss: 0.02942546084523201\n",
      "Epoch 191: train loss: 0.029422419145703316\n",
      "Epoch 192: train loss: 0.029419371858239174\n",
      "Epoch 193: train loss: 0.029416335746645927\n",
      "Epoch 194: train loss: 0.029413310810923576\n",
      "Epoch 195: train loss: 0.029410267248749733\n",
      "Epoch 196: train loss: 0.029407240450382233\n",
      "Epoch 197: train loss: 0.02940421923995018\n",
      "Epoch 198: train loss: 0.02940118871629238\n",
      "Epoch 199: train loss: 0.029398171231150627\n",
      "Epoch 200: train loss: 0.029395144432783127\n",
      "Epoch 201: train loss: 0.029392125084996223\n",
      "Epoch 202: train loss: 0.02938910201191902\n",
      "Epoch 203: train loss: 0.029386082664132118\n",
      "Epoch 204: train loss: 0.02938305214047432\n",
      "Epoch 205: train loss: 0.02938002534210682\n",
      "Epoch 206: train loss: 0.029377000406384468\n",
      "Epoch 207: train loss: 0.02937396988272667\n",
      "Epoch 208: train loss: 0.02937093935906887\n",
      "Epoch 209: train loss: 0.029367899522185326\n",
      "Epoch 210: train loss: 0.029364854097366333\n",
      "Epoch 211: train loss: 0.02936181239783764\n",
      "Epoch 212: train loss: 0.029358772560954094\n",
      "Epoch 213: train loss: 0.029355715960264206\n",
      "Epoch 214: train loss: 0.02935265563428402\n",
      "Epoch 215: train loss: 0.029349595308303833\n",
      "Epoch 216: train loss: 0.0293465256690979\n",
      "Epoch 217: train loss: 0.02934344857931137\n",
      "Epoch 218: train loss: 0.02934037335216999\n",
      "Epoch 219: train loss: 0.029337286949157715\n",
      "Epoch 220: train loss: 0.029334191232919693\n",
      "Epoch 221: train loss: 0.029331089928746223\n",
      "Epoch 222: train loss: 0.029327983036637306\n",
      "Epoch 223: train loss: 0.029324866831302643\n",
      "Epoch 224: train loss: 0.029321754351258278\n",
      "Epoch 225: train loss: 0.029318612068891525\n",
      "Epoch 226: train loss: 0.029315482825040817\n",
      "Epoch 227: train loss: 0.029312334954738617\n",
      "Epoch 228: train loss: 0.02930917963385582\n",
      "Epoch 229: train loss: 0.029306011274456978\n",
      "Epoch 230: train loss: 0.029302841052412987\n",
      "Epoch 231: train loss: 0.029299654066562653\n",
      "Epoch 232: train loss: 0.02929646335542202\n",
      "Epoch 233: train loss: 0.029293250292539597\n",
      "Epoch 234: train loss: 0.02929004654288292\n",
      "Epoch 235: train loss: 0.02928682044148445\n",
      "Epoch 236: train loss: 0.029283594340085983\n",
      "Epoch 237: train loss: 0.029280338436365128\n",
      "Epoch 238: train loss: 0.029277093708515167\n",
      "Epoch 239: train loss: 0.02927381731569767\n",
      "Epoch 240: train loss: 0.029270542785525322\n",
      "Epoch 241: train loss: 0.029267244040966034\n",
      "Epoch 242: train loss: 0.029263941571116447\n",
      "Epoch 243: train loss: 0.029260627925395966\n",
      "Epoch 244: train loss: 0.02925730310380459\n",
      "Epoch 245: train loss: 0.029253950342535973\n",
      "Epoch 246: train loss: 0.029250606894493103\n",
      "Epoch 247: train loss: 0.029247237369418144\n",
      "Epoch 248: train loss: 0.029243871569633484\n",
      "Epoch 249: train loss: 0.029240474104881287\n",
      "Epoch 250: train loss: 0.029237065464258194\n",
      "Epoch 251: train loss: 0.029233647510409355\n",
      "Epoch 252: train loss: 0.029230214655399323\n",
      "Epoch 253: train loss: 0.029226763173937798\n",
      "Epoch 254: train loss: 0.029223304241895676\n",
      "Epoch 255: train loss: 0.029219821095466614\n",
      "Epoch 256: train loss: 0.0292163398116827\n",
      "Epoch 257: train loss: 0.029212836176156998\n",
      "Epoch 258: train loss: 0.02920931577682495\n",
      "Epoch 259: train loss: 0.029205774888396263\n",
      "Epoch 260: train loss: 0.02920222096145153\n",
      "Epoch 261: train loss: 0.029198652133345604\n",
      "Epoch 262: train loss: 0.02919507399201393\n",
      "Epoch 263: train loss: 0.02919146791100502\n",
      "Epoch 264: train loss: 0.02918786182999611\n",
      "Epoch 265: train loss: 0.029184218496084213\n",
      "Epoch 266: train loss: 0.029180575162172318\n",
      "Epoch 267: train loss: 0.029176902025938034\n",
      "Epoch 268: train loss: 0.029173225164413452\n",
      "Epoch 269: train loss: 0.02916952408850193\n",
      "Epoch 270: train loss: 0.029165808111429214\n",
      "Epoch 271: train loss: 0.029162079095840454\n",
      "Epoch 272: train loss: 0.029158318415284157\n",
      "Epoch 273: train loss: 0.029154544696211815\n",
      "Epoch 274: train loss: 0.02915075607597828\n",
      "Epoch 275: train loss: 0.0291469506919384\n",
      "Epoch 276: train loss: 0.029143115505576134\n",
      "Epoch 277: train loss: 0.02913927473127842\n",
      "Epoch 278: train loss: 0.029135404154658318\n",
      "Epoch 279: train loss: 0.02913152240216732\n",
      "Epoch 280: train loss: 0.02912762388586998\n",
      "Epoch 281: train loss: 0.029123693704605103\n",
      "Epoch 282: train loss: 0.029119759798049927\n",
      "Epoch 283: train loss: 0.029115786775946617\n",
      "Epoch 284: train loss: 0.029111815616488457\n",
      "Epoch 285: train loss: 0.029107799753546715\n",
      "Epoch 286: train loss: 0.029103770852088928\n",
      "Epoch 287: train loss: 0.0290997251868248\n",
      "Epoch 288: train loss: 0.029095660895109177\n",
      "Epoch 289: train loss: 0.029091568663716316\n",
      "Epoch 290: train loss: 0.029087452217936516\n",
      "Epoch 291: train loss: 0.02908332087099552\n",
      "Epoch 292: train loss: 0.02907915972173214\n",
      "Epoch 293: train loss: 0.029074979946017265\n",
      "Epoch 294: train loss: 0.029070787131786346\n",
      "Epoch 295: train loss: 0.029066553339362144\n",
      "Epoch 296: train loss: 0.029062306508421898\n",
      "Epoch 297: train loss: 0.029058029875159264\n",
      "Epoch 298: train loss: 0.029053734615445137\n",
      "Epoch 299: train loss: 0.029049424454569817\n",
      "Epoch 300: train loss: 0.029045073315501213\n",
      "Epoch 301: train loss: 0.029040711000561714\n",
      "Epoch 302: train loss: 0.02903631702065468\n",
      "Epoch 303: train loss: 0.029031900689005852\n",
      "Epoch 304: train loss: 0.02902744896709919\n",
      "Epoch 305: train loss: 0.029022986069321632\n",
      "Epoch 306: train loss: 0.02901848591864109\n",
      "Epoch 307: train loss: 0.029013965278863907\n",
      "Epoch 308: train loss: 0.029009420424699783\n",
      "Epoch 309: train loss: 0.029004838317632675\n",
      "Epoch 310: train loss: 0.029000243172049522\n",
      "Epoch 311: train loss: 0.028995612636208534\n",
      "Epoch 312: train loss: 0.028990957885980606\n",
      "Epoch 313: train loss: 0.028986267745494843\n",
      "Epoch 314: train loss: 0.02898155339062214\n",
      "Epoch 315: train loss: 0.028976818546652794\n",
      "Epoch 316: train loss: 0.02897205390036106\n",
      "Epoch 317: train loss: 0.028967246413230896\n",
      "Epoch 318: train loss: 0.02896241843700409\n",
      "Epoch 319: train loss: 0.028957560658454895\n",
      "Epoch 320: train loss: 0.028952673077583313\n",
      "Epoch 321: train loss: 0.02894776128232479\n",
      "Epoch 322: train loss: 0.028942804783582687\n",
      "Epoch 323: train loss: 0.02893782965838909\n",
      "Epoch 324: train loss: 0.028932815417647362\n",
      "Epoch 325: train loss: 0.028927771374583244\n",
      "Epoch 326: train loss: 0.028922706842422485\n",
      "Epoch 327: train loss: 0.028917599469423294\n",
      "Epoch 328: train loss: 0.028912456706166267\n",
      "Epoch 329: train loss: 0.028907282277941704\n",
      "Epoch 330: train loss: 0.028902078047394753\n",
      "Epoch 331: train loss: 0.028896847739815712\n",
      "Epoch 332: train loss: 0.02889157272875309\n",
      "Epoch 333: train loss: 0.028886273503303528\n",
      "Epoch 334: train loss: 0.028880931437015533\n",
      "Epoch 335: train loss: 0.028875553980469704\n",
      "Epoch 336: train loss: 0.028870142996311188\n",
      "Epoch 337: train loss: 0.028864700347185135\n",
      "Epoch 338: train loss: 0.028859220445156097\n",
      "Epoch 339: train loss: 0.02885371632874012\n",
      "Epoch 340: train loss: 0.028848154470324516\n",
      "Epoch 341: train loss: 0.028842568397521973\n",
      "Epoch 342: train loss: 0.028836935758590698\n",
      "Epoch 343: train loss: 0.028831280767917633\n",
      "Epoch 344: train loss: 0.02882556989789009\n",
      "Epoch 345: train loss: 0.028819840401411057\n",
      "Epoch 346: train loss: 0.028814055025577545\n",
      "Epoch 347: train loss: 0.028808243572711945\n",
      "Epoch 348: train loss: 0.028802387416362762\n",
      "Epoch 349: train loss: 0.028796497732400894\n",
      "Epoch 350: train loss: 0.028790559619665146\n",
      "Epoch 351: train loss: 0.028784573078155518\n",
      "Epoch 352: train loss: 0.028778569772839546\n",
      "Epoch 353: train loss: 0.0287725068628788\n",
      "Epoch 354: train loss: 0.028766419738531113\n",
      "Epoch 355: train loss: 0.02876027673482895\n",
      "Epoch 356: train loss: 0.02875409461557865\n",
      "Epoch 357: train loss: 0.02874787710607052\n",
      "Epoch 358: train loss: 0.02874159999191761\n",
      "Epoch 359: train loss: 0.028735296800732613\n",
      "Epoch 360: train loss: 0.028728945180773735\n",
      "Epoch 361: train loss: 0.028722546994686127\n",
      "Epoch 362: train loss: 0.028716109693050385\n",
      "Epoch 363: train loss: 0.028709623962640762\n",
      "Epoch 364: train loss: 0.02870309352874756\n",
      "Epoch 365: train loss: 0.028696522116661072\n",
      "Epoch 366: train loss: 0.028689898550510406\n",
      "Epoch 367: train loss: 0.028683243319392204\n",
      "Epoch 368: train loss: 0.028676526620984077\n",
      "Epoch 369: train loss: 0.028669768944382668\n",
      "Epoch 370: train loss: 0.028662962839007378\n",
      "Epoch 371: train loss: 0.028656115755438805\n",
      "Epoch 372: train loss: 0.028649212792515755\n",
      "Epoch 373: train loss: 0.02864227257668972\n",
      "Epoch 374: train loss: 0.02863527461886406\n",
      "Epoch 375: train loss: 0.02862822823226452\n",
      "Epoch 376: train loss: 0.0286211259663105\n",
      "Epoch 377: train loss: 0.0286139864474535\n",
      "Epoch 378: train loss: 0.028606805950403214\n",
      "Epoch 379: train loss: 0.028599556535482407\n",
      "Epoch 380: train loss: 0.02859225869178772\n",
      "Epoch 381: train loss: 0.028584910556674004\n",
      "Epoch 382: train loss: 0.028577515855431557\n",
      "Epoch 383: train loss: 0.02857006900012493\n",
      "Epoch 384: train loss: 0.028562569990754128\n",
      "Epoch 385: train loss: 0.028555026277899742\n",
      "Epoch 386: train loss: 0.028547413647174835\n",
      "Epoch 387: train loss: 0.028539758175611496\n",
      "Epoch 388: train loss: 0.02853204868733883\n",
      "Epoch 389: train loss: 0.028524285182356834\n",
      "Epoch 390: train loss: 0.02851646952331066\n",
      "Epoch 391: train loss: 0.028508592396974564\n",
      "Epoch 392: train loss: 0.028500664979219437\n",
      "Epoch 393: train loss: 0.028492683544754982\n",
      "Epoch 394: train loss: 0.028484642505645752\n",
      "Epoch 395: train loss: 0.028476549312472343\n",
      "Epoch 396: train loss: 0.02846839837729931\n",
      "Epoch 397: train loss: 0.0284601841121912\n",
      "Epoch 398: train loss: 0.02845192514359951\n",
      "Epoch 399: train loss: 0.028443612158298492\n",
      "Epoch 400: train loss: 0.02843523770570755\n",
      "Epoch 401: train loss: 0.028426792472600937\n",
      "Epoch 402: train loss: 0.028418291360139847\n",
      "Epoch 403: train loss: 0.028409739956259727\n",
      "Epoch 404: train loss: 0.02840113826096058\n",
      "Epoch 405: train loss: 0.028392460197210312\n",
      "Epoch 406: train loss: 0.028383733704686165\n",
      "Epoch 407: train loss: 0.028374936431646347\n",
      "Epoch 408: train loss: 0.0283660888671875\n",
      "Epoch 409: train loss: 0.02835717238485813\n",
      "Epoch 410: train loss: 0.028348198160529137\n",
      "Epoch 411: train loss: 0.028339164331555367\n",
      "Epoch 412: train loss: 0.028330065310001373\n",
      "Epoch 413: train loss: 0.028320908546447754\n",
      "Epoch 414: train loss: 0.02831169217824936\n",
      "Epoch 415: train loss: 0.028302408754825592\n",
      "Epoch 416: train loss: 0.028293054550886154\n",
      "Epoch 417: train loss: 0.028283659368753433\n",
      "Epoch 418: train loss: 0.0282741766422987\n",
      "Epoch 419: train loss: 0.028264641761779785\n",
      "Epoch 420: train loss: 0.0282550398260355\n",
      "Epoch 421: train loss: 0.02824537456035614\n",
      "Epoch 422: train loss: 0.028235645964741707\n",
      "Epoch 423: train loss: 0.028225848451256752\n",
      "Epoch 424: train loss: 0.028215987607836723\n",
      "Epoch 425: train loss: 0.02820606343448162\n",
      "Epoch 426: train loss: 0.028196066617965698\n",
      "Epoch 427: train loss: 0.028186006471514702\n",
      "Epoch 428: train loss: 0.028175871819257736\n",
      "Epoch 429: train loss: 0.02816568687558174\n",
      "Epoch 430: train loss: 0.028155429288744926\n",
      "Epoch 431: train loss: 0.028145097196102142\n",
      "Epoch 432: train loss: 0.028134694322943687\n",
      "Epoch 433: train loss: 0.028124241158366203\n",
      "Epoch 434: train loss: 0.028113707900047302\n",
      "Epoch 435: train loss: 0.02810310199856758\n",
      "Epoch 436: train loss: 0.028092434629797935\n",
      "Epoch 437: train loss: 0.02808169275522232\n",
      "Epoch 438: train loss: 0.028070885688066483\n",
      "Epoch 439: train loss: 0.028060004115104675\n",
      "Epoch 440: train loss: 0.028049051761627197\n",
      "Epoch 441: train loss: 0.028038034215569496\n",
      "Epoch 442: train loss: 0.028026942163705826\n",
      "Epoch 443: train loss: 0.028015779331326485\n",
      "Epoch 444: train loss: 0.028004536405205727\n",
      "Epoch 445: train loss: 0.027993230149149895\n",
      "Epoch 446: train loss: 0.02798185870051384\n",
      "Epoch 447: train loss: 0.02797040343284607\n",
      "Epoch 448: train loss: 0.027958886697888374\n",
      "Epoch 449: train loss: 0.027947280555963516\n",
      "Epoch 450: train loss: 0.02793562225997448\n",
      "Epoch 451: train loss: 0.02792387641966343\n",
      "Epoch 452: train loss: 0.02791205421090126\n",
      "Epoch 453: train loss: 0.02790016680955887\n",
      "Epoch 454: train loss: 0.027888206765055656\n",
      "Epoch 455: train loss: 0.02787616290152073\n",
      "Epoch 456: train loss: 0.027864042669534683\n",
      "Epoch 457: train loss: 0.027851855382323265\n",
      "Epoch 458: train loss: 0.02783958613872528\n",
      "Epoch 459: train loss: 0.027827246114611626\n",
      "Epoch 460: train loss: 0.027814827859401703\n",
      "Epoch 461: train loss: 0.027802329510450363\n",
      "Epoch 462: train loss: 0.027789760380983353\n",
      "Epoch 463: train loss: 0.027777113020420074\n",
      "Epoch 464: train loss: 0.027764393016695976\n",
      "Epoch 465: train loss: 0.027751585468649864\n",
      "Epoch 466: train loss: 0.027738705277442932\n",
      "Epoch 467: train loss: 0.027725746855139732\n",
      "Epoch 468: train loss: 0.027712702751159668\n",
      "Epoch 469: train loss: 0.027699587866663933\n",
      "Epoch 470: train loss: 0.02768639102578163\n",
      "Epoch 471: train loss: 0.02767312154173851\n",
      "Epoch 472: train loss: 0.027659760788083076\n",
      "Epoch 473: train loss: 0.027646319940686226\n",
      "Epoch 474: train loss: 0.027632802724838257\n",
      "Epoch 475: train loss: 0.02761921100318432\n",
      "Epoch 476: train loss: 0.02760552614927292\n",
      "Epoch 477: train loss: 0.0275917649269104\n",
      "Epoch 478: train loss: 0.02757791429758072\n",
      "Epoch 479: train loss: 0.027564002200961113\n",
      "Epoch 480: train loss: 0.0275499876588583\n",
      "Epoch 481: train loss: 0.027535894885659218\n",
      "Epoch 482: train loss: 0.02752171829342842\n",
      "Epoch 483: train loss: 0.027507469058036804\n",
      "Epoch 484: train loss: 0.02749311923980713\n",
      "Epoch 485: train loss: 0.027478687465190887\n",
      "Epoch 486: train loss: 0.02746417000889778\n",
      "Epoch 487: train loss: 0.02744957245886326\n",
      "Epoch 488: train loss: 0.02743489481508732\n",
      "Epoch 489: train loss: 0.02742011658847332\n",
      "Epoch 490: train loss: 0.027405258268117905\n",
      "Epoch 491: train loss: 0.02739032171666622\n",
      "Epoch 492: train loss: 0.027375275269150734\n",
      "Epoch 493: train loss: 0.027360159903764725\n",
      "Epoch 494: train loss: 0.027344955131411552\n",
      "Epoch 495: train loss: 0.02732965163886547\n",
      "Epoch 496: train loss: 0.027314268052577972\n",
      "Epoch 497: train loss: 0.027298782020807266\n",
      "Epoch 498: train loss: 0.027283214032649994\n",
      "Epoch 499: train loss: 0.02726755291223526\n",
      "Epoch 500: train loss: 0.027251798659563065\n",
      "Epoch 501: train loss: 0.027235951274633408\n",
      "Epoch 502: train loss: 0.02722000703215599\n",
      "Epoch 503: train loss: 0.027203986421227455\n",
      "Epoch 504: train loss: 0.027187863364815712\n",
      "Epoch 505: train loss: 0.027171632274985313\n",
      "Epoch 506: train loss: 0.0271553136408329\n",
      "Epoch 507: train loss: 0.027138907462358475\n",
      "Epoch 508: train loss: 0.02712239883840084\n",
      "Epoch 509: train loss: 0.027105797082185745\n",
      "Epoch 510: train loss: 0.027089083567261696\n",
      "Epoch 511: train loss: 0.027072295546531677\n",
      "Epoch 512: train loss: 0.02705538645386696\n",
      "Epoch 513: train loss: 0.027038387954235077\n",
      "Epoch 514: train loss: 0.027021287009119987\n",
      "Epoch 515: train loss: 0.027004089206457138\n",
      "Epoch 516: train loss: 0.026986781507730484\n",
      "Epoch 517: train loss: 0.026969371363520622\n",
      "Epoch 518: train loss: 0.026951871812343597\n",
      "Epoch 519: train loss: 0.02693425491452217\n",
      "Epoch 520: train loss: 0.026916543021798134\n",
      "Epoch 521: train loss: 0.026898711919784546\n",
      "Epoch 522: train loss: 0.026880783960223198\n",
      "Epoch 523: train loss: 0.02686273865401745\n",
      "Epoch 524: train loss: 0.026844605803489685\n",
      "Epoch 525: train loss: 0.026826344430446625\n",
      "Epoch 526: train loss: 0.02680796943604946\n",
      "Epoch 527: train loss: 0.026789503172039986\n",
      "Epoch 528: train loss: 0.026770910248160362\n",
      "Epoch 529: train loss: 0.02675222046673298\n",
      "Epoch 530: train loss: 0.026733404025435448\n",
      "Epoch 531: train loss: 0.026714473962783813\n",
      "Epoch 532: train loss: 0.026695435866713524\n",
      "Epoch 533: train loss: 0.02667626366019249\n",
      "Epoch 534: train loss: 0.026656990870833397\n",
      "Epoch 535: train loss: 0.0266376044601202\n",
      "Epoch 536: train loss: 0.02661808952689171\n",
      "Epoch 537: train loss: 0.026598453521728516\n",
      "Epoch 538: train loss: 0.02657870016992092\n",
      "Epoch 539: train loss: 0.026558827608823776\n",
      "Epoch 540: train loss: 0.02653883397579193\n",
      "Epoch 541: train loss: 0.026518702507019043\n",
      "Epoch 542: train loss: 0.026498455554246902\n",
      "Epoch 543: train loss: 0.02647808939218521\n",
      "Epoch 544: train loss: 0.02645757980644703\n",
      "Epoch 545: train loss: 0.026436951011419296\n",
      "Epoch 546: train loss: 0.026416191831231117\n",
      "Epoch 547: train loss: 0.02639530412852764\n",
      "Epoch 548: train loss: 0.02637428045272827\n",
      "Epoch 549: train loss: 0.026353124529123306\n",
      "Epoch 550: train loss: 0.026331838220357895\n",
      "Epoch 551: train loss: 0.02631041593849659\n",
      "Epoch 552: train loss: 0.026288853958249092\n",
      "Epoch 553: train loss: 0.02626715786755085\n",
      "Epoch 554: train loss: 0.026245325803756714\n",
      "Epoch 555: train loss: 0.026223350316286087\n",
      "Epoch 556: train loss: 0.026201235130429268\n",
      "Epoch 557: train loss: 0.026178976520895958\n",
      "Epoch 558: train loss: 0.026156572625041008\n",
      "Epoch 559: train loss: 0.026134027168154716\n",
      "Epoch 560: train loss: 0.026111342012882233\n",
      "Epoch 561: train loss: 0.02608850598335266\n",
      "Epoch 562: train loss: 0.0260655228048563\n",
      "Epoch 563: train loss: 0.026042386889457703\n",
      "Epoch 564: train loss: 0.02601909451186657\n",
      "Epoch 565: train loss: 0.025995658710598946\n",
      "Epoch 566: train loss: 0.025972072035074234\n",
      "Epoch 567: train loss: 0.025948330760002136\n",
      "Epoch 568: train loss: 0.025924427434802055\n",
      "Epoch 569: train loss: 0.025900373235344887\n",
      "Epoch 570: train loss: 0.025876160711050034\n",
      "Epoch 571: train loss: 0.02585178054869175\n",
      "Epoch 572: train loss: 0.025827255100011826\n",
      "Epoch 573: train loss: 0.02580256015062332\n",
      "Epoch 574: train loss: 0.025777703151106834\n",
      "Epoch 575: train loss: 0.02575267292559147\n",
      "Epoch 576: train loss: 0.025727491825819016\n",
      "Epoch 577: train loss: 0.025702130049467087\n",
      "Epoch 578: train loss: 0.025676609948277473\n",
      "Epoch 579: train loss: 0.02565092220902443\n",
      "Epoch 580: train loss: 0.025625048205256462\n",
      "Epoch 581: train loss: 0.025599021464586258\n",
      "Epoch 582: train loss: 0.02557281404733658\n",
      "Epoch 583: train loss: 0.02554643154144287\n",
      "Epoch 584: train loss: 0.025519879534840584\n",
      "Epoch 585: train loss: 0.02549314685165882\n",
      "Epoch 586: train loss: 0.025466233491897583\n",
      "Epoch 587: train loss: 0.02543913573026657\n",
      "Epoch 588: train loss: 0.02541186474263668\n",
      "Epoch 589: train loss: 0.025384418666362762\n",
      "Epoch 590: train loss: 0.025356777012348175\n",
      "Epoch 591: train loss: 0.02532896399497986\n",
      "Epoch 592: train loss: 0.025300955399870872\n",
      "Epoch 593: train loss: 0.025272754952311516\n",
      "Epoch 594: train loss: 0.025244377553462982\n",
      "Epoch 595: train loss: 0.025215813890099525\n",
      "Epoch 596: train loss: 0.025187036022543907\n",
      "Epoch 597: train loss: 0.025158092379570007\n",
      "Epoch 598: train loss: 0.02512894757091999\n",
      "Epoch 599: train loss: 0.0250996146351099\n",
      "Epoch 600: train loss: 0.0250700730830431\n",
      "Epoch 601: train loss: 0.025040345266461372\n",
      "Epoch 602: train loss: 0.025010431185364723\n",
      "Epoch 603: train loss: 0.024980297312140465\n",
      "Epoch 604: train loss: 0.024949969723820686\n",
      "Epoch 605: train loss: 0.024919450283050537\n",
      "Epoch 606: train loss: 0.024888718500733376\n",
      "Epoch 607: train loss: 0.0248577818274498\n",
      "Epoch 608: train loss: 0.024826649576425552\n",
      "Epoch 609: train loss: 0.024795319885015488\n",
      "Epoch 610: train loss: 0.024763774126768112\n",
      "Epoch 611: train loss: 0.024732021614909172\n",
      "Epoch 612: train loss: 0.02470005862414837\n",
      "Epoch 613: train loss: 0.024667887017130852\n",
      "Epoch 614: train loss: 0.02463550679385662\n",
      "Epoch 615: train loss: 0.024602914229035378\n",
      "Epoch 616: train loss: 0.024570122361183167\n",
      "Epoch 617: train loss: 0.0245371013879776\n",
      "Epoch 618: train loss: 0.02450387179851532\n",
      "Epoch 619: train loss: 0.024470429867506027\n",
      "Epoch 620: train loss: 0.024436771869659424\n",
      "Epoch 621: train loss: 0.02440289780497551\n",
      "Epoch 622: train loss: 0.02436879463493824\n",
      "Epoch 623: train loss: 0.024334492161870003\n",
      "Epoch 624: train loss: 0.02429996058344841\n",
      "Epoch 625: train loss: 0.024265212938189507\n",
      "Epoch 626: train loss: 0.024230239912867546\n",
      "Epoch 627: train loss: 0.024195043370127678\n",
      "Epoch 628: train loss: 0.024159632623195648\n",
      "Epoch 629: train loss: 0.024123992770910263\n",
      "Epoch 630: train loss: 0.024088140577077866\n",
      "Epoch 631: train loss: 0.024052057415246964\n",
      "Epoch 632: train loss: 0.024015750735998154\n",
      "Epoch 633: train loss: 0.023979218676686287\n",
      "Epoch 634: train loss: 0.023942459374666214\n",
      "Epoch 635: train loss: 0.023905474692583084\n",
      "Epoch 636: train loss: 0.023868268355727196\n",
      "Epoch 637: train loss: 0.0238308385014534\n",
      "Epoch 638: train loss: 0.023793170228600502\n",
      "Epoch 639: train loss: 0.023755282163619995\n",
      "Epoch 640: train loss: 0.023717161267995834\n",
      "Epoch 641: train loss: 0.023678818717598915\n",
      "Epoch 642: train loss: 0.023640241473913193\n",
      "Epoch 643: train loss: 0.023601438850164413\n",
      "Epoch 644: train loss: 0.02356240153312683\n",
      "Epoch 645: train loss: 0.023523148149251938\n",
      "Epoch 646: train loss: 0.02348366193473339\n",
      "Epoch 647: train loss: 0.023443937301635742\n",
      "Epoch 648: train loss: 0.023403992876410484\n",
      "Epoch 649: train loss: 0.023363810032606125\n",
      "Epoch 650: train loss: 0.023323416709899902\n",
      "Epoch 651: train loss: 0.023282775655388832\n",
      "Epoch 652: train loss: 0.023241911083459854\n",
      "Epoch 653: train loss: 0.023200806230306625\n",
      "Epoch 654: train loss: 0.023159483447670937\n",
      "Epoch 655: train loss: 0.023117950186133385\n",
      "Epoch 656: train loss: 0.023076163604855537\n",
      "Epoch 657: train loss: 0.023034166544675827\n",
      "Epoch 658: train loss: 0.022991936653852463\n",
      "Epoch 659: train loss: 0.022949479520320892\n",
      "Epoch 660: train loss: 0.022906798869371414\n",
      "Epoch 661: train loss: 0.02286389097571373\n",
      "Epoch 662: train loss: 0.022820759564638138\n",
      "Epoch 663: train loss: 0.022777395322918892\n",
      "Epoch 664: train loss: 0.022733816877007484\n",
      "Epoch 665: train loss: 0.022690018638968468\n",
      "Epoch 666: train loss: 0.022645996883511543\n",
      "Epoch 667: train loss: 0.02260175533592701\n",
      "Epoch 668: train loss: 0.02255728654563427\n",
      "Epoch 669: train loss: 0.022512607276439667\n",
      "Epoch 670: train loss: 0.022467713803052902\n",
      "Epoch 671: train loss: 0.02242260053753853\n",
      "Epoch 672: train loss: 0.022377271205186844\n",
      "Epoch 673: train loss: 0.022331727668642998\n",
      "Epoch 674: train loss: 0.022285981103777885\n",
      "Epoch 675: train loss: 0.022240016609430313\n",
      "Epoch 676: train loss: 0.022193852812051773\n",
      "Epoch 677: train loss: 0.022147467359900475\n",
      "Epoch 678: train loss: 0.022100888192653656\n",
      "Epoch 679: train loss: 0.022054102271795273\n",
      "Epoch 680: train loss: 0.022007107734680176\n",
      "Epoch 681: train loss: 0.021959926933050156\n",
      "Epoch 682: train loss: 0.02191253937780857\n",
      "Epoch 683: train loss: 0.021864958107471466\n",
      "Epoch 684: train loss: 0.021817192435264587\n",
      "Epoch 685: train loss: 0.021769220009446144\n",
      "Epoch 686: train loss: 0.021721065044403076\n",
      "Epoch 687: train loss: 0.021672729402780533\n",
      "Epoch 688: train loss: 0.02162422053515911\n",
      "Epoch 689: train loss: 0.021575504913926125\n",
      "Epoch 690: train loss: 0.021526629105210304\n",
      "Epoch 691: train loss: 0.021477576345205307\n",
      "Epoch 692: train loss: 0.021428342908620834\n",
      "Epoch 693: train loss: 0.021378949284553528\n",
      "Epoch 694: train loss: 0.021329382434487343\n",
      "Epoch 695: train loss: 0.021279659122228622\n",
      "Epoch 696: train loss: 0.021229775622487068\n",
      "Epoch 697: train loss: 0.02117972820997238\n",
      "Epoch 698: train loss: 0.021129537373781204\n",
      "Epoch 699: train loss: 0.021079203113913536\n",
      "Epoch 700: train loss: 0.021028712391853333\n",
      "Epoch 701: train loss: 0.020978078246116638\n",
      "Epoch 702: train loss: 0.020927298814058304\n",
      "Epoch 703: train loss: 0.02087639644742012\n",
      "Epoch 704: train loss: 0.02082536555826664\n",
      "Epoch 705: train loss: 0.020774206146597862\n",
      "Epoch 706: train loss: 0.020722927525639534\n",
      "Epoch 707: train loss: 0.02067152038216591\n",
      "Epoch 708: train loss: 0.02062000334262848\n",
      "Epoch 709: train loss: 0.020568376407027245\n",
      "Epoch 710: train loss: 0.020516645163297653\n",
      "Epoch 711: train loss: 0.020464807748794556\n",
      "Epoch 712: train loss: 0.02041287161409855\n",
      "Epoch 713: train loss: 0.020360849797725677\n",
      "Epoch 714: train loss: 0.020308732986450195\n",
      "Epoch 715: train loss: 0.020256534218788147\n",
      "Epoch 716: train loss: 0.02020425535738468\n",
      "Epoch 717: train loss: 0.02015189826488495\n",
      "Epoch 718: train loss: 0.020099466666579247\n",
      "Epoch 719: train loss: 0.020046966150403023\n",
      "Epoch 720: train loss: 0.019994398579001427\n",
      "Epoch 721: train loss: 0.0199417844414711\n",
      "Epoch 722: train loss: 0.019889099523425102\n",
      "Epoch 723: train loss: 0.019836386665701866\n",
      "Epoch 724: train loss: 0.01978360116481781\n",
      "Epoch 725: train loss: 0.019730785861611366\n",
      "Epoch 726: train loss: 0.019677922129631042\n",
      "Epoch 727: train loss: 0.019625024870038033\n",
      "Epoch 728: train loss: 0.01957210898399353\n",
      "Epoch 729: train loss: 0.019519155845046043\n",
      "Epoch 730: train loss: 0.019466184079647064\n",
      "Epoch 731: train loss: 0.019413189962506294\n",
      "Epoch 732: train loss: 0.019360167905688286\n",
      "Epoch 733: train loss: 0.01930714212357998\n",
      "Epoch 734: train loss: 0.019254114478826523\n",
      "Epoch 735: train loss: 0.019201070070266724\n",
      "Epoch 736: train loss: 0.019148025661706924\n",
      "Epoch 737: train loss: 0.019094983115792274\n",
      "Epoch 738: train loss: 0.019041946157813072\n",
      "Epoch 739: train loss: 0.018988924100995064\n",
      "Epoch 740: train loss: 0.01893589273095131\n",
      "Epoch 741: train loss: 0.01888289302587509\n",
      "Epoch 742: train loss: 0.01882990635931492\n",
      "Epoch 743: train loss: 0.018776947632431984\n",
      "Epoch 744: train loss: 0.018723992630839348\n",
      "Epoch 745: train loss: 0.018671080470085144\n",
      "Epoch 746: train loss: 0.018618188798427582\n",
      "Epoch 747: train loss: 0.018565328791737556\n",
      "Epoch 748: train loss: 0.018512507900595665\n",
      "Epoch 749: train loss: 0.01845971867442131\n",
      "Epoch 750: train loss: 0.018406977877020836\n",
      "Epoch 751: train loss: 0.018354268744587898\n",
      "Epoch 752: train loss: 0.01830161176621914\n",
      "Epoch 753: train loss: 0.01824900694191456\n",
      "Epoch 754: train loss: 0.01819644868373871\n",
      "Epoch 755: train loss: 0.018143940716981888\n",
      "Epoch 756: train loss: 0.018091490492224693\n",
      "Epoch 757: train loss: 0.01803910918533802\n",
      "Epoch 758: train loss: 0.017986778169870377\n",
      "Epoch 759: train loss: 0.017934519797563553\n",
      "Epoch 760: train loss: 0.017882322892546654\n",
      "Epoch 761: train loss: 0.017830202355980873\n",
      "Epoch 762: train loss: 0.017778145149350166\n",
      "Epoch 763: train loss: 0.01772615872323513\n",
      "Epoch 764: train loss: 0.01767425797879696\n",
      "Epoch 765: train loss: 0.017622435465455055\n",
      "Epoch 766: train loss: 0.01757069118320942\n",
      "Epoch 767: train loss: 0.017519034445285797\n",
      "Epoch 768: train loss: 0.01746746338903904\n",
      "Epoch 769: train loss: 0.017415983602404594\n",
      "Epoch 770: train loss: 0.017364591360092163\n",
      "Epoch 771: train loss: 0.017313307151198387\n",
      "Epoch 772: train loss: 0.01726210117340088\n",
      "Epoch 773: train loss: 0.017211006954312325\n",
      "Epoch 774: train loss: 0.017160002142190933\n",
      "Epoch 775: train loss: 0.017109118402004242\n",
      "Epoch 776: train loss: 0.017058327794075012\n",
      "Epoch 777: train loss: 0.017007648944854736\n",
      "Epoch 778: train loss: 0.016957085579633713\n",
      "Epoch 779: train loss: 0.016906635835766792\n",
      "Epoch 780: train loss: 0.016856299713253975\n",
      "Epoch 781: train loss: 0.016806088387966156\n",
      "Epoch 782: train loss: 0.01675598882138729\n",
      "Epoch 783: train loss: 0.01670600287616253\n",
      "Epoch 784: train loss: 0.016656162217259407\n",
      "Epoch 785: train loss: 0.016606448218226433\n",
      "Epoch 786: train loss: 0.01655685342848301\n",
      "Epoch 787: train loss: 0.01650739647448063\n",
      "Epoch 788: train loss: 0.01645808108150959\n",
      "Epoch 789: train loss: 0.01640889048576355\n",
      "Epoch 790: train loss: 0.016359839588403702\n",
      "Epoch 791: train loss: 0.016310928389430046\n",
      "Epoch 792: train loss: 0.016262168064713478\n",
      "Epoch 793: train loss: 0.016213541850447655\n",
      "Epoch 794: train loss: 0.01616506278514862\n",
      "Epoch 795: train loss: 0.01611674576997757\n",
      "Epoch 796: train loss: 0.01606856659054756\n",
      "Epoch 797: train loss: 0.016020553186535835\n",
      "Epoch 798: train loss: 0.015972688794136047\n",
      "Epoch 799: train loss: 0.015924973413348198\n",
      "Epoch 800: train loss: 0.015877416357398033\n",
      "Epoch 801: train loss: 0.015830030664801598\n",
      "Epoch 802: train loss: 0.0157827977091074\n",
      "Epoch 803: train loss: 0.015735730528831482\n",
      "Epoch 804: train loss: 0.0156888198107481\n",
      "Epoch 805: train loss: 0.015642087906599045\n",
      "Epoch 806: train loss: 0.015595520846545696\n",
      "Epoch 807: train loss: 0.01554911956191063\n",
      "Epoch 808: train loss: 0.015502885915338993\n",
      "Epoch 809: train loss: 0.01545682456344366\n",
      "Epoch 810: train loss: 0.015410945750772953\n",
      "Epoch 811: train loss: 0.015365229919552803\n",
      "Epoch 812: train loss: 0.015319702215492725\n",
      "Epoch 813: train loss: 0.015274344943463802\n",
      "Epoch 814: train loss: 0.015229170210659504\n",
      "Epoch 815: train loss: 0.015184169635176659\n",
      "Epoch 816: train loss: 0.015139359049499035\n",
      "Epoch 817: train loss: 0.015094714239239693\n",
      "Epoch 818: train loss: 0.015050259418785572\n",
      "Epoch 819: train loss: 0.015005984343588352\n",
      "Epoch 820: train loss: 0.014961899258196354\n",
      "Epoch 821: train loss: 0.014917990192770958\n",
      "Epoch 822: train loss: 0.014874274842441082\n",
      "Epoch 823: train loss: 0.014830744825303555\n",
      "Epoch 824: train loss: 0.0147874029353261\n",
      "Epoch 825: train loss: 0.014744255691766739\n",
      "Epoch 826: train loss: 0.014701290056109428\n",
      "Epoch 827: train loss: 0.01465850230306387\n",
      "Epoch 828: train loss: 0.014615904539823532\n",
      "Epoch 829: train loss: 0.014573507942259312\n",
      "Epoch 830: train loss: 0.01453130878508091\n",
      "Epoch 831: train loss: 0.014489288441836834\n",
      "Epoch 832: train loss: 0.014447466470301151\n",
      "Epoch 833: train loss: 0.014405818656086922\n",
      "Epoch 834: train loss: 0.014364383183419704\n",
      "Epoch 835: train loss: 0.014323134906589985\n",
      "Epoch 836: train loss: 0.014282078482210636\n",
      "Epoch 837: train loss: 0.014241217635571957\n",
      "Epoch 838: train loss: 0.01420054491609335\n",
      "Epoch 839: train loss: 0.014160067774355412\n",
      "Epoch 840: train loss: 0.014119788073003292\n",
      "Epoch 841: train loss: 0.014079696498811245\n",
      "Epoch 842: train loss: 0.014039792120456696\n",
      "Epoch 843: train loss: 0.014000100083649158\n",
      "Epoch 844: train loss: 0.013960587792098522\n",
      "Epoch 845: train loss: 0.013921276666224003\n",
      "Epoch 846: train loss: 0.013882146216928959\n",
      "Epoch 847: train loss: 0.013843226246535778\n",
      "Epoch 848: train loss: 0.013804484158754349\n",
      "Epoch 849: train loss: 0.013765951618552208\n",
      "Epoch 850: train loss: 0.013727599754929543\n",
      "Epoch 851: train loss: 0.013689445331692696\n",
      "Epoch 852: train loss: 0.01365148089826107\n",
      "Epoch 853: train loss: 0.013613720424473286\n",
      "Epoch 854: train loss: 0.013576135970652103\n",
      "Epoch 855: train loss: 0.01353875920176506\n",
      "Epoch 856: train loss: 0.013501563109457493\n",
      "Epoch 857: train loss: 0.013464570045471191\n",
      "Epoch 858: train loss: 0.01342774648219347\n",
      "Epoch 859: train loss: 0.013391133397817612\n",
      "Epoch 860: train loss: 0.013354703783988953\n",
      "Epoch 861: train loss: 0.013318470679223537\n",
      "Epoch 862: train loss: 0.013282415457069874\n",
      "Epoch 863: train loss: 0.013246565125882626\n",
      "Epoch 864: train loss: 0.013210893608629704\n",
      "Epoch 865: train loss: 0.013175418600440025\n",
      "Epoch 866: train loss: 0.013140125200152397\n",
      "Epoch 867: train loss: 0.013105019927024841\n",
      "Epoch 868: train loss: 0.013070102781057358\n",
      "Epoch 869: train loss: 0.013035371899604797\n",
      "Epoch 870: train loss: 0.01300082914531231\n",
      "Epoch 871: train loss: 0.012966462410986423\n",
      "Epoch 872: train loss: 0.01293228380382061\n",
      "Epoch 873: train loss: 0.012898293323814869\n",
      "Epoch 874: train loss: 0.012864486314356327\n",
      "Epoch 875: train loss: 0.01283086184412241\n",
      "Epoch 876: train loss: 0.012797418050467968\n",
      "Epoch 877: train loss: 0.012764161452651024\n",
      "Epoch 878: train loss: 0.012731075286865234\n",
      "Epoch 879: train loss: 0.012698185630142689\n",
      "Epoch 880: train loss: 0.0126654589548707\n",
      "Epoch 881: train loss: 0.012632915750145912\n",
      "Epoch 882: train loss: 0.0126005494967103\n",
      "Epoch 883: train loss: 0.012568376958370209\n",
      "Epoch 884: train loss: 0.012536364607512951\n",
      "Epoch 885: train loss: 0.012504535727202892\n",
      "Epoch 886: train loss: 0.012472876347601414\n",
      "Epoch 887: train loss: 0.012441391125321388\n",
      "Epoch 888: train loss: 0.012410084716975689\n",
      "Epoch 889: train loss: 0.012378944084048271\n",
      "Epoch 890: train loss: 0.012347983196377754\n",
      "Epoch 891: train loss: 0.01231719832867384\n",
      "Epoch 892: train loss: 0.012286570854485035\n",
      "Epoch 893: train loss: 0.012256127782166004\n",
      "Epoch 894: train loss: 0.012225841172039509\n",
      "Epoch 895: train loss: 0.012195718474686146\n",
      "Epoch 896: train loss: 0.012165774591267109\n",
      "Epoch 897: train loss: 0.012136000208556652\n",
      "Epoch 898: train loss: 0.012106378562748432\n",
      "Epoch 899: train loss: 0.01207693386822939\n",
      "Epoch 900: train loss: 0.012047640047967434\n",
      "Epoch 901: train loss: 0.012018507346510887\n",
      "Epoch 902: train loss: 0.01198955811560154\n",
      "Epoch 903: train loss: 0.011960752308368683\n",
      "Epoch 904: train loss: 0.011932109482586384\n",
      "Epoch 905: train loss: 0.011903627775609493\n",
      "Epoch 906: train loss: 0.011875308118760586\n",
      "Epoch 907: train loss: 0.011847129091620445\n",
      "Epoch 908: train loss: 0.01181913260370493\n",
      "Epoch 909: train loss: 0.011791272088885307\n",
      "Epoch 910: train loss: 0.011763567104935646\n",
      "Epoch 911: train loss: 0.011736028827726841\n",
      "Epoch 912: train loss: 0.011708631180226803\n",
      "Epoch 913: train loss: 0.01168138813227415\n",
      "Epoch 914: train loss: 0.011654295958578587\n",
      "Epoch 915: train loss: 0.01162735465914011\n",
      "Epoch 916: train loss: 0.011600563302636147\n",
      "Epoch 917: train loss: 0.01157392654567957\n",
      "Epoch 918: train loss: 0.011547421105206013\n",
      "Epoch 919: train loss: 0.01152107585221529\n",
      "Epoch 920: train loss: 0.011494866572320461\n",
      "Epoch 921: train loss: 0.011468803510069847\n",
      "Epoch 922: train loss: 0.011442882940173149\n",
      "Epoch 923: train loss: 0.011417102999985218\n",
      "Epoch 924: train loss: 0.011391475796699524\n",
      "Epoch 925: train loss: 0.011365980841219425\n",
      "Epoch 926: train loss: 0.011340615339577198\n",
      "Epoch 927: train loss: 0.011315407231450081\n",
      "Epoch 928: train loss: 0.011290322057902813\n",
      "Epoch 929: train loss: 0.011265371926128864\n",
      "Epoch 930: train loss: 0.011240567080676556\n",
      "Epoch 931: train loss: 0.011215891689062119\n",
      "Epoch 932: train loss: 0.011191345751285553\n",
      "Epoch 933: train loss: 0.011166932061314583\n",
      "Epoch 934: train loss: 0.011142664588987827\n",
      "Epoch 935: train loss: 0.011118514463305473\n",
      "Epoch 936: train loss: 0.01109449565410614\n",
      "Epoch 937: train loss: 0.011070608161389828\n",
      "Epoch 938: train loss: 0.01104684453457594\n",
      "Epoch 939: train loss: 0.011023198254406452\n",
      "Epoch 940: train loss: 0.010999700054526329\n",
      "Epoch 941: train loss: 0.010976308956742287\n",
      "Epoch 942: train loss: 0.01095305010676384\n",
      "Epoch 943: train loss: 0.010929912328720093\n",
      "Epoch 944: train loss: 0.010906892828643322\n",
      "Epoch 945: train loss: 0.010883987881243229\n",
      "Epoch 946: train loss: 0.010861217975616455\n",
      "Epoch 947: train loss: 0.01083855889737606\n",
      "Epoch 948: train loss: 0.010816008783876896\n",
      "Epoch 949: train loss: 0.010793586261570454\n",
      "Epoch 950: train loss: 0.010771273635327816\n",
      "Epoch 951: train loss: 0.010749083012342453\n",
      "Epoch 952: train loss: 0.010726997628808022\n",
      "Epoch 953: train loss: 0.010705037042498589\n",
      "Epoch 954: train loss: 0.010683185420930386\n",
      "Epoch 955: train loss: 0.010661440901458263\n",
      "Epoch 956: train loss: 0.010639810934662819\n",
      "Epoch 957: train loss: 0.010618286207318306\n",
      "Epoch 958: train loss: 0.010596868582069874\n",
      "Epoch 959: train loss: 0.010575559921562672\n",
      "Epoch 960: train loss: 0.010554356500506401\n",
      "Epoch 961: train loss: 0.010533267632126808\n",
      "Epoch 962: train loss: 0.010512275621294975\n",
      "Epoch 963: train loss: 0.0104913841933012\n",
      "Epoch 964: train loss: 0.01047059427946806\n",
      "Epoch 965: train loss: 0.010449925437569618\n",
      "Epoch 966: train loss: 0.010429334826767445\n",
      "Epoch 967: train loss: 0.010408855974674225\n",
      "Epoch 968: train loss: 0.01038848515599966\n",
      "Epoch 969: train loss: 0.01036820188164711\n",
      "Epoch 970: train loss: 0.01034801360219717\n",
      "Epoch 971: train loss: 0.010327927768230438\n",
      "Epoch 972: train loss: 0.010307942517101765\n",
      "Epoch 973: train loss: 0.01028803363442421\n",
      "Epoch 974: train loss: 0.010268240235745907\n",
      "Epoch 975: train loss: 0.010248537175357342\n",
      "Epoch 976: train loss: 0.010228929109871387\n",
      "Epoch 977: train loss: 0.010209402069449425\n",
      "Epoch 978: train loss: 0.010189973749220371\n",
      "Epoch 979: train loss: 0.010170637629926205\n",
      "Epoch 980: train loss: 0.010151374153792858\n",
      "Epoch 981: train loss: 0.010132218711078167\n",
      "Epoch 982: train loss: 0.010113144293427467\n",
      "Epoch 983: train loss: 0.01009415090084076\n",
      "Epoch 984: train loss: 0.01007525809109211\n",
      "Epoch 985: train loss: 0.010056452825665474\n",
      "Epoch 986: train loss: 0.010037717409431934\n",
      "Epoch 987: train loss: 0.010019068606197834\n",
      "Epoch 988: train loss: 0.01000051386654377\n",
      "Epoch 989: train loss: 0.0099820327013731\n",
      "Epoch 990: train loss: 0.009963630698621273\n",
      "Epoch 991: train loss: 0.009945312514901161\n",
      "Epoch 992: train loss: 0.009927078150212765\n",
      "Epoch 993: train loss: 0.009908927604556084\n",
      "Epoch 994: train loss: 0.009890852496027946\n",
      "Epoch 995: train loss: 0.009872853755950928\n",
      "Epoch 996: train loss: 0.009854936972260475\n",
      "Epoch 997: train loss: 0.009837097488343716\n",
      "Epoch 998: train loss: 0.009819328784942627\n",
      "Epoch 999: train loss: 0.009801643900573254\n",
      "Epoch 1000: train loss: 0.009784030728042126\n",
      "Epoch 1001: train loss: 0.00976648461073637\n",
      "Epoch 1002: train loss: 0.009749021381139755\n",
      "Epoch 1003: train loss: 0.009731639176607132\n",
      "Epoch 1004: train loss: 0.009714314714074135\n",
      "Epoch 1005: train loss: 0.009697061032056808\n",
      "Epoch 1006: train loss: 0.009679887443780899\n",
      "Epoch 1007: train loss: 0.009662781842052937\n",
      "Epoch 1008: train loss: 0.009645754471421242\n",
      "Epoch 1009: train loss: 0.009628789499402046\n",
      "Epoch 1010: train loss: 0.009611891582608223\n",
      "Epoch 1011: train loss: 0.009595059789717197\n",
      "Epoch 1012: train loss: 0.009578291326761246\n",
      "Epoch 1013: train loss: 0.009561601094901562\n",
      "Epoch 1014: train loss: 0.009544976986944675\n",
      "Epoch 1015: train loss: 0.00952842179685831\n",
      "Epoch 1016: train loss: 0.009511931799352169\n",
      "Epoch 1017: train loss: 0.009495501406490803\n",
      "Epoch 1018: train loss: 0.009479130618274212\n",
      "Epoch 1019: train loss: 0.009462832473218441\n",
      "Epoch 1020: train loss: 0.009446592070162296\n",
      "Epoch 1021: train loss: 0.00943042617291212\n",
      "Epoch 1022: train loss: 0.009414322674274445\n",
      "Epoch 1023: train loss: 0.009398272261023521\n",
      "Epoch 1024: train loss: 0.00938229076564312\n",
      "Epoch 1025: train loss: 0.009366362355649471\n",
      "Epoch 1026: train loss: 0.009350505657494068\n",
      "Epoch 1027: train loss: 0.009334691800177097\n",
      "Epoch 1028: train loss: 0.009318951517343521\n",
      "Epoch 1029: train loss: 0.0093032605946064\n",
      "Epoch 1030: train loss: 0.009287642315030098\n",
      "Epoch 1031: train loss: 0.009272074326872826\n",
      "Epoch 1032: train loss: 0.009256561286747456\n",
      "Epoch 1033: train loss: 0.009241110645234585\n",
      "Epoch 1034: train loss: 0.009225721471011639\n",
      "Epoch 1035: train loss: 0.009210379794239998\n",
      "Epoch 1036: train loss: 0.009195092134177685\n",
      "Epoch 1037: train loss: 0.009179864078760147\n",
      "Epoch 1038: train loss: 0.009164699353277683\n",
      "Epoch 1039: train loss: 0.009149582125246525\n",
      "Epoch 1040: train loss: 0.009134527295827866\n",
      "Epoch 1041: train loss: 0.009119526483118534\n",
      "Epoch 1042: train loss: 0.009104574099183083\n",
      "Epoch 1043: train loss: 0.009089675731956959\n",
      "Epoch 1044: train loss: 0.009074827656149864\n",
      "Epoch 1045: train loss: 0.009060041978955269\n",
      "Epoch 1046: train loss: 0.009045297279953957\n",
      "Epoch 1047: train loss: 0.009030626155436039\n",
      "Epoch 1048: train loss: 0.009015982039272785\n",
      "Epoch 1049: train loss: 0.009001405909657478\n",
      "Epoch 1050: train loss: 0.008986870758235455\n",
      "Epoch 1051: train loss: 0.008972384966909885\n",
      "Epoch 1052: train loss: 0.008957960642874241\n",
      "Epoch 1053: train loss: 0.008943594992160797\n",
      "Epoch 1054: train loss: 0.008929261937737465\n",
      "Epoch 1055: train loss: 0.008914991281926632\n",
      "Epoch 1056: train loss: 0.008900757879018784\n",
      "Epoch 1057: train loss: 0.008886574767529964\n",
      "Epoch 1058: train loss: 0.008872443810105324\n",
      "Epoch 1059: train loss: 0.008858362212777138\n",
      "Epoch 1060: train loss: 0.00884433276951313\n",
      "Epoch 1061: train loss: 0.008830351755023003\n",
      "Epoch 1062: train loss: 0.008816419169306755\n",
      "Epoch 1063: train loss: 0.008802524767816067\n",
      "Epoch 1064: train loss: 0.00878868531435728\n",
      "Epoch 1065: train loss: 0.008774891495704651\n",
      "Epoch 1066: train loss: 0.00876114796847105\n",
      "Epoch 1067: train loss: 0.008747447282075882\n",
      "Epoch 1068: train loss: 0.00873379223048687\n",
      "Epoch 1069: train loss: 0.008720180951058865\n",
      "Epoch 1070: train loss: 0.00870661623775959\n",
      "Epoch 1071: train loss: 0.008693107403814793\n",
      "Epoch 1072: train loss: 0.008679637685418129\n",
      "Epoch 1073: train loss: 0.008666223846375942\n",
      "Epoch 1074: train loss: 0.008652836084365845\n",
      "Epoch 1075: train loss: 0.008639506995677948\n",
      "Epoch 1076: train loss: 0.008626224473118782\n",
      "Epoch 1077: train loss: 0.0086129792034626\n",
      "Epoch 1078: train loss: 0.008599784225225449\n",
      "Epoch 1079: train loss: 0.008586624637246132\n",
      "Epoch 1080: train loss: 0.008573506027460098\n",
      "Epoch 1081: train loss: 0.00856044702231884\n",
      "Epoch 1082: train loss: 0.008547425270080566\n",
      "Epoch 1083: train loss: 0.0085344472900033\n",
      "Epoch 1084: train loss: 0.008521514013409615\n",
      "Epoch 1085: train loss: 0.008508624508976936\n",
      "Epoch 1086: train loss: 0.008495766669511795\n",
      "Epoch 1087: train loss: 0.008482960984110832\n",
      "Epoch 1088: train loss: 0.008470194414258003\n",
      "Epoch 1089: train loss: 0.008457471616566181\n",
      "Epoch 1090: train loss: 0.008444801904261112\n",
      "Epoch 1091: train loss: 0.008432170376181602\n",
      "Epoch 1092: train loss: 0.008419583551585674\n",
      "Epoch 1093: train loss: 0.00840702187269926\n",
      "Epoch 1094: train loss: 0.008394512347877026\n",
      "Epoch 1095: train loss: 0.008382048457860947\n",
      "Epoch 1096: train loss: 0.008369622752070427\n",
      "Epoch 1097: train loss: 0.008357226848602295\n",
      "Epoch 1098: train loss: 0.008344883099198341\n",
      "Epoch 1099: train loss: 0.008332584984600544\n",
      "Epoch 1100: train loss: 0.008320317603647709\n",
      "Epoch 1101: train loss: 0.008308099582791328\n",
      "Epoch 1102: train loss: 0.008295934647321701\n",
      "Epoch 1103: train loss: 0.008283796720206738\n",
      "Epoch 1104: train loss: 0.00827168021351099\n",
      "Epoch 1105: train loss: 0.00825963169336319\n",
      "Epoch 1106: train loss: 0.008247610181570053\n",
      "Epoch 1107: train loss: 0.008235650137066841\n",
      "Epoch 1108: train loss: 0.008223703131079674\n",
      "Epoch 1109: train loss: 0.008211804553866386\n",
      "Epoch 1110: train loss: 0.008199951611459255\n",
      "Epoch 1111: train loss: 0.008188121020793915\n",
      "Epoch 1112: train loss: 0.008176353760063648\n",
      "Epoch 1113: train loss: 0.008164619095623493\n",
      "Epoch 1114: train loss: 0.008152916096150875\n",
      "Epoch 1115: train loss: 0.008141258731484413\n",
      "Epoch 1116: train loss: 0.008129647932946682\n",
      "Epoch 1117: train loss: 0.008118055760860443\n",
      "Epoch 1118: train loss: 0.008106515742838383\n",
      "Epoch 1119: train loss: 0.008095023222267628\n",
      "Epoch 1120: train loss: 0.008083553984761238\n",
      "Epoch 1121: train loss: 0.00807211920619011\n",
      "Epoch 1122: train loss: 0.008060737513005733\n",
      "Epoch 1123: train loss: 0.008049401454627514\n",
      "Epoch 1124: train loss: 0.008038077503442764\n",
      "Epoch 1125: train loss: 0.008026815950870514\n",
      "Epoch 1126: train loss: 0.008015576750040054\n",
      "Epoch 1127: train loss: 0.008004384115338326\n",
      "Epoch 1128: train loss: 0.00799323245882988\n",
      "Epoch 1129: train loss: 0.007982112467288971\n",
      "Epoch 1130: train loss: 0.007971026003360748\n",
      "Epoch 1131: train loss: 0.007959982380270958\n",
      "Epoch 1132: train loss: 0.007948972284793854\n",
      "Epoch 1133: train loss: 0.007937992922961712\n",
      "Epoch 1134: train loss: 0.00792708620429039\n",
      "Epoch 1135: train loss: 0.007916181348264217\n",
      "Epoch 1136: train loss: 0.00790531374514103\n",
      "Epoch 1137: train loss: 0.007894490845501423\n",
      "Epoch 1138: train loss: 0.007883722893893719\n",
      "Epoch 1139: train loss: 0.00787297636270523\n",
      "Epoch 1140: train loss: 0.007862250320613384\n",
      "Epoch 1141: train loss: 0.007851586677134037\n",
      "Epoch 1142: train loss: 0.007840958423912525\n",
      "Epoch 1143: train loss: 0.007830355316400528\n",
      "Epoch 1144: train loss: 0.007819784805178642\n",
      "Epoch 1145: train loss: 0.007809259928762913\n",
      "Epoch 1146: train loss: 0.007798776961863041\n",
      "Epoch 1147: train loss: 0.007788313087075949\n",
      "Epoch 1148: train loss: 0.007777890656143427\n",
      "Epoch 1149: train loss: 0.007767514791339636\n",
      "Epoch 1150: train loss: 0.007757165469229221\n",
      "Epoch 1151: train loss: 0.007746859919279814\n",
      "Epoch 1152: train loss: 0.007736576721072197\n",
      "Epoch 1153: train loss: 0.007726328447461128\n",
      "Epoch 1154: train loss: 0.00771612161770463\n",
      "Epoch 1155: train loss: 0.007705969270318747\n",
      "Epoch 1156: train loss: 0.007695821579545736\n",
      "Epoch 1157: train loss: 0.007685716263949871\n",
      "Epoch 1158: train loss: 0.007675648666918278\n",
      "Epoch 1159: train loss: 0.007665611337870359\n",
      "Epoch 1160: train loss: 0.007655617780983448\n",
      "Epoch 1161: train loss: 0.007645654026418924\n",
      "Epoch 1162: train loss: 0.007635731250047684\n",
      "Epoch 1163: train loss: 0.00762582803145051\n",
      "Epoch 1164: train loss: 0.00761597603559494\n",
      "Epoch 1165: train loss: 0.007606152910739183\n",
      "Epoch 1166: train loss: 0.0075963567942380905\n",
      "Epoch 1167: train loss: 0.007586599327623844\n",
      "Epoch 1168: train loss: 0.007576864678412676\n",
      "Epoch 1169: train loss: 0.00756719009950757\n",
      "Epoch 1170: train loss: 0.007557517383247614\n",
      "Epoch 1171: train loss: 0.007547910790890455\n",
      "Epoch 1172: train loss: 0.007538306061178446\n",
      "Epoch 1173: train loss: 0.007528763264417648\n",
      "Epoch 1174: train loss: 0.0075192381627857685\n",
      "Epoch 1175: train loss: 0.007509728893637657\n",
      "Epoch 1176: train loss: 0.007500278297811747\n",
      "Epoch 1177: train loss: 0.007490851916372776\n",
      "Epoch 1178: train loss: 0.007481453008949757\n",
      "Epoch 1179: train loss: 0.007472087629139423\n",
      "Epoch 1180: train loss: 0.007462764624506235\n",
      "Epoch 1181: train loss: 0.007453470025211573\n",
      "Epoch 1182: train loss: 0.00744419451802969\n",
      "Epoch 1183: train loss: 0.007434963248670101\n",
      "Epoch 1184: train loss: 0.0074257575906813145\n",
      "Epoch 1185: train loss: 0.007416590116918087\n",
      "Epoch 1186: train loss: 0.007407444529235363\n",
      "Epoch 1187: train loss: 0.00739834550768137\n",
      "Epoch 1188: train loss: 0.0073892585933208466\n",
      "Epoch 1189: train loss: 0.007380199618637562\n",
      "Epoch 1190: train loss: 0.007371190004050732\n",
      "Epoch 1191: train loss: 0.007362217176705599\n",
      "Epoch 1192: train loss: 0.0073532527312636375\n",
      "Epoch 1193: train loss: 0.0073443325236439705\n",
      "Epoch 1194: train loss: 0.007335448171943426\n",
      "Epoch 1195: train loss: 0.007326577324420214\n",
      "Epoch 1196: train loss: 0.007317745592445135\n",
      "Epoch 1197: train loss: 0.007308950647711754\n",
      "Epoch 1198: train loss: 0.007300170138478279\n",
      "Epoch 1199: train loss: 0.007291438058018684\n",
      "Epoch 1200: train loss: 0.007282699458301067\n",
      "Epoch 1201: train loss: 0.0072740186005830765\n",
      "Epoch 1202: train loss: 0.007265378721058369\n",
      "Epoch 1203: train loss: 0.007256743498146534\n",
      "Epoch 1204: train loss: 0.007248143665492535\n",
      "Epoch 1205: train loss: 0.007239558268338442\n",
      "Epoch 1206: train loss: 0.007231019902974367\n",
      "Epoch 1207: train loss: 0.0072225104086101055\n",
      "Epoch 1208: train loss: 0.007214028853923082\n",
      "Epoch 1209: train loss: 0.007205578498542309\n",
      "Epoch 1210: train loss: 0.00719715328887105\n",
      "Epoch 1211: train loss: 0.007188751827925444\n",
      "Epoch 1212: train loss: 0.007180372718721628\n",
      "Epoch 1213: train loss: 0.007172033656388521\n",
      "Epoch 1214: train loss: 0.0071637267246842384\n",
      "Epoch 1215: train loss: 0.007155429106205702\n",
      "Epoch 1216: train loss: 0.007147160358726978\n",
      "Epoch 1217: train loss: 0.007138923276215792\n",
      "Epoch 1218: train loss: 0.007130712270736694\n",
      "Epoch 1219: train loss: 0.0071225352585315704\n",
      "Epoch 1220: train loss: 0.007114380598068237\n",
      "Epoch 1221: train loss: 0.007106253877282143\n",
      "Epoch 1222: train loss: 0.007098153233528137\n",
      "Epoch 1223: train loss: 0.007090079598128796\n",
      "Epoch 1224: train loss: 0.00708200316876173\n",
      "Epoch 1225: train loss: 0.007073994260281324\n",
      "Epoch 1226: train loss: 0.007065983023494482\n",
      "Epoch 1227: train loss: 0.00705802021548152\n",
      "Epoch 1228: train loss: 0.0070500667206943035\n",
      "Epoch 1229: train loss: 0.007042135111987591\n",
      "Epoch 1230: train loss: 0.007034237962216139\n",
      "Epoch 1231: train loss: 0.0070263720117509365\n",
      "Epoch 1232: train loss: 0.007018508855253458\n",
      "Epoch 1233: train loss: 0.0070106894709169865\n",
      "Epoch 1234: train loss: 0.00700288824737072\n",
      "Epoch 1235: train loss: 0.006995109375566244\n",
      "Epoch 1236: train loss: 0.00698736310005188\n",
      "Epoch 1237: train loss: 0.006979620084166527\n",
      "Epoch 1238: train loss: 0.006971912458539009\n",
      "Epoch 1239: train loss: 0.006964239291846752\n",
      "Epoch 1240: train loss: 0.006956573110073805\n",
      "Epoch 1241: train loss: 0.006948945112526417\n",
      "Epoch 1242: train loss: 0.006941338535398245\n",
      "Epoch 1243: train loss: 0.006933728698641062\n",
      "Epoch 1244: train loss: 0.006926164962351322\n",
      "Epoch 1245: train loss: 0.006918619852513075\n",
      "Epoch 1246: train loss: 0.006911109667271376\n",
      "Epoch 1247: train loss: 0.006903599947690964\n",
      "Epoch 1248: train loss: 0.006896126549690962\n",
      "Epoch 1249: train loss: 0.006888683419674635\n",
      "Epoch 1250: train loss: 0.006881239358335733\n",
      "Epoch 1251: train loss: 0.0068738143891096115\n",
      "Epoch 1252: train loss: 0.006866432726383209\n",
      "Epoch 1253: train loss: 0.006859065964818001\n",
      "Epoch 1254: train loss: 0.006851716432720423\n",
      "Epoch 1255: train loss: 0.006844376679509878\n",
      "Epoch 1256: train loss: 0.00683706346899271\n",
      "Epoch 1257: train loss: 0.006829787045717239\n",
      "Epoch 1258: train loss: 0.0068225194700062275\n",
      "Epoch 1259: train loss: 0.0068152789026498795\n",
      "Epoch 1260: train loss: 0.006808042526245117\n",
      "Epoch 1261: train loss: 0.006800847593694925\n",
      "Epoch 1262: train loss: 0.006793653126806021\n",
      "Epoch 1263: train loss: 0.00678648054599762\n",
      "Epoch 1264: train loss: 0.006779323797672987\n",
      "Epoch 1265: train loss: 0.006772199179977179\n",
      "Epoch 1266: train loss: 0.006765084341168404\n",
      "Epoch 1267: train loss: 0.006757987197488546\n",
      "Epoch 1268: train loss: 0.006750922650098801\n",
      "Epoch 1269: train loss: 0.006743867881596088\n",
      "Epoch 1270: train loss: 0.0067368242889642715\n",
      "Epoch 1271: train loss: 0.0067298030480742455\n",
      "Epoch 1272: train loss: 0.006722822319716215\n",
      "Epoch 1273: train loss: 0.006715820170938969\n",
      "Epoch 1274: train loss: 0.006708880420774221\n",
      "Epoch 1275: train loss: 0.006701916456222534\n",
      "Epoch 1276: train loss: 0.00669498834758997\n",
      "Epoch 1277: train loss: 0.0066880760714411736\n",
      "Epoch 1278: train loss: 0.006681171245872974\n",
      "Epoch 1279: train loss: 0.006674298085272312\n",
      "Epoch 1280: train loss: 0.006667419336736202\n",
      "Epoch 1281: train loss: 0.006660589948296547\n",
      "Epoch 1282: train loss: 0.006653765216469765\n",
      "Epoch 1283: train loss: 0.006646942347288132\n",
      "Epoch 1284: train loss: 0.006640149746090174\n",
      "Epoch 1285: train loss: 0.006633377633988857\n",
      "Epoch 1286: train loss: 0.006626609712839127\n",
      "Epoch 1287: train loss: 0.006619849242269993\n",
      "Epoch 1288: train loss: 0.006613115780055523\n",
      "Epoch 1289: train loss: 0.006606394425034523\n",
      "Epoch 1290: train loss: 0.006599692162126303\n",
      "Epoch 1291: train loss: 0.006593001540750265\n",
      "Epoch 1292: train loss: 0.006586338859051466\n",
      "Epoch 1293: train loss: 0.006579678505659103\n",
      "Epoch 1294: train loss: 0.006573021411895752\n",
      "Epoch 1295: train loss: 0.006566392257809639\n",
      "Epoch 1296: train loss: 0.006559782195836306\n",
      "Epoch 1297: train loss: 0.006553175859153271\n",
      "Epoch 1298: train loss: 0.006546576973050833\n",
      "Epoch 1299: train loss: 0.006539999973028898\n",
      "Epoch 1300: train loss: 0.00653344951570034\n",
      "Epoch 1301: train loss: 0.006526902783662081\n",
      "Epoch 1302: train loss: 0.006520356051623821\n",
      "Epoch 1303: train loss: 0.006513827946037054\n",
      "Epoch 1304: train loss: 0.006507307756692171\n",
      "Epoch 1305: train loss: 0.006500813644379377\n",
      "Epoch 1306: train loss: 0.00649433396756649\n",
      "Epoch 1307: train loss: 0.0064878519624471664\n",
      "Epoch 1308: train loss: 0.006481405813246965\n",
      "Epoch 1309: train loss: 0.006474954076111317\n",
      "Epoch 1310: train loss: 0.006468503270298243\n",
      "Epoch 1311: train loss: 0.006462071556597948\n",
      "Epoch 1312: train loss: 0.006455662660300732\n",
      "Epoch 1313: train loss: 0.006449253764003515\n",
      "Epoch 1314: train loss: 0.006442866288125515\n",
      "Epoch 1315: train loss: 0.006436503492295742\n",
      "Epoch 1316: train loss: 0.0064301155507564545\n",
      "Epoch 1317: train loss: 0.0064237588085234165\n",
      "Epoch 1318: train loss: 0.006417401134967804\n",
      "Epoch 1319: train loss: 0.0064110709354281425\n",
      "Epoch 1320: train loss: 0.006404741667211056\n",
      "Epoch 1321: train loss: 0.006398417055606842\n",
      "Epoch 1322: train loss: 0.006392129231244326\n",
      "Epoch 1323: train loss: 0.0063858008943498135\n",
      "Epoch 1324: train loss: 0.006379532162100077\n",
      "Epoch 1325: train loss: 0.006373248063027859\n",
      "Epoch 1326: train loss: 0.006366973277181387\n",
      "Epoch 1327: train loss: 0.006360724568367004\n",
      "Epoch 1328: train loss: 0.006354476325213909\n",
      "Epoch 1329: train loss: 0.00634822528809309\n",
      "Epoch 1330: train loss: 0.006341991946101189\n",
      "Epoch 1331: train loss: 0.006335762795060873\n",
      "Epoch 1332: train loss: 0.0063295322470366955\n",
      "Epoch 1333: train loss: 0.006323338020592928\n",
      "Epoch 1334: train loss: 0.006317127961665392\n",
      "Epoch 1335: train loss: 0.00631093792617321\n",
      "Epoch 1336: train loss: 0.006304759066551924\n",
      "Epoch 1337: train loss: 0.006298591382801533\n",
      "Epoch 1338: train loss: 0.006292424164712429\n",
      "Epoch 1339: train loss: 0.0062862662598490715\n",
      "Epoch 1340: train loss: 0.006280103232711554\n",
      "Epoch 1341: train loss: 0.006273956969380379\n",
      "Epoch 1342: train loss: 0.0062678223475813866\n",
      "Epoch 1343: train loss: 0.006261693779379129\n",
      "Epoch 1344: train loss: 0.006255582440644503\n",
      "Epoch 1345: train loss: 0.006249465048313141\n",
      "Epoch 1346: train loss: 0.006243356037884951\n",
      "Epoch 1347: train loss: 0.006237240042537451\n",
      "Epoch 1348: train loss: 0.006231130100786686\n",
      "Epoch 1349: train loss: 0.006225049961358309\n",
      "Epoch 1350: train loss: 0.006218970287591219\n",
      "Epoch 1351: train loss: 0.006212899461388588\n",
      "Epoch 1352: train loss: 0.006206819787621498\n",
      "Epoch 1353: train loss: 0.006200749892741442\n",
      "Epoch 1354: train loss: 0.006194682326167822\n",
      "Epoch 1355: train loss: 0.006188629195094109\n",
      "Epoch 1356: train loss: 0.006182584911584854\n",
      "Epoch 1357: train loss: 0.0061765434220433235\n",
      "Epoch 1358: train loss: 0.006170515436679125\n",
      "Epoch 1359: train loss: 0.006164466962218285\n",
      "Epoch 1360: train loss: 0.006158432923257351\n",
      "Epoch 1361: train loss: 0.006152414251118898\n",
      "Epoch 1362: train loss: 0.006146385334432125\n",
      "Epoch 1363: train loss: 0.006140382494777441\n",
      "Epoch 1364: train loss: 0.0061343759298324585\n",
      "Epoch 1365: train loss: 0.006128373555839062\n",
      "Epoch 1366: train loss: 0.006122367922216654\n",
      "Epoch 1367: train loss: 0.0061163767240941525\n",
      "Epoch 1368: train loss: 0.006110391113907099\n",
      "Epoch 1369: train loss: 0.006104395259171724\n",
      "Epoch 1370: train loss: 0.006098407320678234\n",
      "Epoch 1371: train loss: 0.006092432886362076\n",
      "Epoch 1372: train loss: 0.006086447276175022\n",
      "Epoch 1373: train loss: 0.0060804858803749084\n",
      "Epoch 1374: train loss: 0.006074518896639347\n",
      "Epoch 1375: train loss: 0.006068549118936062\n",
      "Epoch 1376: train loss: 0.006062588654458523\n",
      "Epoch 1377: train loss: 0.006056623067706823\n",
      "Epoch 1378: train loss: 0.006050670985132456\n",
      "Epoch 1379: train loss: 0.006044718436896801\n",
      "Epoch 1380: train loss: 0.006038770545274019\n",
      "Epoch 1381: train loss: 0.006032831966876984\n",
      "Epoch 1382: train loss: 0.006026876624673605\n",
      "Epoch 1383: train loss: 0.006020928267389536\n",
      "Epoch 1384: train loss: 0.006015000864863396\n",
      "Epoch 1385: train loss: 0.00600906228646636\n",
      "Epoch 1386: train loss: 0.006003149319440126\n",
      "Epoch 1387: train loss: 0.005997197236865759\n",
      "Epoch 1388: train loss: 0.005991282407194376\n",
      "Epoch 1389: train loss: 0.005985364317893982\n",
      "Epoch 1390: train loss: 0.005979421082884073\n",
      "Epoch 1391: train loss: 0.005973499733954668\n",
      "Epoch 1392: train loss: 0.005967576522380114\n",
      "Epoch 1393: train loss: 0.005961674265563488\n",
      "Epoch 1394: train loss: 0.005955755244940519\n",
      "Epoch 1395: train loss: 0.005949844140559435\n",
      "Epoch 1396: train loss: 0.0059439316391944885\n",
      "Epoch 1397: train loss: 0.005938011687248945\n",
      "Epoch 1398: train loss: 0.005932119209319353\n",
      "Epoch 1399: train loss: 0.00592620437964797\n",
      "Epoch 1400: train loss: 0.005920297931879759\n",
      "Epoch 1401: train loss: 0.005914397072046995\n",
      "Epoch 1402: train loss: 0.005908491555601358\n",
      "Epoch 1403: train loss: 0.00590259674936533\n",
      "Epoch 1404: train loss: 0.005896684713661671\n",
      "Epoch 1405: train loss: 0.005890784319490194\n",
      "Epoch 1406: train loss: 0.005884877406060696\n",
      "Epoch 1407: train loss: 0.005878979340195656\n",
      "Epoch 1408: train loss: 0.005873094778507948\n",
      "Epoch 1409: train loss: 0.005867195315659046\n",
      "Epoch 1410: train loss: 0.005861303303390741\n",
      "Epoch 1411: train loss: 0.005855403374880552\n",
      "Epoch 1412: train loss: 0.00584951788187027\n",
      "Epoch 1413: train loss: 0.005843629594892263\n",
      "Epoch 1414: train loss: 0.005837734322994947\n",
      "Epoch 1415: train loss: 0.005831834860146046\n",
      "Epoch 1416: train loss: 0.005825934465974569\n",
      "Epoch 1417: train loss: 0.005820031277835369\n",
      "Epoch 1418: train loss: 0.005814155098050833\n",
      "Epoch 1419: train loss: 0.005808275658637285\n",
      "Epoch 1420: train loss: 0.005802372936159372\n",
      "Epoch 1421: train loss: 0.005796494893729687\n",
      "Epoch 1422: train loss: 0.0057905870489776134\n",
      "Epoch 1423: train loss: 0.005784704815596342\n",
      "Epoch 1424: train loss: 0.005778816994279623\n",
      "Epoch 1425: train loss: 0.0057729328982532024\n",
      "Epoch 1426: train loss: 0.005767041817307472\n",
      "Epoch 1427: train loss: 0.005761147942394018\n",
      "Epoch 1428: train loss: 0.005755259655416012\n",
      "Epoch 1429: train loss: 0.005749344825744629\n",
      "Epoch 1430: train loss: 0.00574346911162138\n",
      "Epoch 1431: train loss: 0.00573757104575634\n",
      "Epoch 1432: train loss: 0.00573167996481061\n",
      "Epoch 1433: train loss: 0.005725792143493891\n",
      "Epoch 1434: train loss: 0.0057198964059352875\n",
      "Epoch 1435: train loss: 0.005714007653295994\n",
      "Epoch 1436: train loss: 0.00570810167118907\n",
      "Epoch 1437: train loss: 0.0057022301480174065\n",
      "Epoch 1438: train loss: 0.005696364678442478\n",
      "Epoch 1439: train loss: 0.005690522491931915\n",
      "Epoch 1440: train loss: 0.005684760399162769\n",
      "Epoch 1441: train loss: 0.005679143127053976\n",
      "Epoch 1442: train loss: 0.005673891399055719\n",
      "Epoch 1443: train loss: 0.005669516045600176\n",
      "Epoch 1444: train loss: 0.005667105317115784\n",
      "Epoch 1445: train loss: 0.005670086480677128\n",
      "Epoch 1446: train loss: 0.005683425813913345\n",
      "Epoch 1447: train loss: 0.005729639437049627\n",
      "Epoch 1448: train loss: 0.005808865185827017\n",
      "Epoch 1449: train loss: 0.006022516172379255\n",
      "Epoch 1450: train loss: 0.006032656878232956\n",
      "Epoch 1451: train loss: 0.006094949785619974\n",
      "Epoch 1452: train loss: 0.00573724927380681\n",
      "Epoch 1453: train loss: 0.005610227584838867\n",
      "Epoch 1454: train loss: 0.005722938105463982\n",
      "Epoch 1455: train loss: 0.005834576673805714\n",
      "Epoch 1456: train loss: 0.005862285383045673\n",
      "Epoch 1457: train loss: 0.005641251802444458\n",
      "Epoch 1458: train loss: 0.00559483515098691\n",
      "Epoch 1459: train loss: 0.005715101957321167\n",
      "Epoch 1460: train loss: 0.005727970972657204\n",
      "Epoch 1461: train loss: 0.005652347579598427\n",
      "Epoch 1462: train loss: 0.005561891011893749\n",
      "Epoch 1463: train loss: 0.005600923206657171\n",
      "Epoch 1464: train loss: 0.005677560344338417\n",
      "Epoch 1465: train loss: 0.005616292357444763\n",
      "Epoch 1466: train loss: 0.005545740015804768\n",
      "Epoch 1467: train loss: 0.005547685083001852\n",
      "Epoch 1468: train loss: 0.005588519386947155\n",
      "Epoch 1469: train loss: 0.0055925678461790085\n",
      "Epoch 1470: train loss: 0.005533686373382807\n",
      "Epoch 1471: train loss: 0.005513798911124468\n",
      "Epoch 1472: train loss: 0.005540660582482815\n",
      "Epoch 1473: train loss: 0.005545000080019236\n",
      "Epoch 1474: train loss: 0.005517635960131884\n",
      "Epoch 1475: train loss: 0.005489757750183344\n",
      "Epoch 1476: train loss: 0.005495954304933548\n",
      "Epoch 1477: train loss: 0.005510709714144468\n",
      "Epoch 1478: train loss: 0.005495364777743816\n",
      "Epoch 1479: train loss: 0.005471403244882822\n",
      "Epoch 1480: train loss: 0.005463693756610155\n",
      "Epoch 1481: train loss: 0.005470863077789545\n",
      "Epoch 1482: train loss: 0.0054714130237698555\n",
      "Epoch 1483: train loss: 0.005454442463815212\n",
      "Epoch 1484: train loss: 0.005439995788037777\n",
      "Epoch 1485: train loss: 0.005438215099275112\n",
      "Epoch 1486: train loss: 0.005440124776214361\n",
      "Epoch 1487: train loss: 0.005434710066765547\n",
      "Epoch 1488: train loss: 0.005421335808932781\n",
      "Epoch 1489: train loss: 0.0054120407439768314\n",
      "Epoch 1490: train loss: 0.005410105921328068\n",
      "Epoch 1491: train loss: 0.005408588331192732\n",
      "Epoch 1492: train loss: 0.0054021598771214485\n",
      "Epoch 1493: train loss: 0.005391883663833141\n",
      "Epoch 1494: train loss: 0.005384268704801798\n",
      "Epoch 1495: train loss: 0.0053808484226465225\n",
      "Epoch 1496: train loss: 0.005377795547246933\n",
      "Epoch 1497: train loss: 0.005372012499719858\n",
      "Epoch 1498: train loss: 0.0053637390956282616\n",
      "Epoch 1499: train loss: 0.005356456618756056\n",
      "Epoch 1500: train loss: 0.005351637024432421\n",
      "Epoch 1501: train loss: 0.005347763188183308\n",
      "Epoch 1502: train loss: 0.005342733580619097\n",
      "Epoch 1503: train loss: 0.0053359526209533215\n",
      "Epoch 1504: train loss: 0.005328953731805086\n",
      "Epoch 1505: train loss: 0.0053230226039886475\n",
      "Epoch 1506: train loss: 0.005318213254213333\n",
      "Epoch 1507: train loss: 0.005313507281243801\n",
      "Epoch 1508: train loss: 0.005307920277118683\n",
      "Epoch 1509: train loss: 0.005301619414240122\n",
      "Epoch 1510: train loss: 0.005295270588248968\n",
      "Epoch 1511: train loss: 0.0052895019762218\n",
      "Epoch 1512: train loss: 0.005284290760755539\n",
      "Epoch 1513: train loss: 0.00527916057035327\n",
      "Epoch 1514: train loss: 0.005273753777146339\n",
      "Epoch 1515: train loss: 0.005267879460006952\n",
      "Epoch 1516: train loss: 0.005261871963739395\n",
      "Epoch 1517: train loss: 0.0052559757605195045\n",
      "Epoch 1518: train loss: 0.005250378046184778\n",
      "Epoch 1519: train loss: 0.005245013162493706\n",
      "Epoch 1520: train loss: 0.00523964362218976\n",
      "Epoch 1521: train loss: 0.005234129726886749\n",
      "Epoch 1522: train loss: 0.005228420719504356\n",
      "Epoch 1523: train loss: 0.005222644656896591\n",
      "Epoch 1524: train loss: 0.005216860678046942\n",
      "Epoch 1525: train loss: 0.005211156792938709\n",
      "Epoch 1526: train loss: 0.005205562803894281\n",
      "Epoch 1527: train loss: 0.005200065206736326\n",
      "Epoch 1528: train loss: 0.005194568540900946\n",
      "Epoch 1529: train loss: 0.0051890271715819836\n",
      "Epoch 1530: train loss: 0.005183445289731026\n",
      "Epoch 1531: train loss: 0.005177768878638744\n",
      "Epoch 1532: train loss: 0.005172085948288441\n",
      "Epoch 1533: train loss: 0.005166415125131607\n",
      "Epoch 1534: train loss: 0.005160737317055464\n",
      "Epoch 1535: train loss: 0.005155092105269432\n",
      "Epoch 1536: train loss: 0.00514946086332202\n",
      "Epoch 1537: train loss: 0.005143861286342144\n",
      "Epoch 1538: train loss: 0.005138286855071783\n",
      "Epoch 1539: train loss: 0.0051326751708984375\n",
      "Epoch 1540: train loss: 0.005127094220370054\n",
      "Epoch 1541: train loss: 0.0051215109415352345\n",
      "Epoch 1542: train loss: 0.005115939304232597\n",
      "Epoch 1543: train loss: 0.005110340658575296\n",
      "Epoch 1544: train loss: 0.005104763433337212\n",
      "Epoch 1545: train loss: 0.0050991796888411045\n",
      "Epoch 1546: train loss: 0.005093621555715799\n",
      "Epoch 1547: train loss: 0.005088100675493479\n",
      "Epoch 1548: train loss: 0.005082622170448303\n",
      "Epoch 1549: train loss: 0.00507719162851572\n",
      "Epoch 1550: train loss: 0.005071892403066158\n",
      "Epoch 1551: train loss: 0.005066829267889261\n",
      "Epoch 1552: train loss: 0.005062034819275141\n",
      "Epoch 1553: train loss: 0.005057893227785826\n",
      "Epoch 1554: train loss: 0.005054605659097433\n",
      "Epoch 1555: train loss: 0.0050534470938146114\n",
      "Epoch 1556: train loss: 0.005054625682532787\n",
      "Epoch 1557: train loss: 0.0050631118938326836\n",
      "Epoch 1558: train loss: 0.005076991394162178\n",
      "Epoch 1559: train loss: 0.005115046165883541\n",
      "Epoch 1560: train loss: 0.005152740515768528\n",
      "Epoch 1561: train loss: 0.005249879322946072\n",
      "Epoch 1562: train loss: 0.00526014156639576\n",
      "Epoch 1563: train loss: 0.005329451523721218\n",
      "Epoch 1564: train loss: 0.005193252116441727\n",
      "Epoch 1565: train loss: 0.005101074930280447\n",
      "Epoch 1566: train loss: 0.005005018785595894\n",
      "Epoch 1567: train loss: 0.004977425094693899\n",
      "Epoch 1568: train loss: 0.0050064679235219955\n",
      "Epoch 1569: train loss: 0.005053843837231398\n",
      "Epoch 1570: train loss: 0.0051067909225821495\n",
      "Epoch 1571: train loss: 0.005075380671769381\n",
      "Epoch 1572: train loss: 0.005036747548729181\n",
      "Epoch 1573: train loss: 0.004972183611243963\n",
      "Epoch 1574: train loss: 0.004939835052937269\n",
      "Epoch 1575: train loss: 0.004942167550325394\n",
      "Epoch 1576: train loss: 0.0049644336104393005\n",
      "Epoch 1577: train loss: 0.004990566987544298\n",
      "Epoch 1578: train loss: 0.004983799532055855\n",
      "Epoch 1579: train loss: 0.004967082291841507\n",
      "Epoch 1580: train loss: 0.004930966999381781\n",
      "Epoch 1581: train loss: 0.00490575423464179\n",
      "Epoch 1582: train loss: 0.004895983263850212\n",
      "Epoch 1583: train loss: 0.004900146275758743\n",
      "Epoch 1584: train loss: 0.004910414572805166\n",
      "Epoch 1585: train loss: 0.004913125187158585\n",
      "Epoch 1586: train loss: 0.004910491406917572\n",
      "Epoch 1587: train loss: 0.004893930163234472\n",
      "Epoch 1588: train loss: 0.004877038300037384\n",
      "Epoch 1589: train loss: 0.004861366935074329\n",
      "Epoch 1590: train loss: 0.004852015990763903\n",
      "Epoch 1591: train loss: 0.004848655313253403\n",
      "Epoch 1592: train loss: 0.004848727025091648\n",
      "Epoch 1593: train loss: 0.004849784541875124\n",
      "Epoch 1594: train loss: 0.004847547970712185\n",
      "Epoch 1595: train loss: 0.004843724891543388\n",
      "Epoch 1596: train loss: 0.004835027270019054\n",
      "Epoch 1597: train loss: 0.004825927317142487\n",
      "Epoch 1598: train loss: 0.004815403837710619\n",
      "Epoch 1599: train loss: 0.004806131590157747\n",
      "Epoch 1600: train loss: 0.004798125475645065\n",
      "Epoch 1601: train loss: 0.004791662562638521\n",
      "Epoch 1602: train loss: 0.0047864606603980064\n",
      "Epoch 1603: train loss: 0.0047821770422160625\n",
      "Epoch 1604: train loss: 0.00477843452244997\n",
      "Epoch 1605: train loss: 0.004774815868586302\n",
      "Epoch 1606: train loss: 0.0047715636901557446\n",
      "Epoch 1607: train loss: 0.0047679184935987\n",
      "Epoch 1608: train loss: 0.004765041172504425\n",
      "Epoch 1609: train loss: 0.00476145651191473\n",
      "Epoch 1610: train loss: 0.004759489558637142\n",
      "Epoch 1611: train loss: 0.004756591748446226\n",
      "Epoch 1612: train loss: 0.004756786860525608\n",
      "Epoch 1613: train loss: 0.004755547270178795\n",
      "Epoch 1614: train loss: 0.004759993404150009\n",
      "Epoch 1615: train loss: 0.0047613936476409435\n",
      "Epoch 1616: train loss: 0.004773093853145838\n",
      "Epoch 1617: train loss: 0.004776686429977417\n",
      "Epoch 1618: train loss: 0.004797103349119425\n",
      "Epoch 1619: train loss: 0.004796803928911686\n",
      "Epoch 1620: train loss: 0.0048184641636908054\n",
      "Epoch 1621: train loss: 0.004801016300916672\n",
      "Epoch 1622: train loss: 0.004804230760782957\n",
      "Epoch 1623: train loss: 0.0047662449069321156\n",
      "Epoch 1624: train loss: 0.004744100850075483\n",
      "Epoch 1625: train loss: 0.004705666098743677\n",
      "Epoch 1626: train loss: 0.00467944610863924\n",
      "Epoch 1627: train loss: 0.004657476209104061\n",
      "Epoch 1628: train loss: 0.004644232802093029\n",
      "Epoch 1629: train loss: 0.0046377326361835\n",
      "Epoch 1630: train loss: 0.004636330064386129\n",
      "Epoch 1631: train loss: 0.004638677928596735\n",
      "Epoch 1632: train loss: 0.004642321262508631\n",
      "Epoch 1633: train loss: 0.004649516195058823\n",
      "Epoch 1634: train loss: 0.004653116688132286\n",
      "Epoch 1635: train loss: 0.004663120489567518\n",
      "Epoch 1636: train loss: 0.004662865307182074\n",
      "Epoch 1637: train loss: 0.004672472830861807\n",
      "Epoch 1638: train loss: 0.004664561245590448\n",
      "Epoch 1639: train loss: 0.0046683079563081264\n",
      "Epoch 1640: train loss: 0.004651215858757496\n",
      "Epoch 1641: train loss: 0.004644995089620352\n",
      "Epoch 1642: train loss: 0.004622724372893572\n",
      "Epoch 1643: train loss: 0.0046091037802398205\n",
      "Epoch 1644: train loss: 0.004588486161082983\n",
      "Epoch 1645: train loss: 0.004573932848870754\n",
      "Epoch 1646: train loss: 0.00455881655216217\n",
      "Epoch 1647: train loss: 0.004547366872429848\n",
      "Epoch 1648: train loss: 0.0045372843742370605\n",
      "Epoch 1649: train loss: 0.004529065918177366\n",
      "Epoch 1650: train loss: 0.004521979484707117\n",
      "Epoch 1651: train loss: 0.004515728913247585\n",
      "Epoch 1652: train loss: 0.004510051570832729\n",
      "Epoch 1653: train loss: 0.004504747688770294\n",
      "Epoch 1654: train loss: 0.004499796777963638\n",
      "Epoch 1655: train loss: 0.004495256580412388\n",
      "Epoch 1656: train loss: 0.004491386469453573\n",
      "Epoch 1657: train loss: 0.004488399252295494\n",
      "Epoch 1658: train loss: 0.0044875540770590305\n",
      "Epoch 1659: train loss: 0.004489017650485039\n",
      "Epoch 1660: train loss: 0.00449781958013773\n",
      "Epoch 1661: train loss: 0.004512106534093618\n",
      "Epoch 1662: train loss: 0.004551112651824951\n",
      "Epoch 1663: train loss: 0.004593114834278822\n",
      "Epoch 1664: train loss: 0.004704089369624853\n",
      "Epoch 1665: train loss: 0.004740809090435505\n",
      "Epoch 1666: train loss: 0.004877512808889151\n",
      "Epoch 1667: train loss: 0.0047394591383636\n",
      "Epoch 1668: train loss: 0.004669516813009977\n",
      "Epoch 1669: train loss: 0.00450475886464119\n",
      "Epoch 1670: train loss: 0.0044216676615178585\n",
      "Epoch 1671: train loss: 0.004408759064972401\n",
      "Epoch 1672: train loss: 0.004450762178748846\n",
      "Epoch 1673: train loss: 0.004524217452853918\n",
      "Epoch 1674: train loss: 0.004544255789369345\n",
      "Epoch 1675: train loss: 0.004557913169264793\n",
      "Epoch 1676: train loss: 0.004478028975427151\n",
      "Epoch 1677: train loss: 0.004415226634591818\n",
      "Epoch 1678: train loss: 0.004370200913399458\n",
      "Epoch 1679: train loss: 0.004363888408988714\n",
      "Epoch 1680: train loss: 0.004386274144053459\n",
      "Epoch 1681: train loss: 0.00441137608140707\n",
      "Epoch 1682: train loss: 0.004434408154338598\n",
      "Epoch 1683: train loss: 0.0044157435186207294\n",
      "Epoch 1684: train loss: 0.004392988979816437\n",
      "Epoch 1685: train loss: 0.004354090895503759\n",
      "Epoch 1686: train loss: 0.0043272050097584724\n",
      "Epoch 1687: train loss: 0.0043141101486980915\n",
      "Epoch 1688: train loss: 0.004314429126679897\n",
      "Epoch 1689: train loss: 0.004322937224060297\n",
      "Epoch 1690: train loss: 0.004330355208367109\n",
      "Epoch 1691: train loss: 0.004337468650192022\n",
      "Epoch 1692: train loss: 0.00433084974065423\n",
      "Epoch 1693: train loss: 0.004323796834796667\n",
      "Epoch 1694: train loss: 0.004305928014218807\n",
      "Epoch 1695: train loss: 0.004290597513318062\n",
      "Epoch 1696: train loss: 0.004274048842489719\n",
      "Epoch 1697: train loss: 0.0042612310498952866\n",
      "Epoch 1698: train loss: 0.004251228645443916\n",
      "Epoch 1699: train loss: 0.004244110081344843\n",
      "Epoch 1700: train loss: 0.004239204339683056\n",
      "Epoch 1701: train loss: 0.004235817585140467\n",
      "Epoch 1702: train loss: 0.0042336974292993546\n",
      "Epoch 1703: train loss: 0.004232130013406277\n",
      "Epoch 1704: train loss: 0.0042324489913880825\n",
      "Epoch 1705: train loss: 0.004232610575854778\n",
      "Epoch 1706: train loss: 0.004237383604049683\n",
      "Epoch 1707: train loss: 0.004240712616592646\n",
      "Epoch 1708: train loss: 0.004254319239407778\n",
      "Epoch 1709: train loss: 0.0042622266337275505\n",
      "Epoch 1710: train loss: 0.0042904275469481945\n",
      "Epoch 1711: train loss: 0.004298556596040726\n",
      "Epoch 1712: train loss: 0.004338723607361317\n",
      "Epoch 1713: train loss: 0.004327258560806513\n",
      "Epoch 1714: train loss: 0.004350669216364622\n",
      "Epoch 1715: train loss: 0.004300709813833237\n",
      "Epoch 1716: train loss: 0.004277781117707491\n",
      "Epoch 1717: train loss: 0.004215913359075785\n",
      "Epoch 1718: train loss: 0.004175000824034214\n",
      "Epoch 1719: train loss: 0.004139321390539408\n",
      "Epoch 1720: train loss: 0.0041199782863259315\n",
      "Epoch 1721: train loss: 0.0041136653162539005\n",
      "Epoch 1722: train loss: 0.004116925876587629\n",
      "Epoch 1723: train loss: 0.004127004183828831\n",
      "Epoch 1724: train loss: 0.00413664011284709\n",
      "Epoch 1725: train loss: 0.004152054898440838\n",
      "Epoch 1726: train loss: 0.004154749680310488\n",
      "Epoch 1727: train loss: 0.00416683591902256\n",
      "Epoch 1728: train loss: 0.004156476818025112\n",
      "Epoch 1729: train loss: 0.004158398602157831\n",
      "Epoch 1730: train loss: 0.004136953037232161\n",
      "Epoch 1731: train loss: 0.004127644468098879\n",
      "Epoch 1732: train loss: 0.0041028219275176525\n",
      "Epoch 1733: train loss: 0.004087676759809256\n",
      "Epoch 1734: train loss: 0.004066433757543564\n",
      "Epoch 1735: train loss: 0.004051706753671169\n",
      "Epoch 1736: train loss: 0.004036284051835537\n",
      "Epoch 1737: train loss: 0.004024581518024206\n",
      "Epoch 1738: train loss: 0.004013731610029936\n",
      "Epoch 1739: train loss: 0.00400472292676568\n",
      "Epoch 1740: train loss: 0.0039964173920452595\n",
      "Epoch 1741: train loss: 0.003988905344158411\n",
      "Epoch 1742: train loss: 0.0039817760698497295\n",
      "Epoch 1743: train loss: 0.003975016064941883\n",
      "Epoch 1744: train loss: 0.003968480508774519\n",
      "Epoch 1745: train loss: 0.003962245769798756\n",
      "Epoch 1746: train loss: 0.003956418484449387\n",
      "Epoch 1747: train loss: 0.0039513977244496346\n",
      "Epoch 1748: train loss: 0.0039475299417972565\n",
      "Epoch 1749: train loss: 0.003946623764932156\n",
      "Epoch 1750: train loss: 0.003949569072574377\n",
      "Epoch 1751: train loss: 0.003964217379689217\n",
      "Epoch 1752: train loss: 0.003990015480667353\n",
      "Epoch 1753: train loss: 0.004061418119817972\n",
      "Epoch 1754: train loss: 0.004139421973377466\n",
      "Epoch 1755: train loss: 0.004351089242845774\n",
      "Epoch 1756: train loss: 0.004365650471299887\n",
      "Epoch 1757: train loss: 0.004525452386587858\n",
      "Epoch 1758: train loss: 0.00421036034822464\n",
      "Epoch 1759: train loss: 0.00402154540643096\n",
      "Epoch 1760: train loss: 0.003878321498632431\n",
      "Epoch 1761: train loss: 0.003873947774991393\n",
      "Epoch 1762: train loss: 0.003970014397054911\n",
      "Epoch 1763: train loss: 0.004050088580697775\n",
      "Epoch 1764: train loss: 0.0041213445365428925\n",
      "Epoch 1765: train loss: 0.004000056069344282\n",
      "Epoch 1766: train loss: 0.003897442715242505\n",
      "Epoch 1767: train loss: 0.003822516882792115\n",
      "Epoch 1768: train loss: 0.0038218179251998663\n",
      "Epoch 1769: train loss: 0.0038731724489480257\n",
      "Epoch 1770: train loss: 0.003914741333574057\n",
      "Epoch 1771: train loss: 0.003945578821003437\n",
      "Epoch 1772: train loss: 0.0038901621010154486\n",
      "Epoch 1773: train loss: 0.0038364233914762735\n",
      "Epoch 1774: train loss: 0.0037824215833097696\n",
      "Epoch 1775: train loss: 0.0037606372497975826\n",
      "Epoch 1776: train loss: 0.003767871530726552\n",
      "Epoch 1777: train loss: 0.003788301721215248\n",
      "Epoch 1778: train loss: 0.0038119859527796507\n",
      "Epoch 1779: train loss: 0.0038082406390458345\n",
      "Epoch 1780: train loss: 0.003799615427851677\n",
      "Epoch 1781: train loss: 0.0037658195942640305\n",
      "Epoch 1782: train loss: 0.003737046616151929\n",
      "Epoch 1783: train loss: 0.003711614292114973\n",
      "Epoch 1784: train loss: 0.003697091480717063\n",
      "Epoch 1785: train loss: 0.0036921652499586344\n",
      "Epoch 1786: train loss: 0.003693563863635063\n",
      "Epoch 1787: train loss: 0.0036986980121582747\n",
      "Epoch 1788: train loss: 0.0037012218963354826\n",
      "Epoch 1789: train loss: 0.0037052335683256388\n",
      "Epoch 1790: train loss: 0.003699718276038766\n",
      "Epoch 1791: train loss: 0.003696884959936142\n",
      "Epoch 1792: train loss: 0.0036834117490798235\n",
      "Epoch 1793: train loss: 0.0036736219190061092\n",
      "Epoch 1794: train loss: 0.0036567419301718473\n",
      "Epoch 1795: train loss: 0.0036433718632906675\n",
      "Epoch 1796: train loss: 0.003627471160143614\n",
      "Epoch 1797: train loss: 0.0036143444012850523\n",
      "Epoch 1798: train loss: 0.003601426025852561\n",
      "Epoch 1799: train loss: 0.003590341890230775\n",
      "Epoch 1800: train loss: 0.003580177901312709\n",
      "Epoch 1801: train loss: 0.0035710290540009737\n",
      "Epoch 1802: train loss: 0.0035625214222818613\n",
      "Epoch 1803: train loss: 0.0035545157734304667\n",
      "Epoch 1804: train loss: 0.003546806052327156\n",
      "Epoch 1805: train loss: 0.0035393747966736555\n",
      "Epoch 1806: train loss: 0.0035321780014783144\n",
      "Epoch 1807: train loss: 0.0035252950619906187\n",
      "Epoch 1808: train loss: 0.0035190170165151358\n",
      "Epoch 1809: train loss: 0.0035135408397763968\n",
      "Epoch 1810: train loss: 0.0035100288223475218\n",
      "Epoch 1811: train loss: 0.003508735215291381\n",
      "Epoch 1812: train loss: 0.003514113137498498\n",
      "Epoch 1813: train loss: 0.0035249488428235054\n",
      "Epoch 1814: train loss: 0.0035584396682679653\n",
      "Epoch 1815: train loss: 0.0035982979461550713\n",
      "Epoch 1816: train loss: 0.003705631010234356\n",
      "Epoch 1817: train loss: 0.0037626817356795073\n",
      "Epoch 1818: train loss: 0.003941540140658617\n",
      "Epoch 1819: train loss: 0.0038377889432013035\n",
      "Epoch 1820: train loss: 0.003821671474725008\n",
      "Epoch 1821: train loss: 0.0035937416832894087\n",
      "Epoch 1822: train loss: 0.00345988548360765\n",
      "Epoch 1823: train loss: 0.0033971930388361216\n",
      "Epoch 1824: train loss: 0.0034181373193860054\n",
      "Epoch 1825: train loss: 0.003493631724268198\n",
      "Epoch 1826: train loss: 0.0035439548082649708\n",
      "Epoch 1827: train loss: 0.0035946876741945744\n",
      "Epoch 1828: train loss: 0.0035191054921597242\n",
      "Epoch 1829: train loss: 0.0034533359576016665\n",
      "Epoch 1830: train loss: 0.003372168866917491\n",
      "Epoch 1831: train loss: 0.00333230197429657\n",
      "Epoch 1832: train loss: 0.003331482643261552\n",
      "Epoch 1833: train loss: 0.0033553631510585546\n",
      "Epoch 1834: train loss: 0.0033901387359946966\n",
      "Epoch 1835: train loss: 0.003395893843844533\n",
      "Epoch 1836: train loss: 0.0033969832584261894\n",
      "Epoch 1837: train loss: 0.003355688415467739\n",
      "Epoch 1838: train loss: 0.0033191856928169727\n",
      "Epoch 1839: train loss: 0.0032807148527354\n",
      "Epoch 1840: train loss: 0.003257113741710782\n",
      "Epoch 1841: train loss: 0.003247530199587345\n",
      "Epoch 1842: train loss: 0.0032486601267009974\n",
      "Epoch 1843: train loss: 0.003256177296862006\n",
      "Epoch 1844: train loss: 0.0032614851370453835\n",
      "Epoch 1845: train loss: 0.003268645843490958\n",
      "Epoch 1846: train loss: 0.0032621975988149643\n",
      "Epoch 1847: train loss: 0.0032582601998001337\n",
      "Epoch 1848: train loss: 0.003239067504182458\n",
      "Epoch 1849: train loss: 0.003223569830879569\n",
      "Epoch 1850: train loss: 0.0032002292573451996\n",
      "Epoch 1851: train loss: 0.003180998843163252\n",
      "Epoch 1852: train loss: 0.0031618515495210886\n",
      "Epoch 1853: train loss: 0.0031464588828384876\n",
      "Epoch 1854: train loss: 0.0031339002307504416\n",
      "Epoch 1855: train loss: 0.0031240470707416534\n",
      "Epoch 1856: train loss: 0.003116284729912877\n",
      "Epoch 1857: train loss: 0.003109917277470231\n",
      "Epoch 1858: train loss: 0.003104657167568803\n",
      "Epoch 1859: train loss: 0.0030996401328593493\n",
      "Epoch 1860: train loss: 0.0030959525611251593\n",
      "Epoch 1861: train loss: 0.003091538092121482\n",
      "Epoch 1862: train loss: 0.0030898519326001406\n",
      "Epoch 1863: train loss: 0.003086162731051445\n",
      "Epoch 1864: train loss: 0.003087588818743825\n",
      "Epoch 1865: train loss: 0.0030846719164401293\n",
      "Epoch 1866: train loss: 0.0030903969891369343\n",
      "Epoch 1867: train loss: 0.003087446792051196\n",
      "Epoch 1868: train loss: 0.0030977402348071337\n",
      "Epoch 1869: train loss: 0.003092072205618024\n",
      "Epoch 1870: train loss: 0.0031041614711284637\n",
      "Epoch 1871: train loss: 0.0030906624160706997\n",
      "Epoch 1872: train loss: 0.0030968952924013138\n",
      "Epoch 1873: train loss: 0.003070980543270707\n",
      "Epoch 1874: train loss: 0.0030625909566879272\n",
      "Epoch 1875: train loss: 0.0030271150171756744\n",
      "Epoch 1876: train loss: 0.0030047749169170856\n",
      "Epoch 1877: train loss: 0.002970280358567834\n",
      "Epoch 1878: train loss: 0.002944947686046362\n",
      "Epoch 1879: train loss: 0.002919786609709263\n",
      "Epoch 1880: train loss: 0.0029006917029619217\n",
      "Epoch 1881: train loss: 0.0028853951953351498\n",
      "Epoch 1882: train loss: 0.0028737543616443872\n",
      "Epoch 1883: train loss: 0.0028648136649280787\n",
      "Epoch 1884: train loss: 0.002857859479263425\n",
      "Epoch 1885: train loss: 0.002852502977475524\n",
      "Epoch 1886: train loss: 0.002848044503480196\n",
      "Epoch 1887: train loss: 0.0028457283042371273\n",
      "Epoch 1888: train loss: 0.002843316877260804\n",
      "Epoch 1889: train loss: 0.002845542971044779\n",
      "Epoch 1890: train loss: 0.0028456556610763073\n",
      "Epoch 1891: train loss: 0.0028551395516842604\n",
      "Epoch 1892: train loss: 0.002857381012290716\n",
      "Epoch 1893: train loss: 0.0028760069981217384\n",
      "Epoch 1894: train loss: 0.002875405130907893\n",
      "Epoch 1895: train loss: 0.0028978008776903152\n",
      "Epoch 1896: train loss: 0.0028818051796406507\n",
      "Epoch 1897: train loss: 0.002889398019760847\n",
      "Epoch 1898: train loss: 0.0028487839736044407\n",
      "Epoch 1899: train loss: 0.0028263807762414217\n",
      "Epoch 1900: train loss: 0.0027753454633057117\n",
      "Epoch 1901: train loss: 0.0027383402921259403\n",
      "Epoch 1902: train loss: 0.002701336517930031\n",
      "Epoch 1903: train loss: 0.0026762185152620077\n",
      "Epoch 1904: train loss: 0.0026606391184031963\n",
      "Epoch 1905: train loss: 0.002653365256264806\n",
      "Epoch 1906: train loss: 0.002651875140145421\n",
      "Epoch 1907: train loss: 0.0026525903958827257\n",
      "Epoch 1908: train loss: 0.0026556269731372595\n",
      "Epoch 1909: train loss: 0.0026535838842391968\n",
      "Epoch 1910: train loss: 0.002653622766956687\n",
      "Epoch 1911: train loss: 0.002643283922225237\n",
      "Epoch 1912: train loss: 0.00263582868501544\n",
      "Epoch 1913: train loss: 0.002617910737171769\n",
      "Epoch 1914: train loss: 0.0026033413596451283\n",
      "Epoch 1915: train loss: 0.0025828524958342314\n",
      "Epoch 1916: train loss: 0.002565630478784442\n",
      "Epoch 1917: train loss: 0.0025472939014434814\n",
      "Epoch 1918: train loss: 0.002531577367335558\n",
      "Epoch 1919: train loss: 0.0025170105509459972\n",
      "Epoch 1920: train loss: 0.0025042507331818342\n",
      "Epoch 1921: train loss: 0.0024928280618041754\n",
      "Epoch 1922: train loss: 0.002482523676007986\n",
      "Epoch 1923: train loss: 0.002473063301295042\n",
      "Epoch 1924: train loss: 0.0024641959462314844\n",
      "Epoch 1925: train loss: 0.0024557190481573343\n",
      "Epoch 1926: train loss: 0.002447401871904731\n",
      "Epoch 1927: train loss: 0.00243943789973855\n",
      "Epoch 1928: train loss: 0.0024312702007591724\n",
      "Epoch 1929: train loss: 0.002423705067485571\n",
      "Epoch 1930: train loss: 0.0024155881255865097\n",
      "Epoch 1931: train loss: 0.002408480504527688\n",
      "Epoch 1932: train loss: 0.0024004317820072174\n",
      "Epoch 1933: train loss: 0.0023939204402267933\n",
      "Epoch 1934: train loss: 0.002385864732787013\n",
      "Epoch 1935: train loss: 0.002379981568083167\n",
      "Epoch 1936: train loss: 0.0023716671857982874\n",
      "Epoch 1937: train loss: 0.002366200787946582\n",
      "Epoch 1938: train loss: 0.002357177436351776\n",
      "Epoch 1939: train loss: 0.002351579489186406\n",
      "Epoch 1940: train loss: 0.002341244835406542\n",
      "Epoch 1941: train loss: 0.002334639662876725\n",
      "Epoch 1942: train loss: 0.0023224276956170797\n",
      "Epoch 1943: train loss: 0.0023137948010116816\n",
      "Epoch 1944: train loss: 0.0022994924802333117\n",
      "Epoch 1945: train loss: 0.0022882630582898855\n",
      "Epoch 1946: train loss: 0.0022724727168679237\n",
      "Epoch 1947: train loss: 0.002259090542793274\n",
      "Epoch 1948: train loss: 0.002243067603558302\n",
      "Epoch 1949: train loss: 0.0022288737818598747\n",
      "Epoch 1950: train loss: 0.002213901374489069\n",
      "Epoch 1951: train loss: 0.0022003105841577053\n",
      "Epoch 1952: train loss: 0.0021870869677513838\n",
      "Epoch 1953: train loss: 0.0021749124862253666\n",
      "Epoch 1954: train loss: 0.002163451164960861\n",
      "Epoch 1955: train loss: 0.002152725588530302\n",
      "Epoch 1956: train loss: 0.0021425792947411537\n",
      "Epoch 1957: train loss: 0.002132879104465246\n",
      "Epoch 1958: train loss: 0.0021235060412436724\n",
      "Epoch 1959: train loss: 0.002114302245900035\n",
      "Epoch 1960: train loss: 0.002105211839079857\n",
      "Epoch 1961: train loss: 0.0020960839465260506\n",
      "Epoch 1962: train loss: 0.002086963038891554\n",
      "Epoch 1963: train loss: 0.0020776211749762297\n",
      "Epoch 1964: train loss: 0.002068295842036605\n",
      "Epoch 1965: train loss: 0.0020586387254297733\n",
      "Epoch 1966: train loss: 0.0020490074530243874\n",
      "Epoch 1967: train loss: 0.0020390143617987633\n",
      "Epoch 1968: train loss: 0.002029088092967868\n",
      "Epoch 1969: train loss: 0.002018854022026062\n",
      "Epoch 1970: train loss: 0.0020086858421564102\n",
      "Epoch 1971: train loss: 0.001998345600441098\n",
      "Epoch 1972: train loss: 0.0019880929030478\n",
      "Epoch 1973: train loss: 0.0019777787383645773\n",
      "Epoch 1974: train loss: 0.0019675674848258495\n",
      "Epoch 1975: train loss: 0.0019574295729398727\n",
      "Epoch 1976: train loss: 0.0019473884021863341\n",
      "Epoch 1977: train loss: 0.001937476103194058\n",
      "Epoch 1978: train loss: 0.0019276620587334037\n",
      "Epoch 1979: train loss: 0.0019179770024493337\n",
      "Epoch 1980: train loss: 0.0019083749502897263\n",
      "Epoch 1981: train loss: 0.0018988626543432474\n",
      "Epoch 1982: train loss: 0.001889404491521418\n",
      "Epoch 1983: train loss: 0.0018799954559653997\n",
      "Epoch 1984: train loss: 0.0018705932889133692\n",
      "Epoch 1985: train loss: 0.0018611907726153731\n",
      "Epoch 1986: train loss: 0.001851787674240768\n",
      "Epoch 1987: train loss: 0.0018423686269670725\n",
      "Epoch 1988: train loss: 0.0018329460872337222\n",
      "Epoch 1989: train loss: 0.001823524129576981\n",
      "Epoch 1990: train loss: 0.0018141076434403658\n",
      "Epoch 1991: train loss: 0.0018047133926302195\n",
      "Epoch 1992: train loss: 0.0017953390488401055\n",
      "Epoch 1993: train loss: 0.0017860180232673883\n",
      "Epoch 1994: train loss: 0.0017767491517588496\n",
      "Epoch 1995: train loss: 0.0017675120616331697\n",
      "Epoch 1996: train loss: 0.001758327940478921\n",
      "Epoch 1997: train loss: 0.0017491912003606558\n",
      "Epoch 1998: train loss: 0.0017400833312422037\n",
      "Epoch 1999: train loss: 0.001730990712530911\n",
      "Epoch 2000: train loss: 0.0017219299916177988\n",
      "Epoch 2001: train loss: 0.0017128769541159272\n",
      "Epoch 2002: train loss: 0.0017038603546097875\n",
      "Epoch 2003: train loss: 0.0016948513220995665\n",
      "Epoch 2004: train loss: 0.001685880240984261\n",
      "Epoch 2005: train loss: 0.0016769355861470103\n",
      "Epoch 2006: train loss: 0.0016680483240634203\n",
      "Epoch 2007: train loss: 0.0016591745661571622\n",
      "Epoch 2008: train loss: 0.001650360063649714\n",
      "Epoch 2009: train loss: 0.001641581067815423\n",
      "Epoch 2010: train loss: 0.0016328172059729695\n",
      "Epoch 2011: train loss: 0.0016240969998762012\n",
      "Epoch 2012: train loss: 0.001615402172319591\n",
      "Epoch 2013: train loss: 0.001606734236702323\n",
      "Epoch 2014: train loss: 0.0015981128672137856\n",
      "Epoch 2015: train loss: 0.0015895154792815447\n",
      "Epoch 2016: train loss: 0.001580958953127265\n",
      "Epoch 2017: train loss: 0.0015724502736702561\n",
      "Epoch 2018: train loss: 0.001563969999551773\n",
      "Epoch 2019: train loss: 0.0015555216232314706\n",
      "Epoch 2020: train loss: 0.001547125051729381\n",
      "Epoch 2021: train loss: 0.0015387394232675433\n",
      "Epoch 2022: train loss: 0.0015304051339626312\n",
      "Epoch 2023: train loss: 0.0015221035573631525\n",
      "Epoch 2024: train loss: 0.0015138358576223254\n",
      "Epoch 2025: train loss: 0.0015056267147883773\n",
      "Epoch 2026: train loss: 0.001497433753684163\n",
      "Epoch 2027: train loss: 0.0014893042389303446\n",
      "Epoch 2028: train loss: 0.0014811933506280184\n",
      "Epoch 2029: train loss: 0.001473127049393952\n",
      "Epoch 2030: train loss: 0.0014650922967121005\n",
      "Epoch 2031: train loss: 0.0014571091160178185\n",
      "Epoch 2032: train loss: 0.0014491555048152804\n",
      "Epoch 2033: train loss: 0.0014412490418180823\n",
      "Epoch 2034: train loss: 0.001433383789844811\n",
      "Epoch 2035: train loss: 0.0014255569549277425\n",
      "Epoch 2036: train loss: 0.0014177609700709581\n",
      "Epoch 2037: train loss: 0.001410017372108996\n",
      "Epoch 2038: train loss: 0.0014023177791386843\n",
      "Epoch 2039: train loss: 0.001394652179442346\n",
      "Epoch 2040: train loss: 0.001387026160955429\n",
      "Epoch 2041: train loss: 0.0013794497353956103\n",
      "Epoch 2042: train loss: 0.0013719076523557305\n",
      "Epoch 2043: train loss: 0.0013644108548760414\n",
      "Epoch 2044: train loss: 0.0013569489819929004\n",
      "Epoch 2045: train loss: 0.0013495380990207195\n",
      "Epoch 2046: train loss: 0.0013421636540442705\n",
      "Epoch 2047: train loss: 0.0013348405482247472\n",
      "Epoch 2048: train loss: 0.0013275508536025882\n",
      "Epoch 2049: train loss: 0.0013203057460486889\n",
      "Epoch 2050: train loss: 0.0013130968436598778\n",
      "Epoch 2051: train loss: 0.0013059418415650725\n",
      "Epoch 2052: train loss: 0.0012988243252038956\n",
      "Epoch 2053: train loss: 0.001291746972128749\n",
      "Epoch 2054: train loss: 0.0012847110629081726\n",
      "Epoch 2055: train loss: 0.0012777260271832347\n",
      "Epoch 2056: train loss: 0.0012707835994660854\n",
      "Epoch 2057: train loss: 0.0012638787738978863\n",
      "Epoch 2058: train loss: 0.0012570183025673032\n",
      "Epoch 2059: train loss: 0.0012502025347203016\n",
      "Epoch 2060: train loss: 0.0012434327509254217\n",
      "Epoch 2061: train loss: 0.0012366942828521132\n",
      "Epoch 2062: train loss: 0.0012300079688429832\n",
      "Epoch 2063: train loss: 0.0012233586749061942\n",
      "Epoch 2064: train loss: 0.00121676130220294\n",
      "Epoch 2065: train loss: 0.0012102052569389343\n",
      "Epoch 2066: train loss: 0.0012036955449730158\n",
      "Epoch 2067: train loss: 0.0011972193606197834\n",
      "Epoch 2068: train loss: 0.0011907834559679031\n",
      "Epoch 2069: train loss: 0.0011843974934890866\n",
      "Epoch 2070: train loss: 0.0011780590284615755\n",
      "Epoch 2071: train loss: 0.0011717553716152906\n",
      "Epoch 2072: train loss: 0.0011654983973130584\n",
      "Epoch 2073: train loss: 0.001159277162514627\n",
      "Epoch 2074: train loss: 0.0011531083146110177\n",
      "Epoch 2075: train loss: 0.0011469866149127483\n",
      "Epoch 2076: train loss: 0.00114089110866189\n",
      "Epoch 2077: train loss: 0.0011348474072292447\n",
      "Epoch 2078: train loss: 0.001128843636251986\n",
      "Epoch 2079: train loss: 0.0011228795628994703\n",
      "Epoch 2080: train loss: 0.0011169648496434093\n",
      "Epoch 2081: train loss: 0.0011110901832580566\n",
      "Epoch 2082: train loss: 0.0011052540503442287\n",
      "Epoch 2083: train loss: 0.001099460874684155\n",
      "Epoch 2084: train loss: 0.0010937051847577095\n",
      "Epoch 2085: train loss: 0.0010880023473873734\n",
      "Epoch 2086: train loss: 0.0010823254706338048\n",
      "Epoch 2087: train loss: 0.0010767009807750583\n",
      "Epoch 2088: train loss: 0.0010711171198636293\n",
      "Epoch 2089: train loss: 0.0010655672522261739\n",
      "Epoch 2090: train loss: 0.001060064067132771\n",
      "Epoch 2091: train loss: 0.0010545972036197782\n",
      "Epoch 2092: train loss: 0.001049170270562172\n",
      "Epoch 2093: train loss: 0.0010437850141897798\n",
      "Epoch 2094: train loss: 0.001038441900163889\n",
      "Epoch 2095: train loss: 0.0010331458179280162\n",
      "Epoch 2096: train loss: 0.0010278677800670266\n",
      "Epoch 2097: train loss: 0.001022647018544376\n",
      "Epoch 2098: train loss: 0.0010174592025578022\n",
      "Epoch 2099: train loss: 0.0010123149259015918\n",
      "Epoch 2100: train loss: 0.00100719986949116\n",
      "Epoch 2101: train loss: 0.0010021321941167116\n",
      "Epoch 2102: train loss: 0.0009970987448468804\n",
      "Epoch 2103: train loss: 0.000992104527540505\n",
      "Epoch 2104: train loss: 0.0009871484944596887\n",
      "Epoch 2105: train loss: 0.0009822275023907423\n",
      "Epoch 2106: train loss: 0.0009773469064384699\n",
      "Epoch 2107: train loss: 0.0009725004201754928\n",
      "Epoch 2108: train loss: 0.0009676927584223449\n",
      "Epoch 2109: train loss: 0.0009629259584471583\n",
      "Epoch 2110: train loss: 0.0009581915801391006\n",
      "Epoch 2111: train loss: 0.0009534904384054244\n",
      "Epoch 2112: train loss: 0.0009488228242844343\n",
      "Epoch 2113: train loss: 0.0009441978181712329\n",
      "Epoch 2114: train loss: 0.0009396050591021776\n",
      "Epoch 2115: train loss: 0.0009350472246296704\n",
      "Epoch 2116: train loss: 0.0009305270505137742\n",
      "Epoch 2117: train loss: 0.0009260321967303753\n",
      "Epoch 2118: train loss: 0.0009215780883096159\n",
      "Epoch 2119: train loss: 0.0009171633282676339\n",
      "Epoch 2120: train loss: 0.0009127755765803158\n",
      "Epoch 2121: train loss: 0.000908427988179028\n",
      "Epoch 2122: train loss: 0.0009041060111485422\n",
      "Epoch 2123: train loss: 0.0008998186676762998\n",
      "Epoch 2124: train loss: 0.0008955645025707781\n",
      "Epoch 2125: train loss: 0.0008913479396142066\n",
      "Epoch 2126: train loss: 0.0008871553000062704\n",
      "Epoch 2127: train loss: 0.0008829972939565778\n",
      "Epoch 2128: train loss: 0.0008788704872131348\n",
      "Epoch 2129: train loss: 0.0008747733663767576\n",
      "Epoch 2130: train loss: 0.0008707080851309001\n",
      "Epoch 2131: train loss: 0.0008666776702739298\n",
      "Epoch 2132: train loss: 0.0008626733906567097\n",
      "Epoch 2133: train loss: 0.0008587019401602447\n",
      "Epoch 2134: train loss: 0.00085475854575634\n",
      "Epoch 2135: train loss: 0.0008508441969752312\n",
      "Epoch 2136: train loss: 0.0008469597087241709\n",
      "Epoch 2137: train loss: 0.0008431082242168486\n",
      "Epoch 2138: train loss: 0.0008392804302275181\n",
      "Epoch 2139: train loss: 0.000835486629512161\n",
      "Epoch 2140: train loss: 0.0008317121537402272\n",
      "Epoch 2141: train loss: 0.0008279717294499278\n",
      "Epoch 2142: train loss: 0.0008242528419941664\n",
      "Epoch 2143: train loss: 0.0008205705089494586\n",
      "Epoch 2144: train loss: 0.0008169112261384726\n",
      "Epoch 2145: train loss: 0.0008132780203595757\n",
      "Epoch 2146: train loss: 0.0008096701349131763\n",
      "Epoch 2147: train loss: 0.000806096417363733\n",
      "Epoch 2148: train loss: 0.0008025366114452481\n",
      "Epoch 2149: train loss: 0.0007990080630406737\n",
      "Epoch 2150: train loss: 0.0007955085020512342\n",
      "Epoch 2151: train loss: 0.0007920245989225805\n",
      "Epoch 2152: train loss: 0.0007885767845436931\n",
      "Epoch 2153: train loss: 0.0007851503323763609\n",
      "Epoch 2154: train loss: 0.0007817451260052621\n",
      "Epoch 2155: train loss: 0.0007783686742186546\n",
      "Epoch 2156: train loss: 0.0007750113727524877\n",
      "Epoch 2157: train loss: 0.0007716828840784729\n",
      "Epoch 2158: train loss: 0.0007683746516704559\n",
      "Epoch 2159: train loss: 0.0007650932529941201\n",
      "Epoch 2160: train loss: 0.0007618279196321964\n",
      "Epoch 2161: train loss: 0.0007585911662317812\n",
      "Epoch 2162: train loss: 0.0007553732139058411\n",
      "Epoch 2163: train loss: 0.0007521809893660247\n",
      "Epoch 2164: train loss: 0.0007490093703381717\n",
      "Epoch 2165: train loss: 0.0007458610343746841\n",
      "Epoch 2166: train loss: 0.0007427356904372573\n",
      "Epoch 2167: train loss: 0.000739626819267869\n",
      "Epoch 2168: train loss: 0.0007365417550317943\n",
      "Epoch 2169: train loss: 0.0007334788097068667\n",
      "Epoch 2170: train loss: 0.0007304320461116731\n",
      "Epoch 2171: train loss: 0.0007274114759638906\n",
      "Epoch 2172: train loss: 0.0007244128501042724\n",
      "Epoch 2173: train loss: 0.0007214252837002277\n",
      "Epoch 2174: train loss: 0.0007184654241427779\n",
      "Epoch 2175: train loss: 0.000715525820851326\n",
      "Epoch 2176: train loss: 0.0007126016425900161\n",
      "Epoch 2177: train loss: 0.000709699175786227\n",
      "Epoch 2178: train loss: 0.0007068170816637576\n",
      "Epoch 2179: train loss: 0.0007039475603960454\n",
      "Epoch 2180: train loss: 0.0007011028937995434\n",
      "Epoch 2181: train loss: 0.0006982727791182697\n",
      "Epoch 2182: train loss: 0.000695462164003402\n",
      "Epoch 2183: train loss: 0.000692673958837986\n",
      "Epoch 2184: train loss: 0.0006898971041664481\n",
      "Epoch 2185: train loss: 0.0006871462101116776\n",
      "Epoch 2186: train loss: 0.0006844023591838777\n",
      "Epoch 2187: train loss: 0.0006816853419877589\n",
      "Epoch 2188: train loss: 0.0006789828767068684\n",
      "Epoch 2189: train loss: 0.0006762949633412063\n",
      "Epoch 2190: train loss: 0.0006736254435963929\n",
      "Epoch 2191: train loss: 0.0006709761219099164\n",
      "Epoch 2192: train loss: 0.0006683344254270196\n",
      "Epoch 2193: train loss: 0.0006657176418229938\n",
      "Epoch 2194: train loss: 0.0006631151190958917\n",
      "Epoch 2195: train loss: 0.00066052854526788\n",
      "Epoch 2196: train loss: 0.0006579598993994296\n",
      "Epoch 2197: train loss: 0.0006554023129865527\n",
      "Epoch 2198: train loss: 0.0006528656231239438\n",
      "Epoch 2199: train loss: 0.0006503348122350872\n",
      "Epoch 2200: train loss: 0.0006478335708379745\n",
      "Epoch 2201: train loss: 0.0006453397800214589\n",
      "Epoch 2202: train loss: 0.000642859551589936\n",
      "Epoch 2203: train loss: 0.0006403974257409573\n",
      "Epoch 2204: train loss: 0.0006379445549100637\n",
      "Epoch 2205: train loss: 0.0006355161312967539\n",
      "Epoch 2206: train loss: 0.0006330946343950927\n",
      "Epoch 2207: train loss: 0.0006306893774308264\n",
      "Epoch 2208: train loss: 0.0006282965186983347\n",
      "Epoch 2209: train loss: 0.0006259198999032378\n",
      "Epoch 2210: train loss: 0.0006235556211322546\n",
      "Epoch 2211: train loss: 0.0006212107837200165\n",
      "Epoch 2212: train loss: 0.0006188766565173864\n",
      "Epoch 2213: train loss: 0.0006165520171634853\n",
      "Epoch 2214: train loss: 0.000614242977462709\n",
      "Epoch 2215: train loss: 0.0006119449390098453\n",
      "Epoch 2216: train loss: 0.0006096667493693531\n",
      "Epoch 2217: train loss: 0.0006073972908779979\n",
      "Epoch 2218: train loss: 0.0006051445379853249\n",
      "Epoch 2219: train loss: 0.0006028995267115533\n",
      "Epoch 2220: train loss: 0.0006006727344356477\n",
      "Epoch 2221: train loss: 0.0005984556628391147\n",
      "Epoch 2222: train loss: 0.000596250465605408\n",
      "Epoch 2223: train loss: 0.0005940546980127692\n",
      "Epoch 2224: train loss: 0.0005918769165873528\n",
      "Epoch 2225: train loss: 0.0005897059454582632\n",
      "Epoch 2226: train loss: 0.0005875510978512466\n",
      "Epoch 2227: train loss: 0.0005854094633832574\n",
      "Epoch 2228: train loss: 0.0005832742899656296\n",
      "Epoch 2229: train loss: 0.0005811552400700748\n",
      "Epoch 2230: train loss: 0.0005790477734990418\n",
      "Epoch 2231: train loss: 0.0005769506678916514\n",
      "Epoch 2232: train loss: 0.0005748619441874325\n",
      "Epoch 2233: train loss: 0.0005727916141040623\n",
      "Epoch 2234: train loss: 0.0005707288510166109\n",
      "Epoch 2235: train loss: 0.0005686772055923939\n",
      "Epoch 2236: train loss: 0.0005666371434926987\n",
      "Epoch 2237: train loss: 0.0005646067438647151\n",
      "Epoch 2238: train loss: 0.0005625850171782076\n",
      "Epoch 2239: train loss: 0.0005605801125057042\n",
      "Epoch 2240: train loss: 0.0005585799226537347\n",
      "Epoch 2241: train loss: 0.0005565963219851255\n",
      "Epoch 2242: train loss: 0.0005546189495362341\n",
      "Epoch 2243: train loss: 0.0005526531604118645\n",
      "Epoch 2244: train loss: 0.0005506968009285629\n",
      "Epoch 2245: train loss: 0.0005487501621246338\n",
      "Epoch 2246: train loss: 0.0005468189483508468\n",
      "Epoch 2247: train loss: 0.0005448926822282374\n",
      "Epoch 2248: train loss: 0.0005429779994301498\n",
      "Epoch 2249: train loss: 0.0005410755984485149\n",
      "Epoch 2250: train loss: 0.0005391811719164252\n",
      "Epoch 2251: train loss: 0.0005372948944568634\n",
      "Epoch 2252: train loss: 0.0005354181630536914\n",
      "Epoch 2253: train loss: 0.000533555168658495\n",
      "Epoch 2254: train loss: 0.0005316982860676944\n",
      "Epoch 2255: train loss: 0.0005298501928336918\n",
      "Epoch 2256: train loss: 0.0005280151963233948\n",
      "Epoch 2257: train loss: 0.0005261857295408845\n",
      "Epoch 2258: train loss: 0.0005243677296675742\n",
      "Epoch 2259: train loss: 0.0005225585191510618\n",
      "Epoch 2260: train loss: 0.0005207600770518184\n",
      "Epoch 2261: train loss: 0.0005189655348658562\n",
      "Epoch 2262: train loss: 0.0005171860684640706\n",
      "Epoch 2263: train loss: 0.0005154130631126463\n",
      "Epoch 2264: train loss: 0.0005136493127793074\n",
      "Epoch 2265: train loss: 0.0005118896369822323\n",
      "Epoch 2266: train loss: 0.0005101431161165237\n",
      "Epoch 2267: train loss: 0.0005084066069684923\n",
      "Epoch 2268: train loss: 0.0005066751036792994\n",
      "Epoch 2269: train loss: 0.0005049547180533409\n",
      "Epoch 2270: train loss: 0.0005032428307458758\n",
      "Epoch 2271: train loss: 0.0005015393253415823\n",
      "Epoch 2272: train loss: 0.0004998435615561903\n",
      "Epoch 2273: train loss: 0.0004981550155207515\n",
      "Epoch 2274: train loss: 0.0004964766558259726\n",
      "Epoch 2275: train loss: 0.0004948062705807388\n",
      "Epoch 2276: train loss: 0.0004931444418616593\n",
      "Epoch 2277: train loss: 0.0004914854071103036\n",
      "Epoch 2278: train loss: 0.0004898374900221825\n",
      "Epoch 2279: train loss: 0.00048820211668498814\n",
      "Epoch 2280: train loss: 0.0004865693917963654\n",
      "Epoch 2281: train loss: 0.00048494775546714664\n",
      "Epoch 2282: train loss: 0.0004833306011278182\n",
      "Epoch 2283: train loss: 0.0004817221488337964\n",
      "Epoch 2284: train loss: 0.00048012472689151764\n",
      "Epoch 2285: train loss: 0.0004785275086760521\n",
      "Epoch 2286: train loss: 0.00047694527893327177\n",
      "Epoch 2287: train loss: 0.0004753693356178701\n",
      "Epoch 2288: train loss: 0.00047379813622683287\n",
      "Epoch 2289: train loss: 0.0004722328740172088\n",
      "Epoch 2290: train loss: 0.00047067945706658065\n",
      "Epoch 2291: train loss: 0.000469130725832656\n",
      "Epoch 2292: train loss: 0.00046759037650190294\n",
      "Epoch 2293: train loss: 0.00046605378156527877\n",
      "Epoch 2294: train loss: 0.0004645285662263632\n",
      "Epoch 2295: train loss: 0.0004630102193914354\n",
      "Epoch 2296: train loss: 0.0004614990612026304\n",
      "Epoch 2297: train loss: 0.00045999258873052895\n",
      "Epoch 2298: train loss: 0.0004584935668390244\n",
      "Epoch 2299: train loss: 0.0004570022865664214\n",
      "Epoch 2300: train loss: 0.00045551679795607924\n",
      "Epoch 2301: train loss: 0.00045403940021060407\n",
      "Epoch 2302: train loss: 0.000452568317996338\n",
      "Epoch 2303: train loss: 0.0004511036677286029\n",
      "Epoch 2304: train loss: 0.0004496477486100048\n",
      "Epoch 2305: train loss: 0.0004481963987927884\n",
      "Epoch 2306: train loss: 0.00044675153912976384\n",
      "Epoch 2307: train loss: 0.0004453131405171007\n",
      "Epoch 2308: train loss: 0.00044388516107574105\n",
      "Epoch 2309: train loss: 0.00044245715253055096\n",
      "Epoch 2310: train loss: 0.00044103938853368163\n",
      "Epoch 2311: train loss: 0.0004396284930408001\n",
      "Epoch 2312: train loss: 0.0004382254555821419\n",
      "Epoch 2313: train loss: 0.00043682302930392325\n",
      "Epoch 2314: train loss: 0.0004354290431365371\n",
      "Epoch 2315: train loss: 0.0004340386076364666\n",
      "Epoch 2316: train loss: 0.0004326631024014205\n",
      "Epoch 2317: train loss: 0.0004312879464123398\n",
      "Epoch 2318: train loss: 0.00042991837835870683\n",
      "Epoch 2319: train loss: 0.00042855553328990936\n",
      "Epoch 2320: train loss: 0.00042720031342469156\n",
      "Epoch 2321: train loss: 0.0004258508561179042\n",
      "Epoch 2322: train loss: 0.0004245077143423259\n",
      "Epoch 2323: train loss: 0.00042316754115745425\n",
      "Epoch 2324: train loss: 0.0004218359536025673\n",
      "Epoch 2325: train loss: 0.00042051158379763365\n",
      "Epoch 2326: train loss: 0.00041918843635357916\n",
      "Epoch 2327: train loss: 0.0004178741364739835\n",
      "Epoch 2328: train loss: 0.0004165667050983757\n",
      "Epoch 2329: train loss: 0.00041526215500198305\n",
      "Epoch 2330: train loss: 0.00041396310552954674\n",
      "Epoch 2331: train loss: 0.000412670720834285\n",
      "Epoch 2332: train loss: 0.00041138436063192785\n",
      "Epoch 2333: train loss: 0.00041010387940332294\n",
      "Epoch 2334: train loss: 0.0004088295972906053\n",
      "Epoch 2335: train loss: 0.0004075590113643557\n",
      "Epoch 2336: train loss: 0.00040629543946124613\n",
      "Epoch 2337: train loss: 0.0004050366987939924\n",
      "Epoch 2338: train loss: 0.00040378293488174677\n",
      "Epoch 2339: train loss: 0.0004025362723041326\n",
      "Epoch 2340: train loss: 0.0004012934223283082\n",
      "Epoch 2341: train loss: 0.0004000558110419661\n",
      "Epoch 2342: train loss: 0.0003988230600953102\n",
      "Epoch 2343: train loss: 0.00039759487845003605\n",
      "Epoch 2344: train loss: 0.0003963755734730512\n",
      "Epoch 2345: train loss: 0.0003951579565182328\n",
      "Epoch 2346: train loss: 0.00039394680061377585\n",
      "Epoch 2347: train loss: 0.00039274172740988433\n",
      "Epoch 2348: train loss: 0.00039153918623924255\n",
      "Epoch 2349: train loss: 0.00039034473593346775\n",
      "Epoch 2350: train loss: 0.00038915424374863505\n",
      "Epoch 2351: train loss: 0.00038796980516053736\n",
      "Epoch 2352: train loss: 0.00038678827695548534\n",
      "Epoch 2353: train loss: 0.000385612016543746\n",
      "Epoch 2354: train loss: 0.00038444175152108073\n",
      "Epoch 2355: train loss: 0.0003832756483461708\n",
      "Epoch 2356: train loss: 0.0003821139980573207\n",
      "Epoch 2357: train loss: 0.00038095974014140666\n",
      "Epoch 2358: train loss: 0.00037980868364684284\n",
      "Epoch 2359: train loss: 0.0003786632732953876\n",
      "Epoch 2360: train loss: 0.00037752219941467047\n",
      "Epoch 2361: train loss: 0.00037638345384038985\n",
      "Epoch 2362: train loss: 0.0003752534103114158\n",
      "Epoch 2363: train loss: 0.0003741245891433209\n",
      "Epoch 2364: train loss: 0.000373004877474159\n",
      "Epoch 2365: train loss: 0.0003718841471709311\n",
      "Epoch 2366: train loss: 0.0003707720316015184\n",
      "Epoch 2367: train loss: 0.00036966329207643867\n",
      "Epoch 2368: train loss: 0.00036855804501101375\n",
      "Epoch 2369: train loss: 0.00036745902616530657\n",
      "Epoch 2370: train loss: 0.00036636495497077703\n",
      "Epoch 2371: train loss: 0.0003652751329354942\n",
      "Epoch 2372: train loss: 0.00036418999661691487\n",
      "Epoch 2373: train loss: 0.000363108585588634\n",
      "Epoch 2374: train loss: 0.0003620317729655653\n",
      "Epoch 2375: train loss: 0.0003609582199715078\n",
      "Epoch 2376: train loss: 0.00035989045863971114\n",
      "Epoch 2377: train loss: 0.00035882784868590534\n",
      "Epoch 2378: train loss: 0.00035776864388026297\n",
      "Epoch 2379: train loss: 0.00035671330988407135\n",
      "Epoch 2380: train loss: 0.0003556619631126523\n",
      "Epoch 2381: train loss: 0.00035461550578475\n",
      "Epoch 2382: train loss: 0.0003535727155394852\n",
      "Epoch 2383: train loss: 0.00035253583337180316\n",
      "Epoch 2384: train loss: 0.00035150154144503176\n",
      "Epoch 2385: train loss: 0.00035047344863414764\n",
      "Epoch 2386: train loss: 0.00034944695653393865\n",
      "Epoch 2387: train loss: 0.00034842401510104537\n",
      "Epoch 2388: train loss: 0.00034741099807433784\n",
      "Epoch 2389: train loss: 0.00034639457589946687\n",
      "Epoch 2390: train loss: 0.0003453864483162761\n",
      "Epoch 2391: train loss: 0.0003443831519689411\n",
      "Epoch 2392: train loss: 0.0003433840174693614\n",
      "Epoch 2393: train loss: 0.0003423843299970031\n",
      "Epoch 2394: train loss: 0.000341391860274598\n",
      "Epoch 2395: train loss: 0.0003404041053727269\n",
      "Epoch 2396: train loss: 0.0003394176601432264\n",
      "Epoch 2397: train loss: 0.0003384389856364578\n",
      "Epoch 2398: train loss: 0.0003374622610863298\n",
      "Epoch 2399: train loss: 0.0003364885924383998\n",
      "Epoch 2400: train loss: 0.0003355210355948657\n",
      "Epoch 2401: train loss: 0.0003345550212543458\n",
      "Epoch 2402: train loss: 0.0003335945657454431\n",
      "Epoch 2403: train loss: 0.00033263672958128154\n",
      "Epoch 2404: train loss: 0.00033168256049975753\n",
      "Epoch 2405: train loss: 0.0003307336301077157\n",
      "Epoch 2406: train loss: 0.0003297888033557683\n",
      "Epoch 2407: train loss: 0.0003288462175987661\n",
      "Epoch 2408: train loss: 0.0003279093070887029\n",
      "Epoch 2409: train loss: 0.00032697050482966006\n",
      "Epoch 2410: train loss: 0.0003260417142882943\n",
      "Epoch 2411: train loss: 0.0003251145244576037\n",
      "Epoch 2412: train loss: 0.0003241911472287029\n",
      "Epoch 2413: train loss: 0.00032327062217518687\n",
      "Epoch 2414: train loss: 0.000322353356750682\n",
      "Epoch 2415: train loss: 0.00032144133001565933\n",
      "Epoch 2416: train loss: 0.00032053355243988335\n",
      "Epoch 2417: train loss: 0.00031962612411007285\n",
      "Epoch 2418: train loss: 0.00031872454565018415\n",
      "Epoch 2419: train loss: 0.0003178256447426975\n",
      "Epoch 2420: train loss: 0.0003169313713442534\n",
      "Epoch 2421: train loss: 0.0003160399501211941\n",
      "Epoch 2422: train loss: 0.00031515254522673786\n",
      "Epoch 2423: train loss: 0.00031426851637661457\n",
      "Epoch 2424: train loss: 0.000313386699417606\n",
      "Epoch 2425: train loss: 0.0003125110233668238\n",
      "Epoch 2426: train loss: 0.0003116359584964812\n",
      "Epoch 2427: train loss: 0.0003107640368398279\n",
      "Epoch 2428: train loss: 0.0003098981687799096\n",
      "Epoch 2429: train loss: 0.0003090333193540573\n",
      "Epoch 2430: train loss: 0.00030817490187473595\n",
      "Epoch 2431: train loss: 0.0003073164843954146\n",
      "Epoch 2432: train loss: 0.0003064630145672709\n",
      "Epoch 2433: train loss: 0.0003056121349800378\n",
      "Epoch 2434: train loss: 0.00030476576648652554\n",
      "Epoch 2435: train loss: 0.0003039211733266711\n",
      "Epoch 2436: train loss: 0.000303081440506503\n",
      "Epoch 2437: train loss: 0.0003022433666046709\n",
      "Epoch 2438: train loss: 0.000301410851534456\n",
      "Epoch 2439: train loss: 0.00030057967524044216\n",
      "Epoch 2440: train loss: 0.00029975181678310037\n",
      "Epoch 2441: train loss: 0.0002989289059769362\n",
      "Epoch 2442: train loss: 0.00029810567502863705\n",
      "Epoch 2443: train loss: 0.0002972878864966333\n",
      "Epoch 2444: train loss: 0.0002964728628285229\n",
      "Epoch 2445: train loss: 0.000295660545816645\n",
      "Epoch 2446: train loss: 0.0002948534383904189\n",
      "Epoch 2447: train loss: 0.0002940476406365633\n",
      "Epoch 2448: train loss: 0.0002932455681730062\n",
      "Epoch 2449: train loss: 0.00029244323377497494\n",
      "Epoch 2450: train loss: 0.00029164942679926753\n",
      "Epoch 2451: train loss: 0.0002908564347308129\n",
      "Epoch 2452: train loss: 0.00029006547993049026\n",
      "Epoch 2453: train loss: 0.00028928223764523864\n",
      "Epoch 2454: train loss: 0.0002884923596866429\n",
      "Epoch 2455: train loss: 0.00028771371580660343\n",
      "Epoch 2456: train loss: 0.00028693649801425636\n",
      "Epoch 2457: train loss: 0.0002861615503206849\n",
      "Epoch 2458: train loss: 0.00028538747574202716\n",
      "Epoch 2459: train loss: 0.0002846189890988171\n",
      "Epoch 2460: train loss: 0.0002838516666088253\n",
      "Epoch 2461: train loss: 0.00028309051413089037\n",
      "Epoch 2462: train loss: 0.0002823268296197057\n",
      "Epoch 2463: train loss: 0.00028157143970020115\n",
      "Epoch 2464: train loss: 0.00028081584605388343\n",
      "Epoch 2465: train loss: 0.00028006514185108244\n",
      "Epoch 2466: train loss: 0.0002793158346321434\n",
      "Epoch 2467: train loss: 0.00027856850647367537\n",
      "Epoch 2468: train loss: 0.0002778239140752703\n",
      "Epoch 2469: train loss: 0.0002770841238088906\n",
      "Epoch 2470: train loss: 0.00027634791331365705\n",
      "Epoch 2471: train loss: 0.0002756117028184235\n",
      "Epoch 2472: train loss: 0.0002748817205429077\n",
      "Epoch 2473: train loss: 0.00027414996293373406\n",
      "Epoch 2474: train loss: 0.0002734239387791604\n",
      "Epoch 2475: train loss: 0.0002727003884501755\n",
      "Epoch 2476: train loss: 0.00027197800227440894\n",
      "Epoch 2477: train loss: 0.0002712624263949692\n",
      "Epoch 2478: train loss: 0.0002705444348976016\n",
      "Epoch 2479: train loss: 0.0002698326134122908\n",
      "Epoch 2480: train loss: 0.0002691224217414856\n",
      "Epoch 2481: train loss: 0.00026841432554647326\n",
      "Epoch 2482: train loss: 0.0002677094889804721\n",
      "Epoch 2483: train loss: 0.00026700805756263435\n",
      "Epoch 2484: train loss: 0.0002663085178937763\n",
      "Epoch 2485: train loss: 0.0002656098804436624\n",
      "Epoch 2486: train loss: 0.00026491578319109976\n",
      "Epoch 2487: train loss: 0.0002642243343871087\n",
      "Epoch 2488: train loss: 0.00026353320572525263\n",
      "Epoch 2489: train loss: 0.00026284559862688184\n",
      "Epoch 2490: train loss: 0.0002621623279992491\n",
      "Epoch 2491: train loss: 0.0002614822005853057\n",
      "Epoch 2492: train loss: 0.00026080323732458055\n",
      "Epoch 2493: train loss: 0.0002601246815174818\n",
      "Epoch 2494: train loss: 0.00025945049128495157\n",
      "Epoch 2495: train loss: 0.0002587797353044152\n",
      "Epoch 2496: train loss: 0.0002581090375315398\n",
      "Epoch 2497: train loss: 0.00025744433514773846\n",
      "Epoch 2498: train loss: 0.00025678036035969853\n",
      "Epoch 2499: train loss: 0.0002561183355282992\n",
      "Epoch 2500: train loss: 0.00025545997777953744\n",
      "Epoch 2501: train loss: 0.00025480444310232997\n",
      "Epoch 2502: train loss: 0.00025414882111363113\n",
      "Epoch 2503: train loss: 0.00025349733186885715\n",
      "Epoch 2504: train loss: 0.000252847617957741\n",
      "Epoch 2505: train loss: 0.0002522012509871274\n",
      "Epoch 2506: train loss: 0.00025155406910926104\n",
      "Epoch 2507: train loss: 0.0002509123587515205\n",
      "Epoch 2508: train loss: 0.00025027149240486324\n",
      "Epoch 2509: train loss: 0.00024963554460555315\n",
      "Epoch 2510: train loss: 0.0002490011102054268\n",
      "Epoch 2511: train loss: 0.0002483665884938091\n",
      "Epoch 2512: train loss: 0.00024773780023679137\n",
      "Epoch 2513: train loss: 0.00024710892466828227\n",
      "Epoch 2514: train loss: 0.00024648444377817214\n",
      "Epoch 2515: train loss: 0.00024585967184975743\n",
      "Epoch 2516: train loss: 0.00024523818865418434\n",
      "Epoch 2517: train loss: 0.00024461900466121733\n",
      "Epoch 2518: train loss: 0.0002440019597997889\n",
      "Epoch 2519: train loss: 0.0002433887857478112\n",
      "Epoch 2520: train loss: 0.00024277264310512692\n",
      "Epoch 2521: train loss: 0.00024216547899413854\n",
      "Epoch 2522: train loss: 0.00024155800929293036\n",
      "Epoch 2523: train loss: 0.00024095216940622777\n",
      "Epoch 2524: train loss: 0.00024034833768382668\n",
      "Epoch 2525: train loss: 0.0002397473726887256\n",
      "Epoch 2526: train loss: 0.00023914672783575952\n",
      "Epoch 2527: train loss: 0.00023855007020756602\n",
      "Epoch 2528: train loss: 0.00023795677407179028\n",
      "Epoch 2529: train loss: 0.0002373663301113993\n",
      "Epoch 2530: train loss: 0.00023677341232541949\n",
      "Epoch 2531: train loss: 0.00023618715931661427\n",
      "Epoch 2532: train loss: 0.0002355983160668984\n",
      "Epoch 2533: train loss: 0.00023501587565988302\n",
      "Epoch 2534: train loss: 0.00023443430836778134\n",
      "Epoch 2535: train loss: 0.0002338549675187096\n",
      "Epoch 2536: train loss: 0.00023327660164795816\n",
      "Epoch 2537: train loss: 0.00023270172823686153\n",
      "Epoch 2538: train loss: 0.0002321278880117461\n",
      "Epoch 2539: train loss: 0.0002315573365194723\n",
      "Epoch 2540: train loss: 0.000230986435781233\n",
      "Epoch 2541: train loss: 0.00023042051179800183\n",
      "Epoch 2542: train loss: 0.00022985416580922902\n",
      "Epoch 2543: train loss: 0.00022929327678866684\n",
      "Epoch 2544: train loss: 0.0002287310198880732\n",
      "Epoch 2545: train loss: 0.00022817197896074504\n",
      "Epoch 2546: train loss: 0.00022761366562917829\n",
      "Epoch 2547: train loss: 0.00022705778246745467\n",
      "Epoch 2548: train loss: 0.00022650565369985998\n",
      "Epoch 2549: train loss: 0.00022595601330976933\n",
      "Epoch 2550: train loss: 0.00022540574718732387\n",
      "Epoch 2551: train loss: 0.00022485879890155047\n",
      "Epoch 2552: train loss: 0.00022431444085668772\n",
      "Epoch 2553: train loss: 0.00022377067944034934\n",
      "Epoch 2554: train loss: 0.0002232291008112952\n",
      "Epoch 2555: train loss: 0.00022269001055974513\n",
      "Epoch 2556: train loss: 0.0002221525792265311\n",
      "Epoch 2557: train loss: 0.000221617694478482\n",
      "Epoch 2558: train loss: 0.00022108209668658674\n",
      "Epoch 2559: train loss: 0.0002205531345680356\n",
      "Epoch 2560: train loss: 0.0002200212620664388\n",
      "Epoch 2561: train loss: 0.0002194948901887983\n",
      "Epoch 2562: train loss: 0.00021896706311963499\n",
      "Epoch 2563: train loss: 0.00021844296134077013\n",
      "Epoch 2564: train loss: 0.00021792056213598698\n",
      "Epoch 2565: train loss: 0.0002174002438550815\n",
      "Epoch 2566: train loss: 0.00021687994012609124\n",
      "Epoch 2567: train loss: 0.00021636740711983293\n",
      "Epoch 2568: train loss: 0.0002158509159926325\n",
      "Epoch 2569: train loss: 0.00021533806284423918\n",
      "Epoch 2570: train loss: 0.00021482649026438594\n",
      "Epoch 2571: train loss: 0.00021431819186545908\n",
      "Epoch 2572: train loss: 0.0002138096169801429\n",
      "Epoch 2573: train loss: 0.00021330414165277034\n",
      "Epoch 2574: train loss: 0.00021279958309605718\n",
      "Epoch 2575: train loss: 0.00021229684352874756\n",
      "Epoch 2576: train loss: 0.0002117987460223958\n",
      "Epoch 2577: train loss: 0.0002112970978487283\n",
      "Epoch 2578: train loss: 0.00021080035367049277\n",
      "Epoch 2579: train loss: 0.00021030513744335622\n",
      "Epoch 2580: train loss: 0.0002098133263643831\n",
      "Epoch 2581: train loss: 0.00020931917242705822\n",
      "Epoch 2582: train loss: 0.00020883127581328154\n",
      "Epoch 2583: train loss: 0.00020834077440667897\n",
      "Epoch 2584: train loss: 0.00020785474043805152\n",
      "Epoch 2585: train loss: 0.00020737027807626873\n",
      "Epoch 2586: train loss: 0.00020688667427748442\n",
      "Epoch 2587: train loss: 0.00020640688308048993\n",
      "Epoch 2588: train loss: 0.00020592792134266347\n",
      "Epoch 2589: train loss: 0.00020544655853882432\n",
      "Epoch 2590: train loss: 0.00020497011428233236\n",
      "Epoch 2591: train loss: 0.00020449620205909014\n",
      "Epoch 2592: train loss: 0.0002040229446720332\n",
      "Epoch 2593: train loss: 0.00020355131709948182\n",
      "Epoch 2594: train loss: 0.0002030825417023152\n",
      "Epoch 2595: train loss: 0.00020261344616301358\n",
      "Epoch 2596: train loss: 0.00020214737742207944\n",
      "Epoch 2597: train loss: 0.000201681861653924\n",
      "Epoch 2598: train loss: 0.00020121794659644365\n",
      "Epoch 2599: train loss: 0.00020075682550668716\n",
      "Epoch 2600: train loss: 0.00020029701408930123\n",
      "Epoch 2601: train loss: 0.00019983742095064372\n",
      "Epoch 2602: train loss: 0.00019937945762649179\n",
      "Epoch 2603: train loss: 0.0001989262382267043\n",
      "Epoch 2604: train loss: 0.00019847010844387114\n",
      "Epoch 2605: train loss: 0.0001980186498258263\n",
      "Epoch 2606: train loss: 0.00019756820984184742\n",
      "Epoch 2607: train loss: 0.00019712014182005078\n",
      "Epoch 2608: train loss: 0.00019667233573272824\n",
      "Epoch 2609: train loss: 0.00019622607214841992\n",
      "Epoch 2610: train loss: 0.00019578156934585422\n",
      "Epoch 2611: train loss: 0.000195338943740353\n",
      "Epoch 2612: train loss: 0.00019489522674120963\n",
      "Epoch 2613: train loss: 0.00019445657380856574\n",
      "Epoch 2614: train loss: 0.00019401809549890459\n",
      "Epoch 2615: train loss: 0.00019358107238076627\n",
      "Epoch 2616: train loss: 0.0001931457663886249\n",
      "Epoch 2617: train loss: 0.0001927118719322607\n",
      "Epoch 2618: train loss: 0.0001922799419844523\n",
      "Epoch 2619: train loss: 0.00019184678967576474\n",
      "Epoch 2620: train loss: 0.00019141784287057817\n",
      "Epoch 2621: train loss: 0.00019099126802757382\n",
      "Epoch 2622: train loss: 0.00019056416931562126\n",
      "Epoch 2623: train loss: 0.0001901395444292575\n",
      "Epoch 2624: train loss: 0.00018971417739521712\n",
      "Epoch 2625: train loss: 0.0001892932632472366\n",
      "Epoch 2626: train loss: 0.00018887191254179925\n",
      "Epoch 2627: train loss: 0.00018845309386961162\n",
      "Epoch 2628: train loss: 0.00018803526472765952\n",
      "Epoch 2629: train loss: 0.00018761827959679067\n",
      "Epoch 2630: train loss: 0.0001872039574664086\n",
      "Epoch 2631: train loss: 0.00018679030472412705\n",
      "Epoch 2632: train loss: 0.00018637724861036986\n",
      "Epoch 2633: train loss: 0.00018596486188471317\n",
      "Epoch 2634: train loss: 0.00018555713177192956\n",
      "Epoch 2635: train loss: 0.00018514863040763885\n",
      "Epoch 2636: train loss: 0.0001847413368523121\n",
      "Epoch 2637: train loss: 0.00018433763762004673\n",
      "Epoch 2638: train loss: 0.00018393316713627428\n",
      "Epoch 2639: train loss: 0.00018352987535763532\n",
      "Epoch 2640: train loss: 0.00018312872271053493\n",
      "Epoch 2641: train loss: 0.00018272764282301068\n",
      "Epoch 2642: train loss: 0.00018232925503980368\n",
      "Epoch 2643: train loss: 0.00018193160940427333\n",
      "Epoch 2644: train loss: 0.00018153584096580744\n",
      "Epoch 2645: train loss: 0.00018114234262611717\n",
      "Epoch 2646: train loss: 0.0001807486405596137\n",
      "Epoch 2647: train loss: 0.00018035501125268638\n",
      "Epoch 2648: train loss: 0.00017996347742155194\n",
      "Epoch 2649: train loss: 0.0001795756834326312\n",
      "Epoch 2650: train loss: 0.00017918602679856122\n",
      "Epoch 2651: train loss: 0.0001788016379578039\n",
      "Epoch 2652: train loss: 0.00017841490625869483\n",
      "Epoch 2653: train loss: 0.00017803160881157964\n",
      "Epoch 2654: train loss: 0.00017764752556104213\n",
      "Epoch 2655: train loss: 0.00017726670193951577\n",
      "Epoch 2656: train loss: 0.00017688560183160007\n",
      "Epoch 2657: train loss: 0.00017650709196459502\n",
      "Epoch 2658: train loss: 0.0001761288585839793\n",
      "Epoch 2659: train loss: 0.00017575181846041232\n",
      "Epoch 2660: train loss: 0.00017537646635901183\n",
      "Epoch 2661: train loss: 0.00017500107060186565\n",
      "Epoch 2662: train loss: 0.00017463004041928798\n",
      "Epoch 2663: train loss: 0.0001742578751873225\n",
      "Epoch 2664: train loss: 0.00017388720880262554\n",
      "Epoch 2665: train loss: 0.00017351836140733212\n",
      "Epoch 2666: train loss: 0.00017315008153673261\n",
      "Epoch 2667: train loss: 0.0001727825147099793\n",
      "Epoch 2668: train loss: 0.0001724176836432889\n",
      "Epoch 2669: train loss: 0.0001720540167298168\n",
      "Epoch 2670: train loss: 0.00017168869089800864\n",
      "Epoch 2671: train loss: 0.0001713274687062949\n",
      "Epoch 2672: train loss: 0.00017096789088100195\n",
      "Epoch 2673: train loss: 0.000170607803738676\n",
      "Epoch 2674: train loss: 0.00017024869157467037\n",
      "Epoch 2675: train loss: 0.0001698922278592363\n",
      "Epoch 2676: train loss: 0.00016953470185399055\n",
      "Epoch 2677: train loss: 0.00016917858738452196\n",
      "Epoch 2678: train loss: 0.0001688251068117097\n",
      "Epoch 2679: train loss: 0.00016847292135935277\n",
      "Epoch 2680: train loss: 0.00016812175454106182\n",
      "Epoch 2681: train loss: 0.0001677701366133988\n",
      "Epoch 2682: train loss: 0.0001674218656262383\n",
      "Epoch 2683: train loss: 0.00016707256145309657\n",
      "Epoch 2684: train loss: 0.00016672626952640712\n",
      "Epoch 2685: train loss: 0.00016638103988952935\n",
      "Epoch 2686: train loss: 0.00016603581025265157\n",
      "Epoch 2687: train loss: 0.00016569225408602506\n",
      "Epoch 2688: train loss: 0.00016534932365175337\n",
      "Epoch 2689: train loss: 0.0001650067715672776\n",
      "Epoch 2690: train loss: 0.00016466749366372824\n",
      "Epoch 2691: train loss: 0.00016432738630101085\n",
      "Epoch 2692: train loss: 0.00016398973821196705\n",
      "Epoch 2693: train loss: 0.00016365209012292325\n",
      "Epoch 2694: train loss: 0.0001633170322747901\n",
      "Epoch 2695: train loss: 0.00016298203263431787\n",
      "Epoch 2696: train loss: 0.00016264668374788016\n",
      "Epoch 2697: train loss: 0.00016231481276918203\n",
      "Epoch 2698: train loss: 0.00016198416415136307\n",
      "Epoch 2699: train loss: 0.00016165210399776697\n",
      "Epoch 2700: train loss: 0.00016132235759869218\n",
      "Epoch 2701: train loss: 0.00016099376080092043\n",
      "Epoch 2702: train loss: 0.00016066626994870603\n",
      "Epoch 2703: train loss: 0.00016033965221140534\n",
      "Epoch 2704: train loss: 0.00016001364565454423\n",
      "Epoch 2705: train loss: 0.00015968995285220444\n",
      "Epoch 2706: train loss: 0.00015936585259623826\n",
      "Epoch 2707: train loss: 0.00015904380416031927\n",
      "Epoch 2708: train loss: 0.0001587220758665353\n",
      "Epoch 2709: train loss: 0.00015840187552385032\n",
      "Epoch 2710: train loss: 0.00015808225725777447\n",
      "Epoch 2711: train loss: 0.0001577646762598306\n",
      "Epoch 2712: train loss: 0.00015744721167720854\n",
      "Epoch 2713: train loss: 0.00015713110042270273\n",
      "Epoch 2714: train loss: 0.00015681541117373854\n",
      "Epoch 2715: train loss: 0.00015650055138394237\n",
      "Epoch 2716: train loss: 0.00015618675388395786\n",
      "Epoch 2717: train loss: 0.00015587439702358097\n",
      "Epoch 2718: train loss: 0.0001555635972181335\n",
      "Epoch 2719: train loss: 0.00015525391791015863\n",
      "Epoch 2720: train loss: 0.00015494294348172843\n",
      "Epoch 2721: train loss: 0.00015463469026144594\n",
      "Epoch 2722: train loss: 0.00015432776126544923\n",
      "Epoch 2723: train loss: 0.00015402091958094388\n",
      "Epoch 2724: train loss: 0.0001537143107270822\n",
      "Epoch 2725: train loss: 0.0001534101174911484\n",
      "Epoch 2726: train loss: 0.00015310665185097605\n",
      "Epoch 2727: train loss: 0.00015280301158782095\n",
      "Epoch 2728: train loss: 0.00015249926946125925\n",
      "Epoch 2729: train loss: 0.0001522013481007889\n",
      "Epoch 2730: train loss: 0.00015190061822067946\n",
      "Epoch 2731: train loss: 0.00015160140173975378\n",
      "Epoch 2732: train loss: 0.0001513028983026743\n",
      "Epoch 2733: train loss: 0.00015100532618816942\n",
      "Epoch 2734: train loss: 0.0001507089618826285\n",
      "Epoch 2735: train loss: 0.0001504134852439165\n",
      "Epoch 2736: train loss: 0.00015011864888947457\n",
      "Epoch 2737: train loss: 0.00014982599532231688\n",
      "Epoch 2738: train loss: 0.00014953243953641504\n",
      "Epoch 2739: train loss: 0.00014924044080544263\n",
      "Epoch 2740: train loss: 0.00014895066851750016\n",
      "Epoch 2741: train loss: 0.00014865903358440846\n",
      "Epoch 2742: train loss: 0.00014837052731309086\n",
      "Epoch 2743: train loss: 0.00014808177365921438\n",
      "Epoch 2744: train loss: 0.0001477947662351653\n",
      "Epoch 2745: train loss: 0.0001475081080570817\n",
      "Epoch 2746: train loss: 0.00014722325431648642\n",
      "Epoch 2747: train loss: 0.0001469371491111815\n",
      "Epoch 2748: train loss: 0.00014665463822893798\n",
      "Epoch 2749: train loss: 0.00014637109416071326\n",
      "Epoch 2750: train loss: 0.0001460880448576063\n",
      "Epoch 2751: train loss: 0.00014580730930902064\n",
      "Epoch 2752: train loss: 0.0001455269957659766\n",
      "Epoch 2753: train loss: 0.00014524758444167674\n",
      "Epoch 2754: train loss: 0.0001449685514671728\n",
      "Epoch 2755: train loss: 0.00014468978042714298\n",
      "Epoch 2756: train loss: 0.00014441284292843193\n",
      "Epoch 2757: train loss: 0.00014413539611268789\n",
      "Epoch 2758: train loss: 0.0001438608014723286\n",
      "Epoch 2759: train loss: 0.00014358559565152973\n",
      "Epoch 2760: train loss: 0.00014331203419715166\n",
      "Epoch 2761: train loss: 0.00014303858915809542\n",
      "Epoch 2762: train loss: 0.00014276668662205338\n",
      "Epoch 2763: train loss: 0.00014249484229367226\n",
      "Epoch 2764: train loss: 0.00014222509344108403\n",
      "Epoch 2765: train loss: 0.00014195374387782067\n",
      "Epoch 2766: train loss: 0.00014168595953378826\n",
      "Epoch 2767: train loss: 0.0001414183061569929\n",
      "Epoch 2768: train loss: 0.00014115085650701076\n",
      "Epoch 2769: train loss: 0.00014088276657275856\n",
      "Epoch 2770: train loss: 0.00014061713591217995\n",
      "Epoch 2771: train loss: 0.00014035227650310844\n",
      "Epoch 2772: train loss: 0.00014008674770593643\n",
      "Epoch 2773: train loss: 0.00013982357631903142\n",
      "Epoch 2774: train loss: 0.00013956261682324111\n",
      "Epoch 2775: train loss: 0.00013930046407040209\n",
      "Epoch 2776: train loss: 0.00013903835497330874\n",
      "Epoch 2777: train loss: 0.0001387773227179423\n",
      "Epoch 2778: train loss: 0.00013851691619493067\n",
      "Epoch 2779: train loss: 0.0001382588961860165\n",
      "Epoch 2780: train loss: 0.00013800023589283228\n",
      "Epoch 2781: train loss: 0.00013774321996606886\n",
      "Epoch 2782: train loss: 0.00013748678611591458\n",
      "Epoch 2783: train loss: 0.00013723046868108213\n",
      "Epoch 2784: train loss: 0.0001369751407764852\n",
      "Epoch 2785: train loss: 0.0001367213117191568\n",
      "Epoch 2786: train loss: 0.00013646841398440301\n",
      "Epoch 2787: train loss: 0.0001362149341730401\n",
      "Epoch 2788: train loss: 0.00013596229837276042\n",
      "Epoch 2789: train loss: 0.00013571033196058124\n",
      "Epoch 2790: train loss: 0.0001354595151497051\n",
      "Epoch 2791: train loss: 0.00013520877109840512\n",
      "Epoch 2792: train loss: 0.0001349607337033376\n",
      "Epoch 2793: train loss: 0.0001347119832644239\n",
      "Epoch 2794: train loss: 0.000134464047732763\n",
      "Epoch 2795: train loss: 0.0001342162286164239\n",
      "Epoch 2796: train loss: 0.00013397078146226704\n",
      "Epoch 2797: train loss: 0.00013372387911658734\n",
      "Epoch 2798: train loss: 0.00013347979984246194\n",
      "Epoch 2799: train loss: 0.0001332356478087604\n",
      "Epoch 2800: train loss: 0.00013299104466568679\n",
      "Epoch 2801: train loss: 0.00013274775119498372\n",
      "Epoch 2802: train loss: 0.00013250552001409233\n",
      "Epoch 2803: train loss: 0.0001322636380791664\n",
      "Epoch 2804: train loss: 0.00013202219270169735\n",
      "Epoch 2805: train loss: 0.00013178246445022523\n",
      "Epoch 2806: train loss: 0.00013154295447748154\n",
      "Epoch 2807: train loss: 0.00013130334264133126\n",
      "Epoch 2808: train loss: 0.0001310661027673632\n",
      "Epoch 2809: train loss: 0.00013082877558190376\n",
      "Epoch 2810: train loss: 0.00013059253979008645\n",
      "Epoch 2811: train loss: 0.0001303544850088656\n",
      "Epoch 2812: train loss: 0.00013012054841965437\n",
      "Epoch 2813: train loss: 0.00012988393427804112\n",
      "Epoch 2814: train loss: 0.00012964991037733853\n",
      "Epoch 2815: train loss: 0.00012941686145495623\n",
      "Epoch 2816: train loss: 0.00012918387074023485\n",
      "Epoch 2817: train loss: 0.00012895141844637692\n",
      "Epoch 2818: train loss: 0.00012871951912529767\n",
      "Epoch 2819: train loss: 0.00012848901678808033\n",
      "Epoch 2820: train loss: 0.00012825796147808433\n",
      "Epoch 2821: train loss: 0.00012802764831576496\n",
      "Epoch 2822: train loss: 0.00012780018732883036\n",
      "Epoch 2823: train loss: 0.00012757063086610287\n",
      "Epoch 2824: train loss: 0.0001273435918847099\n",
      "Epoch 2825: train loss: 0.0001271159271709621\n",
      "Epoch 2826: train loss: 0.00012688899005297571\n",
      "Epoch 2827: train loss: 0.00012666442489717156\n",
      "Epoch 2828: train loss: 0.0001264382735826075\n",
      "Epoch 2829: train loss: 0.0001262137811863795\n",
      "Epoch 2830: train loss: 0.0001259888376807794\n",
      "Epoch 2831: train loss: 0.0001257652329513803\n",
      "Epoch 2832: train loss: 0.00012554308341350406\n",
      "Epoch 2833: train loss: 0.0001253204100066796\n",
      "Epoch 2834: train loss: 0.00012509820226114243\n",
      "Epoch 2835: train loss: 0.00012487891945056617\n",
      "Epoch 2836: train loss: 0.00012465709005482495\n",
      "Epoch 2837: train loss: 0.00012443731247913092\n",
      "Epoch 2838: train loss: 0.00012421845167409629\n",
      "Epoch 2839: train loss: 0.00012400014384184033\n",
      "Epoch 2840: train loss: 0.00012378196697682142\n",
      "Epoch 2841: train loss: 0.00012356415390968323\n",
      "Epoch 2842: train loss: 0.0001233466900885105\n",
      "Epoch 2843: train loss: 0.00012313028855714947\n",
      "Epoch 2844: train loss: 0.00012291640450712293\n",
      "Epoch 2845: train loss: 0.00012270061415620148\n",
      "Epoch 2846: train loss: 0.00012248571147210896\n",
      "Epoch 2847: train loss: 0.00012227156548760831\n",
      "Epoch 2848: train loss: 0.00012205747771076858\n",
      "Epoch 2849: train loss: 0.00012184585648356006\n",
      "Epoch 2850: train loss: 0.00012163274368504062\n",
      "Epoch 2851: train loss: 0.00012142244668211788\n",
      "Epoch 2852: train loss: 0.0001212099232361652\n",
      "Epoch 2853: train loss: 0.00012099921150365844\n",
      "Epoch 2854: train loss: 0.00012078935105819255\n",
      "Epoch 2855: train loss: 0.00012057901039952412\n",
      "Epoch 2856: train loss: 0.00012037120905006304\n",
      "Epoch 2857: train loss: 0.0001201635823235847\n",
      "Epoch 2858: train loss: 0.00011995444219792262\n",
      "Epoch 2859: train loss: 0.00011974868539255112\n",
      "Epoch 2860: train loss: 0.00011954103683819994\n",
      "Epoch 2861: train loss: 0.00011933474888792261\n",
      "Epoch 2862: train loss: 0.00011912931950064376\n",
      "Epoch 2863: train loss: 0.00011892396287294105\n",
      "Epoch 2864: train loss: 0.00011871993774548173\n",
      "Epoch 2865: train loss: 0.00011851546150865033\n",
      "Epoch 2866: train loss: 0.00011831207666546106\n",
      "Epoch 2867: train loss: 0.00011810837168013677\n",
      "Epoch 2868: train loss: 0.00011790658027166501\n",
      "Epoch 2869: train loss: 0.00011770504352170974\n",
      "Epoch 2870: train loss: 0.00011750325938919559\n",
      "Epoch 2871: train loss: 0.00011730255937436596\n",
      "Epoch 2872: train loss: 0.00011710214312188327\n",
      "Epoch 2873: train loss: 0.00011690179235301912\n",
      "Epoch 2874: train loss: 0.00011670312233036384\n",
      "Epoch 2875: train loss: 0.00011650546366581693\n",
      "Epoch 2876: train loss: 0.00011630680819507688\n",
      "Epoch 2877: train loss: 0.00011610816727625206\n",
      "Epoch 2878: train loss: 0.0001159113016910851\n",
      "Epoch 2879: train loss: 0.00011571449431357905\n",
      "Epoch 2880: train loss: 0.00011551861098269\n",
      "Epoch 2881: train loss: 0.0001153230550698936\n",
      "Epoch 2882: train loss: 0.00011512838682392612\n",
      "Epoch 2883: train loss: 0.00011493275087559596\n",
      "Epoch 2884: train loss: 0.00011473834456410259\n",
      "Epoch 2885: train loss: 0.00011454509512986988\n",
      "Epoch 2886: train loss: 0.0001143526314990595\n",
      "Epoch 2887: train loss: 0.00011415855988161638\n",
      "Epoch 2888: train loss: 0.00011396671470720321\n",
      "Epoch 2889: train loss: 0.00011377605551388115\n",
      "Epoch 2890: train loss: 0.00011358375195413828\n",
      "Epoch 2891: train loss: 0.00011339352931827307\n",
      "Epoch 2892: train loss: 0.00011320299381623045\n",
      "Epoch 2893: train loss: 0.00011301300401100889\n",
      "Epoch 2894: train loss: 0.00011282367631793022\n",
      "Epoch 2895: train loss: 0.00011263589112786576\n",
      "Epoch 2896: train loss: 0.00011244636698393151\n",
      "Epoch 2897: train loss: 0.0001122589674196206\n",
      "Epoch 2898: train loss: 0.00011207196075702086\n",
      "Epoch 2899: train loss: 0.00011188430653419346\n",
      "Epoch 2900: train loss: 0.00011169776553288102\n",
      "Epoch 2901: train loss: 0.00011151253420393914\n",
      "Epoch 2902: train loss: 0.00011132625513710082\n",
      "Epoch 2903: train loss: 0.00011114169319625944\n",
      "Epoch 2904: train loss: 0.00011095531226601452\n",
      "Epoch 2905: train loss: 0.00011077214003307745\n",
      "Epoch 2906: train loss: 0.00011058906966354698\n",
      "Epoch 2907: train loss: 0.00011040579556720331\n",
      "Epoch 2908: train loss: 0.00011022399849025533\n",
      "Epoch 2909: train loss: 0.00011004020052496344\n",
      "Epoch 2910: train loss: 0.00010985804692609236\n",
      "Epoch 2911: train loss: 0.00010967721027554944\n",
      "Epoch 2912: train loss: 0.00010949693387374282\n",
      "Epoch 2913: train loss: 0.00010931576980510727\n",
      "Epoch 2914: train loss: 0.0001091352678486146\n",
      "Epoch 2915: train loss: 0.00010895659943344072\n",
      "Epoch 2916: train loss: 0.00010877654131036252\n",
      "Epoch 2917: train loss: 0.00010859880421776325\n",
      "Epoch 2918: train loss: 0.00010841963376151398\n",
      "Epoch 2919: train loss: 0.00010824181663338095\n",
      "Epoch 2920: train loss: 0.00010806432692334056\n",
      "Epoch 2921: train loss: 0.00010788771760417148\n",
      "Epoch 2922: train loss: 0.0001077111519407481\n",
      "Epoch 2923: train loss: 0.00010753505193861201\n",
      "Epoch 2924: train loss: 0.00010735880641732365\n",
      "Epoch 2925: train loss: 0.00010718371777329594\n",
      "Epoch 2926: train loss: 0.0001070088692358695\n",
      "Epoch 2927: train loss: 0.00010683382424758747\n",
      "Epoch 2928: train loss: 0.00010665980516932905\n",
      "Epoch 2929: train loss: 0.00010648685565683991\n",
      "Epoch 2930: train loss: 0.00010631272016325966\n",
      "Epoch 2931: train loss: 0.00010614110215101391\n",
      "Epoch 2932: train loss: 0.0001059682181221433\n",
      "Epoch 2933: train loss: 0.00010579634545138106\n",
      "Epoch 2934: train loss: 0.0001056246692314744\n",
      "Epoch 2935: train loss: 0.00010545265104155988\n",
      "Epoch 2936: train loss: 0.0001052826046361588\n",
      "Epoch 2937: train loss: 0.00010511225264053792\n",
      "Epoch 2938: train loss: 0.00010494322486920282\n",
      "Epoch 2939: train loss: 0.00010477306932443753\n",
      "Epoch 2940: train loss: 0.00010460357589181513\n",
      "Epoch 2941: train loss: 0.00010443534119985998\n",
      "Epoch 2942: train loss: 0.00010426761582493782\n",
      "Epoch 2943: train loss: 0.00010409989772597328\n",
      "Epoch 2944: train loss: 0.00010393264528829604\n",
      "Epoch 2945: train loss: 0.00010376502177678049\n",
      "Epoch 2946: train loss: 0.0001035981549648568\n",
      "Epoch 2947: train loss: 0.00010343100439058617\n",
      "Epoch 2948: train loss: 0.00010326672781957313\n",
      "Epoch 2949: train loss: 0.00010310057405149564\n",
      "Epoch 2950: train loss: 0.00010293491504853591\n",
      "Epoch 2951: train loss: 0.00010276991088176146\n",
      "Epoch 2952: train loss: 0.00010260667477268726\n",
      "Epoch 2953: train loss: 0.00010244283475913107\n",
      "Epoch 2954: train loss: 0.00010227855091216043\n",
      "Epoch 2955: train loss: 0.00010211562766926363\n",
      "Epoch 2956: train loss: 0.00010195195500273257\n",
      "Epoch 2957: train loss: 0.00010179029777646065\n",
      "Epoch 2958: train loss: 0.00010162901889998466\n",
      "Epoch 2959: train loss: 0.00010146503336727619\n",
      "Epoch 2960: train loss: 0.00010130538430530578\n",
      "Epoch 2961: train loss: 0.00010114443284692243\n",
      "Epoch 2962: train loss: 0.00010098404163727537\n",
      "Epoch 2963: train loss: 0.00010082376684295014\n",
      "Epoch 2964: train loss: 0.00010066474351333454\n",
      "Epoch 2965: train loss: 0.00010050393757410347\n",
      "Epoch 2966: train loss: 0.00010034495790023357\n",
      "Epoch 2967: train loss: 0.00010018715693149716\n",
      "Epoch 2968: train loss: 0.00010002859926316887\n",
      "Epoch 2969: train loss: 9.98699470073916e-05\n",
      "Epoch 2970: train loss: 9.971292456611991e-05\n",
      "Epoch 2971: train loss: 9.955560381058604e-05\n",
      "Epoch 2972: train loss: 9.939839219441637e-05\n",
      "Epoch 2973: train loss: 9.92418426903896e-05\n",
      "Epoch 2974: train loss: 9.908557694870979e-05\n",
      "Epoch 2975: train loss: 9.893017704598606e-05\n",
      "Epoch 2976: train loss: 9.877439151750877e-05\n",
      "Epoch 2977: train loss: 9.861986472969875e-05\n",
      "Epoch 2978: train loss: 9.846442844718695e-05\n",
      "Epoch 2979: train loss: 9.83103018370457e-05\n",
      "Epoch 2980: train loss: 9.815581142902374e-05\n",
      "Epoch 2981: train loss: 9.80027616606094e-05\n",
      "Epoch 2982: train loss: 9.784908615984023e-05\n",
      "Epoch 2983: train loss: 9.769577445695177e-05\n",
      "Epoch 2984: train loss: 9.754359780345112e-05\n",
      "Epoch 2985: train loss: 9.739161760080606e-05\n",
      "Epoch 2986: train loss: 9.72388734226115e-05\n",
      "Epoch 2987: train loss: 9.708727156976238e-05\n",
      "Epoch 2988: train loss: 9.693655738374218e-05\n",
      "Epoch 2989: train loss: 9.67843079706654e-05\n",
      "Epoch 2990: train loss: 9.663433593232185e-05\n",
      "Epoch 2991: train loss: 9.648392006056383e-05\n",
      "Epoch 2992: train loss: 9.633573790779337e-05\n",
      "Epoch 2993: train loss: 9.618465992389247e-05\n",
      "Epoch 2994: train loss: 9.603479702491313e-05\n",
      "Epoch 2995: train loss: 9.58863747655414e-05\n",
      "Epoch 2996: train loss: 9.573830175213516e-05\n",
      "Epoch 2997: train loss: 9.558998135617003e-05\n",
      "Epoch 2998: train loss: 9.54419665504247e-05\n",
      "Epoch 2999: train loss: 9.529439557809383e-05\n",
      "Epoch 3000: train loss: 9.514743578620255e-05\n",
      "Epoch 3001: train loss: 9.500009036855772e-05\n",
      "Epoch 3002: train loss: 9.485362534178421e-05\n",
      "Epoch 3003: train loss: 9.470719669479877e-05\n",
      "Epoch 3004: train loss: 9.456135740038007e-05\n",
      "Epoch 3005: train loss: 9.441563452128321e-05\n",
      "Epoch 3006: train loss: 9.42698388826102e-05\n",
      "Epoch 3007: train loss: 9.412452345713973e-05\n",
      "Epoch 3008: train loss: 9.398029214935377e-05\n",
      "Epoch 3009: train loss: 9.383608994539827e-05\n",
      "Epoch 3010: train loss: 9.369169129058719e-05\n",
      "Epoch 3011: train loss: 9.354846406495199e-05\n",
      "Epoch 3012: train loss: 9.340466203866526e-05\n",
      "Epoch 3013: train loss: 9.326190047431737e-05\n",
      "Epoch 3014: train loss: 9.311913890996948e-05\n",
      "Epoch 3015: train loss: 9.297570068156347e-05\n",
      "Epoch 3016: train loss: 9.283453255193308e-05\n",
      "Epoch 3017: train loss: 9.269163274439052e-05\n",
      "Epoch 3018: train loss: 9.25505009945482e-05\n",
      "Epoch 3019: train loss: 9.240928193321452e-05\n",
      "Epoch 3020: train loss: 9.22677936614491e-05\n",
      "Epoch 3021: train loss: 9.212704753736034e-05\n",
      "Epoch 3022: train loss: 9.198665793519467e-05\n",
      "Epoch 3023: train loss: 9.184703230857849e-05\n",
      "Epoch 3024: train loss: 9.170659177470952e-05\n",
      "Epoch 3025: train loss: 9.156743180938065e-05\n",
      "Epoch 3026: train loss: 9.142862836597487e-05\n",
      "Epoch 3027: train loss: 9.128963574767113e-05\n",
      "Epoch 3028: train loss: 9.115159627981484e-05\n",
      "Epoch 3029: train loss: 9.101373143494129e-05\n",
      "Epoch 3030: train loss: 9.087489888770506e-05\n",
      "Epoch 3031: train loss: 9.073781984625384e-05\n",
      "Epoch 3032: train loss: 9.060095180757344e-05\n",
      "Epoch 3033: train loss: 9.046264312928542e-05\n",
      "Epoch 3034: train loss: 9.032636444317177e-05\n",
      "Epoch 3035: train loss: 9.019001299748197e-05\n",
      "Epoch 3036: train loss: 9.005373431136832e-05\n",
      "Epoch 3037: train loss: 8.9917732111644e-05\n",
      "Epoch 3038: train loss: 8.978156256489456e-05\n",
      "Epoch 3039: train loss: 8.964607695816085e-05\n",
      "Epoch 3040: train loss: 8.95114935701713e-05\n",
      "Epoch 3041: train loss: 8.937701204558834e-05\n",
      "Epoch 3042: train loss: 8.924173744162545e-05\n",
      "Epoch 3043: train loss: 8.910816541174427e-05\n",
      "Epoch 3044: train loss: 8.89741349965334e-05\n",
      "Epoch 3045: train loss: 8.88411232153885e-05\n",
      "Epoch 3046: train loss: 8.870694728102535e-05\n",
      "Epoch 3047: train loss: 8.857450302457437e-05\n",
      "Epoch 3048: train loss: 8.844162948662415e-05\n",
      "Epoch 3049: train loss: 8.830936712911353e-05\n",
      "Epoch 3050: train loss: 8.81765954545699e-05\n",
      "Epoch 3051: train loss: 8.804520621197298e-05\n",
      "Epoch 3052: train loss: 8.791336586000398e-05\n",
      "Epoch 3053: train loss: 8.778113260632381e-05\n",
      "Epoch 3054: train loss: 8.765071106608957e-05\n",
      "Epoch 3055: train loss: 8.751941641094163e-05\n",
      "Epoch 3056: train loss: 8.73890530783683e-05\n",
      "Epoch 3057: train loss: 8.725909719942138e-05\n",
      "Epoch 3058: train loss: 8.712895214557648e-05\n",
      "Epoch 3059: train loss: 8.699887985130772e-05\n",
      "Epoch 3060: train loss: 8.686957880854607e-05\n",
      "Epoch 3061: train loss: 8.674048149259761e-05\n",
      "Epoch 3062: train loss: 8.6611820734106e-05\n",
      "Epoch 3063: train loss: 8.648231596453115e-05\n",
      "Epoch 3064: train loss: 8.635433187009767e-05\n",
      "Epoch 3065: train loss: 8.622602035757154e-05\n",
      "Epoch 3066: train loss: 8.609779615653679e-05\n",
      "Epoch 3067: train loss: 8.597051782999188e-05\n",
      "Epoch 3068: train loss: 8.584306488046423e-05\n",
      "Epoch 3069: train loss: 8.571668149670586e-05\n",
      "Epoch 3070: train loss: 8.558907575206831e-05\n",
      "Epoch 3071: train loss: 8.546163007849827e-05\n",
      "Epoch 3072: train loss: 8.53369856486097e-05\n",
      "Epoch 3073: train loss: 8.521006384398788e-05\n",
      "Epoch 3074: train loss: 8.508440805599093e-05\n",
      "Epoch 3075: train loss: 8.495930524077266e-05\n",
      "Epoch 3076: train loss: 8.483301644446328e-05\n",
      "Epoch 3077: train loss: 8.470849570585415e-05\n",
      "Epoch 3078: train loss: 8.458416414214298e-05\n",
      "Epoch 3079: train loss: 8.445933053735644e-05\n",
      "Epoch 3080: train loss: 8.43351663206704e-05\n",
      "Epoch 3081: train loss: 8.421120583079755e-05\n",
      "Epoch 3082: train loss: 8.408758731093258e-05\n",
      "Epoch 3083: train loss: 8.396364864893258e-05\n",
      "Epoch 3084: train loss: 8.383989916183054e-05\n",
      "Epoch 3085: train loss: 8.371764124603942e-05\n",
      "Epoch 3086: train loss: 8.359398634638637e-05\n",
      "Epoch 3087: train loss: 8.347224502358586e-05\n",
      "Epoch 3088: train loss: 8.334948506671935e-05\n",
      "Epoch 3089: train loss: 8.322738722199574e-05\n",
      "Epoch 3090: train loss: 8.310640987474471e-05\n",
      "Epoch 3091: train loss: 8.298442844534293e-05\n",
      "Epoch 3092: train loss: 8.286373486043885e-05\n",
      "Epoch 3093: train loss: 8.274229185190052e-05\n",
      "Epoch 3094: train loss: 8.262186747742817e-05\n",
      "Epoch 3095: train loss: 8.250105747720227e-05\n",
      "Epoch 3096: train loss: 8.237983274739236e-05\n",
      "Epoch 3097: train loss: 8.226033969549462e-05\n",
      "Epoch 3098: train loss: 8.214030822273344e-05\n",
      "Epoch 3099: train loss: 8.202101162169129e-05\n",
      "Epoch 3100: train loss: 8.190124935936183e-05\n",
      "Epoch 3101: train loss: 8.178229472832754e-05\n",
      "Epoch 3102: train loss: 8.166357292793691e-05\n",
      "Epoch 3103: train loss: 8.154489478329197e-05\n",
      "Epoch 3104: train loss: 8.14259474282153e-05\n",
      "Epoch 3105: train loss: 8.130739297484979e-05\n",
      "Epoch 3106: train loss: 8.11905920272693e-05\n",
      "Epoch 3107: train loss: 8.10723940958269e-05\n",
      "Epoch 3108: train loss: 8.0955280282069e-05\n",
      "Epoch 3109: train loss: 8.083782449830323e-05\n",
      "Epoch 3110: train loss: 8.072124910540879e-05\n",
      "Epoch 3111: train loss: 8.060368418227881e-05\n",
      "Epoch 3112: train loss: 8.048747986322269e-05\n",
      "Epoch 3113: train loss: 8.037091902224347e-05\n",
      "Epoch 3114: train loss: 8.025512215681374e-05\n",
      "Epoch 3115: train loss: 8.013955084607005e-05\n",
      "Epoch 3116: train loss: 8.002360846148804e-05\n",
      "Epoch 3117: train loss: 7.990824815351516e-05\n",
      "Epoch 3118: train loss: 7.97926913946867e-05\n",
      "Epoch 3119: train loss: 7.967854617163539e-05\n",
      "Epoch 3120: train loss: 7.956232002470642e-05\n",
      "Epoch 3121: train loss: 7.944917888380587e-05\n",
      "Epoch 3122: train loss: 7.93344879639335e-05\n",
      "Epoch 3123: train loss: 7.922001532278955e-05\n",
      "Epoch 3124: train loss: 7.910676504252478e-05\n",
      "Epoch 3125: train loss: 7.899316551629454e-05\n",
      "Epoch 3126: train loss: 7.887898391345516e-05\n",
      "Epoch 3127: train loss: 7.876661402406171e-05\n",
      "Epoch 3128: train loss: 7.865364023018628e-05\n",
      "Epoch 3129: train loss: 7.854031719034538e-05\n",
      "Epoch 3130: train loss: 7.842899503884837e-05\n",
      "Epoch 3131: train loss: 7.831565744709224e-05\n",
      "Epoch 3132: train loss: 7.820352038834244e-05\n",
      "Epoch 3133: train loss: 7.809133967384696e-05\n",
      "Epoch 3134: train loss: 7.797999569447711e-05\n",
      "Epoch 3135: train loss: 7.786878995830193e-05\n",
      "Epoch 3136: train loss: 7.775816629873589e-05\n",
      "Epoch 3137: train loss: 7.76466986280866e-05\n",
      "Epoch 3138: train loss: 7.753622776363045e-05\n",
      "Epoch 3139: train loss: 7.74250365793705e-05\n",
      "Epoch 3140: train loss: 7.731452933512628e-05\n",
      "Epoch 3141: train loss: 7.720412395428866e-05\n",
      "Epoch 3142: train loss: 7.709461351623759e-05\n",
      "Epoch 3143: train loss: 7.698460103711113e-05\n",
      "Epoch 3144: train loss: 7.687541074119508e-05\n",
      "Epoch 3145: train loss: 7.676642417209223e-05\n",
      "Epoch 3146: train loss: 7.665695011382923e-05\n",
      "Epoch 3147: train loss: 7.65481308917515e-05\n",
      "Epoch 3148: train loss: 7.643867138540372e-05\n",
      "Epoch 3149: train loss: 7.633064524270594e-05\n",
      "Epoch 3150: train loss: 7.6222640927881e-05\n",
      "Epoch 3151: train loss: 7.611441105837002e-05\n",
      "Epoch 3152: train loss: 7.600716344313696e-05\n",
      "Epoch 3153: train loss: 7.589939923491329e-05\n",
      "Epoch 3154: train loss: 7.579181692562997e-05\n",
      "Epoch 3155: train loss: 7.568432920379564e-05\n",
      "Epoch 3156: train loss: 7.557853678008541e-05\n",
      "Epoch 3157: train loss: 7.54708526073955e-05\n",
      "Epoch 3158: train loss: 7.536444172728807e-05\n",
      "Epoch 3159: train loss: 7.525685214204714e-05\n",
      "Epoch 3160: train loss: 7.515092875109985e-05\n",
      "Epoch 3161: train loss: 7.504485256504267e-05\n",
      "Epoch 3162: train loss: 7.493968587368727e-05\n",
      "Epoch 3163: train loss: 7.483367517124861e-05\n",
      "Epoch 3164: train loss: 7.472853030776605e-05\n",
      "Epoch 3165: train loss: 7.462332723662257e-05\n",
      "Epoch 3166: train loss: 7.451754208886996e-05\n",
      "Epoch 3167: train loss: 7.441305933753029e-05\n",
      "Epoch 3168: train loss: 7.430869300151244e-05\n",
      "Epoch 3169: train loss: 7.420442852890119e-05\n",
      "Epoch 3170: train loss: 7.41003968869336e-05\n",
      "Epoch 3171: train loss: 7.399531750706956e-05\n",
      "Epoch 3172: train loss: 7.38919188734144e-05\n",
      "Epoch 3173: train loss: 7.378846930805594e-05\n",
      "Epoch 3174: train loss: 7.368467777268961e-05\n",
      "Epoch 3175: train loss: 7.358157745329663e-05\n",
      "Epoch 3176: train loss: 7.347828068304807e-05\n",
      "Epoch 3177: train loss: 7.33757988200523e-05\n",
      "Epoch 3178: train loss: 7.327257480937988e-05\n",
      "Epoch 3179: train loss: 7.317025301745161e-05\n",
      "Epoch 3180: train loss: 7.306792394956574e-05\n",
      "Epoch 3181: train loss: 7.296592957573012e-05\n",
      "Epoch 3182: train loss: 7.286392792593688e-05\n",
      "Epoch 3183: train loss: 7.276219548657537e-05\n",
      "Epoch 3184: train loss: 7.266094326041639e-05\n",
      "Epoch 3185: train loss: 7.255940727191046e-05\n",
      "Epoch 3186: train loss: 7.245778397191316e-05\n",
      "Epoch 3187: train loss: 7.23569537512958e-05\n",
      "Epoch 3188: train loss: 7.225643639685586e-05\n",
      "Epoch 3189: train loss: 7.215607183752581e-05\n",
      "Epoch 3190: train loss: 7.205519068520516e-05\n",
      "Epoch 3191: train loss: 7.195500802481547e-05\n",
      "Epoch 3192: train loss: 7.185505819506943e-05\n",
      "Epoch 3193: train loss: 7.17550065019168e-05\n",
      "Epoch 3194: train loss: 7.165484566939995e-05\n",
      "Epoch 3195: train loss: 7.15557107469067e-05\n",
      "Epoch 3196: train loss: 7.145677227526903e-05\n",
      "Epoch 3197: train loss: 7.135752821341157e-05\n",
      "Epoch 3198: train loss: 7.125846605049446e-05\n",
      "Epoch 3199: train loss: 7.115893822629005e-05\n",
      "Epoch 3200: train loss: 7.106107659637928e-05\n",
      "Epoch 3201: train loss: 7.096248009474948e-05\n",
      "Epoch 3202: train loss: 7.086450204951689e-05\n",
      "Epoch 3203: train loss: 7.07661165506579e-05\n",
      "Epoch 3204: train loss: 7.066841499181464e-05\n",
      "Epoch 3205: train loss: 7.057042239466682e-05\n",
      "Epoch 3206: train loss: 7.047304097795859e-05\n",
      "Epoch 3207: train loss: 7.03753757989034e-05\n",
      "Epoch 3208: train loss: 7.027825631666929e-05\n",
      "Epoch 3209: train loss: 7.01813623891212e-05\n",
      "Epoch 3210: train loss: 7.008492684690282e-05\n",
      "Epoch 3211: train loss: 6.998805474722758e-05\n",
      "Epoch 3212: train loss: 6.989187386352569e-05\n",
      "Epoch 3213: train loss: 6.979440513532609e-05\n",
      "Epoch 3214: train loss: 6.969932292122394e-05\n",
      "Epoch 3215: train loss: 6.960223254282027e-05\n",
      "Epoch 3216: train loss: 6.950717943254858e-05\n",
      "Epoch 3217: train loss: 6.941211177036166e-05\n",
      "Epoch 3218: train loss: 6.931631651241332e-05\n",
      "Epoch 3219: train loss: 6.92212488502264e-05\n",
      "Epoch 3220: train loss: 6.912523531354964e-05\n",
      "Epoch 3221: train loss: 6.90309825586155e-05\n",
      "Epoch 3222: train loss: 6.893663521623239e-05\n",
      "Epoch 3223: train loss: 6.884120375616476e-05\n",
      "Epoch 3224: train loss: 6.874722748761997e-05\n",
      "Epoch 3225: train loss: 6.86529601807706e-05\n",
      "Epoch 3226: train loss: 6.855885294498876e-05\n",
      "Epoch 3227: train loss: 6.846509495517239e-05\n",
      "Epoch 3228: train loss: 6.837128603365272e-05\n",
      "Epoch 3229: train loss: 6.827776087448001e-05\n",
      "Epoch 3230: train loss: 6.818470865255222e-05\n",
      "Epoch 3231: train loss: 6.809074693592265e-05\n",
      "Epoch 3232: train loss: 6.799824768677354e-05\n",
      "Epoch 3233: train loss: 6.790525367250666e-05\n",
      "Epoch 3234: train loss: 6.781188130844384e-05\n",
      "Epoch 3235: train loss: 6.771912012482062e-05\n",
      "Epoch 3236: train loss: 6.762791599612683e-05\n",
      "Epoch 3237: train loss: 6.753459456376731e-05\n",
      "Epoch 3238: train loss: 6.744259007973596e-05\n",
      "Epoch 3239: train loss: 6.735041824867949e-05\n",
      "Epoch 3240: train loss: 6.725919229211286e-05\n",
      "Epoch 3241: train loss: 6.716735515510663e-05\n",
      "Epoch 3242: train loss: 6.707575084874406e-05\n",
      "Epoch 3243: train loss: 6.698446668451652e-05\n",
      "Epoch 3244: train loss: 6.689398287562653e-05\n",
      "Epoch 3245: train loss: 6.680327351205051e-05\n",
      "Epoch 3246: train loss: 6.671238952549174e-05\n",
      "Epoch 3247: train loss: 6.66217747493647e-05\n",
      "Epoch 3248: train loss: 6.653139280388132e-05\n",
      "Epoch 3249: train loss: 6.644054519711062e-05\n",
      "Epoch 3250: train loss: 6.635021418333054e-05\n",
      "Epoch 3251: train loss: 6.626085814787075e-05\n",
      "Epoch 3252: train loss: 6.617133476538584e-05\n",
      "Epoch 3253: train loss: 6.608111289096996e-05\n",
      "Epoch 3254: train loss: 6.59919751342386e-05\n",
      "Epoch 3255: train loss: 6.590246630366892e-05\n",
      "Epoch 3256: train loss: 6.581343768630177e-05\n",
      "Epoch 3257: train loss: 6.572350685019046e-05\n",
      "Epoch 3258: train loss: 6.56352422083728e-05\n",
      "Epoch 3259: train loss: 6.554627907462418e-05\n",
      "Epoch 3260: train loss: 6.545791984535754e-05\n",
      "Epoch 3261: train loss: 6.536937871715054e-05\n",
      "Epoch 3262: train loss: 6.528125959448516e-05\n",
      "Epoch 3263: train loss: 6.519281305372715e-05\n",
      "Epoch 3264: train loss: 6.510495586553589e-05\n",
      "Epoch 3265: train loss: 6.501717871287838e-05\n",
      "Epoch 3266: train loss: 6.492930697277188e-05\n",
      "Epoch 3267: train loss: 6.484143523266539e-05\n",
      "Epoch 3268: train loss: 6.475430564023554e-05\n",
      "Epoch 3269: train loss: 6.466798367910087e-05\n",
      "Epoch 3270: train loss: 6.458016287069768e-05\n",
      "Epoch 3271: train loss: 6.449352076742798e-05\n",
      "Epoch 3272: train loss: 6.44067840767093e-05\n",
      "Epoch 3273: train loss: 6.432022928493097e-05\n",
      "Epoch 3274: train loss: 6.42338054603897e-05\n",
      "Epoch 3275: train loss: 6.414811650756747e-05\n",
      "Epoch 3276: train loss: 6.406140164472163e-05\n",
      "Epoch 3277: train loss: 6.397550168912858e-05\n",
      "Epoch 3278: train loss: 6.388967449311167e-05\n",
      "Epoch 3279: train loss: 6.380333798006177e-05\n",
      "Epoch 3280: train loss: 6.371786003001034e-05\n",
      "Epoch 3281: train loss: 6.363218562910333e-05\n",
      "Epoch 3282: train loss: 6.354759534588084e-05\n",
      "Epoch 3283: train loss: 6.346237933030352e-05\n",
      "Epoch 3284: train loss: 6.337706872727722e-05\n",
      "Epoch 3285: train loss: 6.32927767583169e-05\n",
      "Epoch 3286: train loss: 6.320817192317918e-05\n",
      "Epoch 3287: train loss: 6.312329787760973e-05\n",
      "Epoch 3288: train loss: 6.3038693042472e-05\n",
      "Epoch 3289: train loss: 6.295504135778174e-05\n",
      "Epoch 3290: train loss: 6.287031283136457e-05\n",
      "Epoch 3291: train loss: 6.27869667368941e-05\n",
      "Epoch 3292: train loss: 6.270285666687414e-05\n",
      "Epoch 3293: train loss: 6.261872476898134e-05\n",
      "Epoch 3294: train loss: 6.253559695323929e-05\n",
      "Epoch 3295: train loss: 6.245201802812517e-05\n",
      "Epoch 3296: train loss: 6.236909393919632e-05\n",
      "Epoch 3297: train loss: 6.22859297436662e-05\n",
      "Epoch 3298: train loss: 6.220326031325385e-05\n",
      "Epoch 3299: train loss: 6.212000880623236e-05\n",
      "Epoch 3300: train loss: 6.203708471730351e-05\n",
      "Epoch 3301: train loss: 6.195419700816274e-05\n",
      "Epoch 3302: train loss: 6.187208055052906e-05\n",
      "Epoch 3303: train loss: 6.179032061481848e-05\n",
      "Epoch 3304: train loss: 6.170757114887238e-05\n",
      "Epoch 3305: train loss: 6.162623321870342e-05\n",
      "Epoch 3306: train loss: 6.154402944957837e-05\n",
      "Epoch 3307: train loss: 6.14621676504612e-05\n",
      "Epoch 3308: train loss: 6.13802985753864e-05\n",
      "Epoch 3309: train loss: 6.129901885287836e-05\n",
      "Epoch 3310: train loss: 6.121785554569215e-05\n",
      "Epoch 3311: train loss: 6.113675772212446e-05\n",
      "Epoch 3312: train loss: 6.105543434387073e-05\n",
      "Epoch 3313: train loss: 6.097453660913743e-05\n",
      "Epoch 3314: train loss: 6.089479575166479e-05\n",
      "Epoch 3315: train loss: 6.081346873543225e-05\n",
      "Epoch 3316: train loss: 6.0733269492629915e-05\n",
      "Epoch 3317: train loss: 6.0652837419183925e-05\n",
      "Epoch 3318: train loss: 6.057248174329288e-05\n",
      "Epoch 3319: train loss: 6.049182775313966e-05\n",
      "Epoch 3320: train loss: 6.041255619493313e-05\n",
      "Epoch 3321: train loss: 6.033207682776265e-05\n",
      "Epoch 3322: train loss: 6.025203038007021e-05\n",
      "Epoch 3323: train loss: 6.017315172357485e-05\n",
      "Epoch 3324: train loss: 6.009327989886515e-05\n",
      "Epoch 3325: train loss: 6.001459405524656e-05\n",
      "Epoch 3326: train loss: 5.993498780298978e-05\n",
      "Epoch 3327: train loss: 5.985590905766003e-05\n",
      "Epoch 3328: train loss: 5.9777306887554005e-05\n",
      "Epoch 3329: train loss: 5.9698300901800394e-05\n",
      "Epoch 3330: train loss: 5.9619778767228127e-05\n",
      "Epoch 3331: train loss: 5.9541336668189615e-05\n",
      "Epoch 3332: train loss: 5.9463192883413285e-05\n",
      "Epoch 3333: train loss: 5.9384874475654215e-05\n",
      "Epoch 3334: train loss: 5.930606130277738e-05\n",
      "Epoch 3335: train loss: 5.9228776080999523e-05\n",
      "Epoch 3336: train loss: 5.915043584536761e-05\n",
      "Epoch 3337: train loss: 5.907325976295397e-05\n",
      "Epoch 3338: train loss: 5.8995658037019894e-05\n",
      "Epoch 3339: train loss: 5.891778346267529e-05\n",
      "Epoch 3340: train loss: 5.884062193217687e-05\n",
      "Epoch 3341: train loss: 5.876328214071691e-05\n",
      "Epoch 3342: train loss: 5.868620064575225e-05\n",
      "Epoch 3343: train loss: 5.860886085429229e-05\n",
      "Epoch 3344: train loss: 5.853243783349171e-05\n",
      "Epoch 3345: train loss: 5.8455891121411696e-05\n",
      "Epoch 3346: train loss: 5.837907156092115e-05\n",
      "Epoch 3347: train loss: 5.830208465340547e-05\n",
      "Epoch 3348: train loss: 5.822605453431606e-05\n",
      "Epoch 3349: train loss: 5.8149995311396196e-05\n",
      "Epoch 3350: train loss: 5.8073943364433944e-05\n",
      "Epoch 3351: train loss: 5.799778227810748e-05\n",
      "Epoch 3352: train loss: 5.792197771370411e-05\n",
      "Epoch 3353: train loss: 5.784643508377485e-05\n",
      "Epoch 3354: train loss: 5.777115802629851e-05\n",
      "Epoch 3355: train loss: 5.7695644500199705e-05\n",
      "Epoch 3356: train loss: 5.76197198824957e-05\n",
      "Epoch 3357: train loss: 5.75439516978804e-05\n",
      "Epoch 3358: train loss: 5.746972965425812e-05\n",
      "Epoch 3359: train loss: 5.7394365285290405e-05\n",
      "Epoch 3360: train loss: 5.731927740271203e-05\n",
      "Epoch 3361: train loss: 5.724542643292807e-05\n",
      "Epoch 3362: train loss: 5.717014573747292e-05\n",
      "Epoch 3363: train loss: 5.709535616915673e-05\n",
      "Epoch 3364: train loss: 5.7021858083317056e-05\n",
      "Epoch 3365: train loss: 5.694707579095848e-05\n",
      "Epoch 3366: train loss: 5.687273369403556e-05\n",
      "Epoch 3367: train loss: 5.679911555489525e-05\n",
      "Epoch 3368: train loss: 5.6725195463513955e-05\n",
      "Epoch 3369: train loss: 5.6651493650861084e-05\n",
      "Epoch 3370: train loss: 5.657687142957002e-05\n",
      "Epoch 3371: train loss: 5.650347156915814e-05\n",
      "Epoch 3372: train loss: 5.6430231779813766e-05\n",
      "Epoch 3373: train loss: 5.635681009152904e-05\n",
      "Epoch 3374: train loss: 5.6283704907400534e-05\n",
      "Epoch 3375: train loss: 5.621087984764017e-05\n",
      "Epoch 3376: train loss: 5.613753091893159e-05\n",
      "Epoch 3377: train loss: 5.60647931706626e-05\n",
      "Epoch 3378: train loss: 5.599219730356708e-05\n",
      "Epoch 3379: train loss: 5.59189502382651e-05\n",
      "Epoch 3380: train loss: 5.5847140174591914e-05\n",
      "Epoch 3381: train loss: 5.5774347856640816e-05\n",
      "Epoch 3382: train loss: 5.5702435929561034e-05\n",
      "Epoch 3383: train loss: 5.563009108300321e-05\n",
      "Epoch 3384: train loss: 5.5557717132614926e-05\n",
      "Epoch 3385: train loss: 5.548623812501319e-05\n",
      "Epoch 3386: train loss: 5.541331120184623e-05\n",
      "Epoch 3387: train loss: 5.534219235414639e-05\n",
      "Epoch 3388: train loss: 5.527069515665062e-05\n",
      "Epoch 3389: train loss: 5.5199372582137585e-05\n",
      "Epoch 3390: train loss: 5.5128122767200693e-05\n",
      "Epoch 3391: train loss: 5.505669832928106e-05\n",
      "Epoch 3392: train loss: 5.498573591466993e-05\n",
      "Epoch 3393: train loss: 5.491437332239002e-05\n",
      "Epoch 3394: train loss: 5.48433126823511e-05\n",
      "Epoch 3395: train loss: 5.4772663133917376e-05\n",
      "Epoch 3396: train loss: 5.470195901580155e-05\n",
      "Epoch 3397: train loss: 5.463118577608839e-05\n",
      "Epoch 3398: train loss: 5.4561300203204155e-05\n",
      "Epoch 3399: train loss: 5.4490250477101654e-05\n",
      "Epoch 3400: train loss: 5.441999019240029e-05\n",
      "Epoch 3401: train loss: 5.4349824495147914e-05\n",
      "Epoch 3402: train loss: 5.427977885119617e-05\n",
      "Epoch 3403: train loss: 5.4210111557040364e-05\n",
      "Epoch 3404: train loss: 5.4139673011377454e-05\n",
      "Epoch 3405: train loss: 5.4070635087555274e-05\n",
      "Epoch 3406: train loss: 5.4001047828933224e-05\n",
      "Epoch 3407: train loss: 5.393144965637475e-05\n",
      "Epoch 3408: train loss: 5.386219345382415e-05\n",
      "Epoch 3409: train loss: 5.3793195547768846e-05\n",
      "Epoch 3410: train loss: 5.372333180275746e-05\n",
      "Epoch 3411: train loss: 5.365422839531675e-05\n",
      "Epoch 3412: train loss: 5.358596899895929e-05\n",
      "Epoch 3413: train loss: 5.351668005459942e-05\n",
      "Epoch 3414: train loss: 5.344790770323016e-05\n",
      "Epoch 3415: train loss: 5.337939001037739e-05\n",
      "Epoch 3416: train loss: 5.331063221092336e-05\n",
      "Epoch 3417: train loss: 5.3242001740727574e-05\n",
      "Epoch 3418: train loss: 5.317399700288661e-05\n",
      "Epoch 3419: train loss: 5.3105737606529146e-05\n",
      "Epoch 3420: train loss: 5.303719808580354e-05\n",
      "Epoch 3421: train loss: 5.2969502576161176e-05\n",
      "Epoch 3422: train loss: 5.2902141760569066e-05\n",
      "Epoch 3423: train loss: 5.283447535475716e-05\n",
      "Epoch 3424: train loss: 5.276608862914145e-05\n",
      "Epoch 3425: train loss: 5.269898611004464e-05\n",
      "Epoch 3426: train loss: 5.263115599518642e-05\n",
      "Epoch 3427: train loss: 5.2563675126293674e-05\n",
      "Epoch 3428: train loss: 5.249675814411603e-05\n",
      "Epoch 3429: train loss: 5.242929910309613e-05\n",
      "Epoch 3430: train loss: 5.2362152928253636e-05\n",
      "Epoch 3431: train loss: 5.229507223702967e-05\n",
      "Epoch 3432: train loss: 5.222847539698705e-05\n",
      "Epoch 3433: train loss: 5.216177669353783e-05\n",
      "Epoch 3434: train loss: 5.209483060752973e-05\n",
      "Epoch 3435: train loss: 5.2028452046215534e-05\n",
      "Epoch 3436: train loss: 5.1961502322228625e-05\n",
      "Epoch 3437: train loss: 5.189523653825745e-05\n",
      "Epoch 3438: train loss: 5.182896347832866e-05\n",
      "Epoch 3439: train loss: 5.1763123337877914e-05\n",
      "Epoch 3440: train loss: 5.16970430908259e-05\n",
      "Epoch 3441: train loss: 5.163114474271424e-05\n",
      "Epoch 3442: train loss: 5.156534462003037e-05\n",
      "Epoch 3443: train loss: 5.1499144319677725e-05\n",
      "Epoch 3444: train loss: 5.143376984051429e-05\n",
      "Epoch 3445: train loss: 5.136823529028334e-05\n",
      "Epoch 3446: train loss: 5.1302627980476245e-05\n",
      "Epoch 3447: train loss: 5.123715891386382e-05\n",
      "Epoch 3448: train loss: 5.1172046369174495e-05\n",
      "Epoch 3449: train loss: 5.110674101160839e-05\n",
      "Epoch 3450: train loss: 5.104161027702503e-05\n",
      "Epoch 3451: train loss: 5.097701432532631e-05\n",
      "Epoch 3452: train loss: 5.091163984616287e-05\n",
      "Epoch 3453: train loss: 5.084685471956618e-05\n",
      "Epoch 3454: train loss: 5.0782680773409083e-05\n",
      "Epoch 3455: train loss: 5.0717721023829654e-05\n",
      "Epoch 3456: train loss: 5.065319055574946e-05\n",
      "Epoch 3457: train loss: 5.058837996330112e-05\n",
      "Epoch 3458: train loss: 5.052455890108831e-05\n",
      "Epoch 3459: train loss: 5.046013029641472e-05\n",
      "Epoch 3460: train loss: 5.0396112783346325e-05\n",
      "Epoch 3461: train loss: 5.033206980442628e-05\n",
      "Epoch 3462: train loss: 5.0268747145310044e-05\n",
      "Epoch 3463: train loss: 5.020448224968277e-05\n",
      "Epoch 3464: train loss: 5.014041380491108e-05\n",
      "Epoch 3465: train loss: 5.007649087929167e-05\n",
      "Epoch 3466: train loss: 5.0013513828162104e-05\n",
      "Epoch 3467: train loss: 4.995068229618482e-05\n",
      "Epoch 3468: train loss: 4.9887097702594474e-05\n",
      "Epoch 3469: train loss: 4.982366954209283e-05\n",
      "Epoch 3470: train loss: 4.9760368710849434e-05\n",
      "Epoch 3471: train loss: 4.969732617610134e-05\n",
      "Epoch 3472: train loss: 4.9634414608590305e-05\n",
      "Epoch 3473: train loss: 4.957127384841442e-05\n",
      "Epoch 3474: train loss: 4.950884249410592e-05\n",
      "Epoch 3475: train loss: 4.9446342018200085e-05\n",
      "Epoch 3476: train loss: 4.9383346777176484e-05\n",
      "Epoch 3477: train loss: 4.932091906084679e-05\n",
      "Epoch 3478: train loss: 4.925851317238994e-05\n",
      "Epoch 3479: train loss: 4.919637649436481e-05\n",
      "Epoch 3480: train loss: 4.913390876026824e-05\n",
      "Epoch 3481: train loss: 4.9071884859586135e-05\n",
      "Epoch 3482: train loss: 4.900962085230276e-05\n",
      "Epoch 3483: train loss: 4.894810263067484e-05\n",
      "Epoch 3484: train loss: 4.888598414254375e-05\n",
      "Epoch 3485: train loss: 4.882404755335301e-05\n",
      "Epoch 3486: train loss: 4.8762536607682705e-05\n",
      "Epoch 3487: train loss: 4.87012330268044e-05\n",
      "Epoch 3488: train loss: 4.863935828325339e-05\n",
      "Epoch 3489: train loss: 4.8577989218756557e-05\n",
      "Epoch 3490: train loss: 4.8516809329157695e-05\n",
      "Epoch 3491: train loss: 4.84557676827535e-05\n",
      "Epoch 3492: train loss: 4.8393860197393224e-05\n",
      "Epoch 3493: train loss: 4.833284401684068e-05\n",
      "Epoch 3494: train loss: 4.827231168746948e-05\n",
      "Epoch 3495: train loss: 4.821163747692481e-05\n",
      "Epoch 3496: train loss: 4.815064312424511e-05\n",
      "Epoch 3497: train loss: 4.809055462828837e-05\n",
      "Epoch 3498: train loss: 4.802923649549484e-05\n",
      "Epoch 3499: train loss: 4.7969166189432144e-05\n",
      "Epoch 3500: train loss: 4.79084046673961e-05\n",
      "Epoch 3501: train loss: 4.784786870004609e-05\n",
      "Epoch 3502: train loss: 4.7787660150788724e-05\n",
      "Epoch 3503: train loss: 4.772767351823859e-05\n",
      "Epoch 3504: train loss: 4.7667697799624875e-05\n",
      "Epoch 3505: train loss: 4.760716183227487e-05\n",
      "Epoch 3506: train loss: 4.7547247959300876e-05\n",
      "Epoch 3507: train loss: 4.7487919800914824e-05\n",
      "Epoch 3508: train loss: 4.742747114505619e-05\n",
      "Epoch 3509: train loss: 4.736813571071252e-05\n",
      "Epoch 3510: train loss: 4.7308712964877486e-05\n",
      "Epoch 3511: train loss: 4.7249410272343084e-05\n",
      "Epoch 3512: train loss: 4.7189565520966426e-05\n",
      "Epoch 3513: train loss: 4.71299936180003e-05\n",
      "Epoch 3514: train loss: 4.7071156586753204e-05\n",
      "Epoch 3515: train loss: 4.7011988499434665e-05\n",
      "Epoch 3516: train loss: 4.695335519500077e-05\n",
      "Epoch 3517: train loss: 4.68937105324585e-05\n",
      "Epoch 3518: train loss: 4.683509541791864e-05\n",
      "Epoch 3519: train loss: 4.6775843657087535e-05\n",
      "Epoch 3520: train loss: 4.67172940261662e-05\n",
      "Epoch 3521: train loss: 4.665859160013497e-05\n",
      "Epoch 3522: train loss: 4.660046397475526e-05\n",
      "Epoch 3523: train loss: 4.65420562250074e-05\n",
      "Epoch 3524: train loss: 4.648412505048327e-05\n",
      "Epoch 3525: train loss: 4.6425666369032115e-05\n",
      "Epoch 3526: train loss: 4.636718222172931e-05\n",
      "Epoch 3527: train loss: 4.630897456081584e-05\n",
      "Epoch 3528: train loss: 4.625111250788905e-05\n",
      "Epoch 3529: train loss: 4.619246828951873e-05\n",
      "Epoch 3530: train loss: 4.6135482989484444e-05\n",
      "Epoch 3531: train loss: 4.6077802835498005e-05\n",
      "Epoch 3532: train loss: 4.6019897126825526e-05\n",
      "Epoch 3533: train loss: 4.596179860527627e-05\n",
      "Epoch 3534: train loss: 4.590431854012422e-05\n",
      "Epoch 3535: train loss: 4.584659109241329e-05\n",
      "Epoch 3536: train loss: 4.578964217216708e-05\n",
      "Epoch 3537: train loss: 4.573233672999777e-05\n",
      "Epoch 3538: train loss: 4.567506766761653e-05\n",
      "Epoch 3539: train loss: 4.561790046864189e-05\n",
      "Epoch 3540: train loss: 4.55601402791217e-05\n",
      "Epoch 3541: train loss: 4.5503689761972055e-05\n",
      "Epoch 3542: train loss: 4.5447086449712515e-05\n",
      "Epoch 3543: train loss: 4.539019937510602e-05\n",
      "Epoch 3544: train loss: 4.5332890294957906e-05\n",
      "Epoch 3545: train loss: 4.527678538579494e-05\n",
      "Epoch 3546: train loss: 4.5219658204587176e-05\n",
      "Epoch 3547: train loss: 4.516296030487865e-05\n",
      "Epoch 3548: train loss: 4.510751023190096e-05\n",
      "Epoch 3549: train loss: 4.505036849877797e-05\n",
      "Epoch 3550: train loss: 4.49945691798348e-05\n",
      "Epoch 3551: train loss: 4.4937907659914345e-05\n",
      "Epoch 3552: train loss: 4.4882068323204294e-05\n",
      "Epoch 3553: train loss: 4.482615986489691e-05\n",
      "Epoch 3554: train loss: 4.4769887608708814e-05\n",
      "Epoch 3555: train loss: 4.4714106479659677e-05\n",
      "Epoch 3556: train loss: 4.4657994294539094e-05\n",
      "Epoch 3557: train loss: 4.4602438720175996e-05\n",
      "Epoch 3558: train loss: 4.4546861317940056e-05\n",
      "Epoch 3559: train loss: 4.449154585017823e-05\n",
      "Epoch 3560: train loss: 4.4436194002628326e-05\n",
      "Epoch 3561: train loss: 4.438059841049835e-05\n",
      "Epoch 3562: train loss: 4.432509740581736e-05\n",
      "Epoch 3563: train loss: 4.426977830007672e-05\n",
      "Epoch 3564: train loss: 4.4214197259861976e-05\n",
      "Epoch 3565: train loss: 4.4159372919239104e-05\n",
      "Epoch 3566: train loss: 4.4104530388722196e-05\n",
      "Epoch 3567: train loss: 4.40492840425577e-05\n",
      "Epoch 3568: train loss: 4.399469253257848e-05\n",
      "Epoch 3569: train loss: 4.393967174110003e-05\n",
      "Epoch 3570: train loss: 4.388561501400545e-05\n",
      "Epoch 3571: train loss: 4.38301867689006e-05\n",
      "Epoch 3572: train loss: 4.377609002403915e-05\n",
      "Epoch 3573: train loss: 4.372171315480955e-05\n",
      "Epoch 3574: train loss: 4.3667030695360154e-05\n",
      "Epoch 3575: train loss: 4.3612566514639184e-05\n",
      "Epoch 3576: train loss: 4.3558389734243974e-05\n",
      "Epoch 3577: train loss: 4.3503761844476685e-05\n",
      "Epoch 3578: train loss: 4.344938133726828e-05\n",
      "Epoch 3579: train loss: 4.339592851465568e-05\n",
      "Epoch 3580: train loss: 4.334156619734131e-05\n",
      "Epoch 3581: train loss: 4.328777140472084e-05\n",
      "Epoch 3582: train loss: 4.323400207795203e-05\n",
      "Epoch 3583: train loss: 4.3180767534067854e-05\n",
      "Epoch 3584: train loss: 4.312668534112163e-05\n",
      "Epoch 3585: train loss: 4.307350900489837e-05\n",
      "Epoch 3586: train loss: 4.301969238440506e-05\n",
      "Epoch 3587: train loss: 4.296606857678853e-05\n",
      "Epoch 3588: train loss: 4.2912386561511084e-05\n",
      "Epoch 3589: train loss: 4.285923205316067e-05\n",
      "Epoch 3590: train loss: 4.280628127162345e-05\n",
      "Epoch 3591: train loss: 4.275330502423458e-05\n",
      "Epoch 3592: train loss: 4.270045610610396e-05\n",
      "Epoch 3593: train loss: 4.264703602530062e-05\n",
      "Epoch 3594: train loss: 4.259421621100046e-05\n",
      "Epoch 3595: train loss: 4.25411963078659e-05\n",
      "Epoch 3596: train loss: 4.248792538419366e-05\n",
      "Epoch 3597: train loss: 4.243582225171849e-05\n",
      "Epoch 3598: train loss: 4.238268593326211e-05\n",
      "Epoch 3599: train loss: 4.233009531162679e-05\n",
      "Epoch 3600: train loss: 4.227747194818221e-05\n",
      "Epoch 3601: train loss: 4.2225427023367956e-05\n",
      "Epoch 3602: train loss: 4.217305104248226e-05\n",
      "Epoch 3603: train loss: 4.212042404105887e-05\n",
      "Epoch 3604: train loss: 4.206832454656251e-05\n",
      "Epoch 3605: train loss: 4.201609408482909e-05\n",
      "Epoch 3606: train loss: 4.196403460809961e-05\n",
      "Epoch 3607: train loss: 4.191213520243764e-05\n",
      "Epoch 3608: train loss: 4.186025762464851e-05\n",
      "Epoch 3609: train loss: 4.1808190871961415e-05\n",
      "Epoch 3610: train loss: 4.175636058789678e-05\n",
      "Epoch 3611: train loss: 4.170465763309039e-05\n",
      "Epoch 3612: train loss: 4.16533621319104e-05\n",
      "Epoch 3613: train loss: 4.160138632869348e-05\n",
      "Epoch 3614: train loss: 4.15498398069758e-05\n",
      "Epoch 3615: train loss: 4.1498878999846056e-05\n",
      "Epoch 3616: train loss: 4.144722697674297e-05\n",
      "Epoch 3617: train loss: 4.139592419960536e-05\n",
      "Epoch 3618: train loss: 4.13444395235274e-05\n",
      "Epoch 3619: train loss: 4.129352237214334e-05\n",
      "Epoch 3620: train loss: 4.124243787373416e-05\n",
      "Epoch 3621: train loss: 4.119157893001102e-05\n",
      "Epoch 3622: train loss: 4.1140297980746254e-05\n",
      "Epoch 3623: train loss: 4.108921348233707e-05\n",
      "Epoch 3624: train loss: 4.1038838389795274e-05\n",
      "Epoch 3625: train loss: 4.098773933947086e-05\n",
      "Epoch 3626: train loss: 4.093657844350673e-05\n",
      "Epoch 3627: train loss: 4.088650530320592e-05\n",
      "Epoch 3628: train loss: 4.083620660821907e-05\n",
      "Epoch 3629: train loss: 4.078538768226281e-05\n",
      "Epoch 3630: train loss: 4.073501986567862e-05\n",
      "Epoch 3631: train loss: 4.068485213792883e-05\n",
      "Epoch 3632: train loss: 4.063413507537916e-05\n",
      "Epoch 3633: train loss: 4.0584258385933936e-05\n",
      "Epoch 3634: train loss: 4.053360316902399e-05\n",
      "Epoch 3635: train loss: 4.0484224882675335e-05\n",
      "Epoch 3636: train loss: 4.043388616992161e-05\n",
      "Epoch 3637: train loss: 4.0383965824730694e-05\n",
      "Epoch 3638: train loss: 4.03339508920908e-05\n",
      "Epoch 3639: train loss: 4.0284441638505086e-05\n",
      "Epoch 3640: train loss: 4.023406290798448e-05\n",
      "Epoch 3641: train loss: 4.0184710087487474e-05\n",
      "Epoch 3642: train loss: 4.0134986193152145e-05\n",
      "Epoch 3643: train loss: 4.008578980574384e-05\n",
      "Epoch 3644: train loss: 4.003610592917539e-05\n",
      "Epoch 3645: train loss: 3.998696047347039e-05\n",
      "Epoch 3646: train loss: 3.993756035924889e-05\n",
      "Epoch 3647: train loss: 3.9887821912998334e-05\n",
      "Epoch 3648: train loss: 3.983902934123762e-05\n",
      "Epoch 3649: train loss: 3.9789712900528684e-05\n",
      "Epoch 3650: train loss: 3.9740527427056804e-05\n",
      "Epoch 3651: train loss: 3.9691505662631243e-05\n",
      "Epoch 3652: train loss: 3.964268398704007e-05\n",
      "Epoch 3653: train loss: 3.9594309782842174e-05\n",
      "Epoch 3654: train loss: 3.954501880798489e-05\n",
      "Epoch 3655: train loss: 3.9496113458881155e-05\n",
      "Epoch 3656: train loss: 3.944748459616676e-05\n",
      "Epoch 3657: train loss: 3.939923772122711e-05\n",
      "Epoch 3658: train loss: 3.9350281440420076e-05\n",
      "Epoch 3659: train loss: 3.930189268430695e-05\n",
      "Epoch 3660: train loss: 3.925333294318989e-05\n",
      "Epoch 3661: train loss: 3.920520975952968e-05\n",
      "Epoch 3662: train loss: 3.915689012501389e-05\n",
      "Epoch 3663: train loss: 3.9108410419430584e-05\n",
      "Epoch 3664: train loss: 3.906057827407494e-05\n",
      "Epoch 3665: train loss: 3.901193485944532e-05\n",
      "Epoch 3666: train loss: 3.896445923601277e-05\n",
      "Epoch 3667: train loss: 3.891609230777249e-05\n",
      "Epoch 3668: train loss: 3.886828199028969e-05\n",
      "Epoch 3669: train loss: 3.8820449844934046e-05\n",
      "Epoch 3670: train loss: 3.8772403058828786e-05\n",
      "Epoch 3671: train loss: 3.872458546538837e-05\n",
      "Epoch 3672: train loss: 3.867733175866306e-05\n",
      "Epoch 3673: train loss: 3.862942321575247e-05\n",
      "Epoch 3674: train loss: 3.858204581774771e-05\n",
      "Epoch 3675: train loss: 3.853465022984892e-05\n",
      "Epoch 3676: train loss: 3.8487349229399115e-05\n",
      "Epoch 3677: train loss: 3.843992089969106e-05\n",
      "Epoch 3678: train loss: 3.839255805360153e-05\n",
      "Epoch 3679: train loss: 3.8345322536770254e-05\n",
      "Epoch 3680: train loss: 3.829868001048453e-05\n",
      "Epoch 3681: train loss: 3.825116436928511e-05\n",
      "Epoch 3682: train loss: 3.8204296288313344e-05\n",
      "Epoch 3683: train loss: 3.8157435483299196e-05\n",
      "Epoch 3684: train loss: 3.8110225432319567e-05\n",
      "Epoch 3685: train loss: 3.806302265729755e-05\n",
      "Epoch 3686: train loss: 3.8016511098248884e-05\n",
      "Epoch 3687: train loss: 3.796965393121354e-05\n",
      "Epoch 3688: train loss: 3.792292773141526e-05\n",
      "Epoch 3689: train loss: 3.787668174481951e-05\n",
      "Epoch 3690: train loss: 3.7830111978109926e-05\n",
      "Epoch 3691: train loss: 3.778358586714603e-05\n",
      "Epoch 3692: train loss: 3.773730350076221e-05\n",
      "Epoch 3693: train loss: 3.769065733649768e-05\n",
      "Epoch 3694: train loss: 3.7644254916813225e-05\n",
      "Epoch 3695: train loss: 3.75981762772426e-05\n",
      "Epoch 3696: train loss: 3.755213037948124e-05\n",
      "Epoch 3697: train loss: 3.7506066291825846e-05\n",
      "Epoch 3698: train loss: 3.7459660234162584e-05\n",
      "Epoch 3699: train loss: 3.741417094715871e-05\n",
      "Epoch 3700: train loss: 3.736788858077489e-05\n",
      "Epoch 3701: train loss: 3.7322435673559085e-05\n",
      "Epoch 3702: train loss: 3.727649891516194e-05\n",
      "Epoch 3703: train loss: 3.723049667314626e-05\n",
      "Epoch 3704: train loss: 3.71850146621e-05\n",
      "Epoch 3705: train loss: 3.7139314372325316e-05\n",
      "Epoch 3706: train loss: 3.709367956616916e-05\n",
      "Epoch 3707: train loss: 3.704811024363153e-05\n",
      "Epoch 3708: train loss: 3.700311208376661e-05\n",
      "Epoch 3709: train loss: 3.695744453580119e-05\n",
      "Epoch 3710: train loss: 3.6912439099978656e-05\n",
      "Epoch 3711: train loss: 3.686700074467808e-05\n",
      "Epoch 3712: train loss: 3.682166789076291e-05\n",
      "Epoch 3713: train loss: 3.677655695355497e-05\n",
      "Epoch 3714: train loss: 3.6731646105181426e-05\n",
      "Epoch 3715: train loss: 3.6686502426164225e-05\n",
      "Epoch 3716: train loss: 3.664161704364233e-05\n",
      "Epoch 3717: train loss: 3.659646608866751e-05\n",
      "Epoch 3718: train loss: 3.65517771570012e-05\n",
      "Epoch 3719: train loss: 3.650697908597067e-05\n",
      "Epoch 3720: train loss: 3.6461839044932276e-05\n",
      "Epoch 3721: train loss: 3.641729199443944e-05\n",
      "Epoch 3722: train loss: 3.637283589341678e-05\n",
      "Epoch 3723: train loss: 3.632820153143257e-05\n",
      "Epoch 3724: train loss: 3.628352715168148e-05\n",
      "Epoch 3725: train loss: 3.623938755481504e-05\n",
      "Epoch 3726: train loss: 3.619486960815266e-05\n",
      "Epoch 3727: train loss: 3.615071909734979e-05\n",
      "Epoch 3728: train loss: 3.610683779697865e-05\n",
      "Epoch 3729: train loss: 3.606233804021031e-05\n",
      "Epoch 3730: train loss: 3.601829303079285e-05\n",
      "Epoch 3731: train loss: 3.597413524403237e-05\n",
      "Epoch 3732: train loss: 3.592963548726402e-05\n",
      "Epoch 3733: train loss: 3.588611798477359e-05\n",
      "Epoch 3734: train loss: 3.584232399589382e-05\n",
      "Epoch 3735: train loss: 3.5798475437331945e-05\n",
      "Epoch 3736: train loss: 3.575403752620332e-05\n",
      "Epoch 3737: train loss: 3.571089109755121e-05\n",
      "Epoch 3738: train loss: 3.5666955227497965e-05\n",
      "Epoch 3739: train loss: 3.56234195351135e-05\n",
      "Epoch 3740: train loss: 3.5580156691139564e-05\n",
      "Epoch 3741: train loss: 3.553636997821741e-05\n",
      "Epoch 3742: train loss: 3.549276516423561e-05\n",
      "Epoch 3743: train loss: 3.544935316313058e-05\n",
      "Epoch 3744: train loss: 3.5406104871071875e-05\n",
      "Epoch 3745: train loss: 3.5362936614546925e-05\n",
      "Epoch 3746: train loss: 3.5319579183124006e-05\n",
      "Epoch 3747: train loss: 3.527645822032355e-05\n",
      "Epoch 3748: train loss: 3.5234032111475244e-05\n",
      "Epoch 3749: train loss: 3.5190198104828596e-05\n",
      "Epoch 3750: train loss: 3.5147280868841335e-05\n",
      "Epoch 3751: train loss: 3.5104301787214354e-05\n",
      "Epoch 3752: train loss: 3.506156645016745e-05\n",
      "Epoch 3753: train loss: 3.5018776543438435e-05\n",
      "Epoch 3754: train loss: 3.4975982998730615e-05\n",
      "Epoch 3755: train loss: 3.4933338611153886e-05\n",
      "Epoch 3756: train loss: 3.489066148176789e-05\n",
      "Epoch 3757: train loss: 3.484809712972492e-05\n",
      "Epoch 3758: train loss: 3.480539453448728e-05\n",
      "Epoch 3759: train loss: 3.476286292425357e-05\n",
      "Epoch 3760: train loss: 3.472004027571529e-05\n",
      "Epoch 3761: train loss: 3.4678381780395284e-05\n",
      "Epoch 3762: train loss: 3.4635573683772236e-05\n",
      "Epoch 3763: train loss: 3.459365325397812e-05\n",
      "Epoch 3764: train loss: 3.455105979810469e-05\n",
      "Epoch 3765: train loss: 3.450888107181527e-05\n",
      "Epoch 3766: train loss: 3.446679329499602e-05\n",
      "Epoch 3767: train loss: 3.4424909244989976e-05\n",
      "Epoch 3768: train loss: 3.438273415667936e-05\n",
      "Epoch 3769: train loss: 3.434087921050377e-05\n",
      "Epoch 3770: train loss: 3.429901335039176e-05\n",
      "Epoch 3771: train loss: 3.425706745474599e-05\n",
      "Epoch 3772: train loss: 3.421536894165911e-05\n",
      "Epoch 3773: train loss: 3.417358675505966e-05\n",
      "Epoch 3774: train loss: 3.413212107261643e-05\n",
      "Epoch 3775: train loss: 3.409021155675873e-05\n",
      "Epoch 3776: train loss: 3.404899325687438e-05\n",
      "Epoch 3777: train loss: 3.4007698559435084e-05\n",
      "Epoch 3778: train loss: 3.396591273485683e-05\n",
      "Epoch 3779: train loss: 3.392461076145992e-05\n",
      "Epoch 3780: train loss: 3.388291224837303e-05\n",
      "Epoch 3781: train loss: 3.384146111784503e-05\n",
      "Epoch 3782: train loss: 3.380044654477388e-05\n",
      "Epoch 3783: train loss: 3.3759006328182295e-05\n",
      "Epoch 3784: train loss: 3.371869752299972e-05\n",
      "Epoch 3785: train loss: 3.36770317517221e-05\n",
      "Epoch 3786: train loss: 3.363635914865881e-05\n",
      "Epoch 3787: train loss: 3.35955701302737e-05\n",
      "Epoch 3788: train loss: 3.355450098752044e-05\n",
      "Epoch 3789: train loss: 3.3513792004669085e-05\n",
      "Epoch 3790: train loss: 3.347304300405085e-05\n",
      "Epoch 3791: train loss: 3.343235584907234e-05\n",
      "Epoch 3792: train loss: 3.3391341276001185e-05\n",
      "Epoch 3793: train loss: 3.335034489282407e-05\n",
      "Epoch 3794: train loss: 3.331009793328121e-05\n",
      "Epoch 3795: train loss: 3.326990190544166e-05\n",
      "Epoch 3796: train loss: 3.322874181321822e-05\n",
      "Epoch 3797: train loss: 3.318880408187397e-05\n",
      "Epoch 3798: train loss: 3.314837158541195e-05\n",
      "Epoch 3799: train loss: 3.310832107672468e-05\n",
      "Epoch 3800: train loss: 3.3067659387597814e-05\n",
      "Epoch 3801: train loss: 3.302713594166562e-05\n",
      "Epoch 3802: train loss: 3.298759475001134e-05\n",
      "Epoch 3803: train loss: 3.294731504865922e-05\n",
      "Epoch 3804: train loss: 3.290706445113756e-05\n",
      "Epoch 3805: train loss: 3.28671740135178e-05\n",
      "Epoch 3806: train loss: 3.282717807451263e-05\n",
      "Epoch 3807: train loss: 3.278747317381203e-05\n",
      "Epoch 3808: train loss: 3.274721166235395e-05\n",
      "Epoch 3809: train loss: 3.270760862505995e-05\n",
      "Epoch 3810: train loss: 3.2668009225744754e-05\n",
      "Epoch 3811: train loss: 3.262816971982829e-05\n",
      "Epoch 3812: train loss: 3.258847937104292e-05\n",
      "Epoch 3813: train loss: 3.254893817938864e-05\n",
      "Epoch 3814: train loss: 3.250963709433563e-05\n",
      "Epoch 3815: train loss: 3.246998312533833e-05\n",
      "Epoch 3816: train loss: 3.2430561986984685e-05\n",
      "Epoch 3817: train loss: 3.2390882552135736e-05\n",
      "Epoch 3818: train loss: 3.235176336602308e-05\n",
      "Epoch 3819: train loss: 3.231253504054621e-05\n",
      "Epoch 3820: train loss: 3.227321940357797e-05\n",
      "Epoch 3821: train loss: 3.2234009267995134e-05\n",
      "Epoch 3822: train loss: 3.219513746444136e-05\n",
      "Epoch 3823: train loss: 3.215606193407439e-05\n",
      "Epoch 3824: train loss: 3.21169791277498e-05\n",
      "Epoch 3825: train loss: 3.207797999493778e-05\n",
      "Epoch 3826: train loss: 3.203903543180786e-05\n",
      "Epoch 3827: train loss: 3.199994171154685e-05\n",
      "Epoch 3828: train loss: 3.1961801141733304e-05\n",
      "Epoch 3829: train loss: 3.192278018104844e-05\n",
      "Epoch 3830: train loss: 3.18839265673887e-05\n",
      "Epoch 3831: train loss: 3.184497472830117e-05\n",
      "Epoch 3832: train loss: 3.180681960657239e-05\n",
      "Epoch 3833: train loss: 3.176785685354844e-05\n",
      "Epoch 3834: train loss: 3.1729283364256844e-05\n",
      "Epoch 3835: train loss: 3.169064802932553e-05\n",
      "Epoch 3836: train loss: 3.1652514735469595e-05\n",
      "Epoch 3837: train loss: 3.161406857543625e-05\n",
      "Epoch 3838: train loss: 3.15758261422161e-05\n",
      "Epoch 3839: train loss: 3.153722718707286e-05\n",
      "Epoch 3840: train loss: 3.149906842736527e-05\n",
      "Epoch 3841: train loss: 3.146100425510667e-05\n",
      "Epoch 3842: train loss: 3.142276545986533e-05\n",
      "Epoch 3843: train loss: 3.13846867356915e-05\n",
      "Epoch 3844: train loss: 3.134708822472021e-05\n",
      "Epoch 3845: train loss: 3.1308754842029884e-05\n",
      "Epoch 3846: train loss: 3.127128366031684e-05\n",
      "Epoch 3847: train loss: 3.123354326817207e-05\n",
      "Epoch 3848: train loss: 3.1195348128676414e-05\n",
      "Epoch 3849: train loss: 3.115756408078596e-05\n",
      "Epoch 3850: train loss: 3.1119769118959084e-05\n",
      "Epoch 3851: train loss: 3.108234886894934e-05\n",
      "Epoch 3852: train loss: 3.104485949734226e-05\n",
      "Epoch 3853: train loss: 3.100688991253264e-05\n",
      "Epoch 3854: train loss: 3.0969396902946755e-05\n",
      "Epoch 3855: train loss: 3.093172563239932e-05\n",
      "Epoch 3856: train loss: 3.089512392762117e-05\n",
      "Epoch 3857: train loss: 3.08572816720698e-05\n",
      "Epoch 3858: train loss: 3.081970862695016e-05\n",
      "Epoch 3859: train loss: 3.078260488109663e-05\n",
      "Epoch 3860: train loss: 3.0745271942578256e-05\n",
      "Epoch 3861: train loss: 3.070794264203869e-05\n",
      "Epoch 3862: train loss: 3.0670969863422215e-05\n",
      "Epoch 3863: train loss: 3.063403710257262e-05\n",
      "Epoch 3864: train loss: 3.059674781979993e-05\n",
      "Epoch 3865: train loss: 3.055958222830668e-05\n",
      "Epoch 3866: train loss: 3.052295505767688e-05\n",
      "Epoch 3867: train loss: 3.0485951356240548e-05\n",
      "Epoch 3868: train loss: 3.044917139050085e-05\n",
      "Epoch 3869: train loss: 3.0412655178224668e-05\n",
      "Epoch 3870: train loss: 3.0375740607269108e-05\n",
      "Epoch 3871: train loss: 3.0339568183990195e-05\n",
      "Epoch 3872: train loss: 3.030215339094866e-05\n",
      "Epoch 3873: train loss: 3.026567901542876e-05\n",
      "Epoch 3874: train loss: 3.0228802643250674e-05\n",
      "Epoch 3875: train loss: 3.019274299731478e-05\n",
      "Epoch 3876: train loss: 3.015641777892597e-05\n",
      "Epoch 3877: train loss: 3.011953413079027e-05\n",
      "Epoch 3878: train loss: 3.008321982633788e-05\n",
      "Epoch 3879: train loss: 3.004666359629482e-05\n",
      "Epoch 3880: train loss: 3.0010773116373457e-05\n",
      "Epoch 3881: train loss: 2.99740659102099e-05\n",
      "Epoch 3882: train loss: 2.9938430088805035e-05\n",
      "Epoch 3883: train loss: 2.9901839297963306e-05\n",
      "Epoch 3884: train loss: 2.9865632313885726e-05\n",
      "Epoch 3885: train loss: 2.9829941922798753e-05\n",
      "Epoch 3886: train loss: 2.97939022857463e-05\n",
      "Epoch 3887: train loss: 2.9757868105662055e-05\n",
      "Epoch 3888: train loss: 2.972202855744399e-05\n",
      "Epoch 3889: train loss: 2.9685983463423327e-05\n",
      "Epoch 3890: train loss: 2.9650396754732355e-05\n",
      "Epoch 3891: train loss: 2.9614575396408327e-05\n",
      "Epoch 3892: train loss: 2.95787540380843e-05\n",
      "Epoch 3893: train loss: 2.95428453682689e-05\n",
      "Epoch 3894: train loss: 2.9507338695111685e-05\n",
      "Epoch 3895: train loss: 2.9471557354554534e-05\n",
      "Epoch 3896: train loss: 2.9436592740239576e-05\n",
      "Epoch 3897: train loss: 2.9400811399682425e-05\n",
      "Epoch 3898: train loss: 2.9365235604927875e-05\n",
      "Epoch 3899: train loss: 2.9329665267141536e-05\n",
      "Epoch 3900: train loss: 2.929500988102518e-05\n",
      "Epoch 3901: train loss: 2.9259430448291823e-05\n",
      "Epoch 3902: train loss: 2.9224025638541207e-05\n",
      "Epoch 3903: train loss: 2.9188422558945604e-05\n",
      "Epoch 3904: train loss: 2.9153667128412053e-05\n",
      "Epoch 3905: train loss: 2.9118276870576665e-05\n",
      "Epoch 3906: train loss: 2.908336500695441e-05\n",
      "Epoch 3907: train loss: 2.904814391513355e-05\n",
      "Epoch 3908: train loss: 2.9012975574005395e-05\n",
      "Epoch 3909: train loss: 2.897832564485725e-05\n",
      "Epoch 3910: train loss: 2.8943650249857455e-05\n",
      "Epoch 3911: train loss: 2.8908409149153158e-05\n",
      "Epoch 3912: train loss: 2.887388109229505e-05\n",
      "Epoch 3913: train loss: 2.883872002712451e-05\n",
      "Epoch 3914: train loss: 2.8804235626012087e-05\n",
      "Epoch 3915: train loss: 2.8769842174369842e-05\n",
      "Epoch 3916: train loss: 2.8735059458995238e-05\n",
      "Epoch 3917: train loss: 2.8700185794150457e-05\n",
      "Epoch 3918: train loss: 2.8665859645116143e-05\n",
      "Epoch 3919: train loss: 2.8631366149056703e-05\n",
      "Epoch 3920: train loss: 2.859661981347017e-05\n",
      "Epoch 3921: train loss: 2.8562282750499435e-05\n",
      "Epoch 3922: train loss: 2.8528449547593482e-05\n",
      "Epoch 3923: train loss: 2.8493799618445337e-05\n",
      "Epoch 3924: train loss: 2.8459464374464005e-05\n",
      "Epoch 3925: train loss: 2.8424903575796634e-05\n",
      "Epoch 3926: train loss: 2.839072658389341e-05\n",
      "Epoch 3927: train loss: 2.8356609618640505e-05\n",
      "Epoch 3928: train loss: 2.832246173056774e-05\n",
      "Epoch 3929: train loss: 2.828809738275595e-05\n",
      "Epoch 3930: train loss: 2.825444971676916e-05\n",
      "Epoch 3931: train loss: 2.8220496460562572e-05\n",
      "Epoch 3932: train loss: 2.8186565032228827e-05\n",
      "Epoch 3933: train loss: 2.8152895538369194e-05\n",
      "Epoch 3934: train loss: 2.8118796763010323e-05\n",
      "Epoch 3935: train loss: 2.808519457175862e-05\n",
      "Epoch 3936: train loss: 2.8051133995177224e-05\n",
      "Epoch 3937: train loss: 2.8017404474667273e-05\n",
      "Epoch 3938: train loss: 2.7983936888631433e-05\n",
      "Epoch 3939: train loss: 2.7950063667958602e-05\n",
      "Epoch 3940: train loss: 2.7916239559999667e-05\n",
      "Epoch 3941: train loss: 2.788305573631078e-05\n",
      "Epoch 3942: train loss: 2.7849522666656412e-05\n",
      "Epoch 3943: train loss: 2.7815764042316005e-05\n",
      "Epoch 3944: train loss: 2.7782425604527816e-05\n",
      "Epoch 3945: train loss: 2.774889253487345e-05\n",
      "Epoch 3946: train loss: 2.7715645046555437e-05\n",
      "Epoch 3947: train loss: 2.768239937722683e-05\n",
      "Epoch 3948: train loss: 2.7648873583530076e-05\n",
      "Epoch 3949: train loss: 2.7615913495537825e-05\n",
      "Epoch 3950: train loss: 2.7582836992223747e-05\n",
      "Epoch 3951: train loss: 2.7549651349545456e-05\n",
      "Epoch 3952: train loss: 2.7516587579157203e-05\n",
      "Epoch 3953: train loss: 2.7483332814881578e-05\n",
      "Epoch 3954: train loss: 2.7450352718005888e-05\n",
      "Epoch 3955: train loss: 2.7417154342401773e-05\n",
      "Epoch 3956: train loss: 2.7384174245526083e-05\n",
      "Epoch 3957: train loss: 2.735194175329525e-05\n",
      "Epoch 3958: train loss: 2.7318927095620893e-05\n",
      "Epoch 3959: train loss: 2.7286110707791522e-05\n",
      "Epoch 3960: train loss: 2.7253439839114435e-05\n",
      "Epoch 3961: train loss: 2.7220265110372566e-05\n",
      "Epoch 3962: train loss: 2.7187865271116607e-05\n",
      "Epoch 3963: train loss: 2.7155196221428923e-05\n",
      "Epoch 3964: train loss: 2.712269633775577e-05\n",
      "Epoch 3965: train loss: 2.709009640966542e-05\n",
      "Epoch 3966: train loss: 2.705742917896714e-05\n",
      "Epoch 3967: train loss: 2.702491110539995e-05\n",
      "Epoch 3968: train loss: 2.699243850656785e-05\n",
      "Epoch 3969: train loss: 2.6959975002682768e-05\n",
      "Epoch 3970: train loss: 2.692812449822668e-05\n",
      "Epoch 3971: train loss: 2.689500615815632e-05\n",
      "Epoch 3972: train loss: 2.6862986487685703e-05\n",
      "Epoch 3973: train loss: 2.6830972274183296e-05\n",
      "Epoch 3974: train loss: 2.679878889466636e-05\n",
      "Epoch 3975: train loss: 2.6766540031530894e-05\n",
      "Epoch 3976: train loss: 2.6734371203929186e-05\n",
      "Epoch 3977: train loss: 2.6702487957663834e-05\n",
      "Epoch 3978: train loss: 2.667045555426739e-05\n",
      "Epoch 3979: train loss: 2.663841405592393e-05\n",
      "Epoch 3980: train loss: 2.660615609784145e-05\n",
      "Epoch 3981: train loss: 2.6574451112537645e-05\n",
      "Epoch 3982: train loss: 2.654270974744577e-05\n",
      "Epoch 3983: train loss: 2.6510788302402943e-05\n",
      "Epoch 3984: train loss: 2.6479005100554787e-05\n",
      "Epoch 3985: train loss: 2.6446859919815324e-05\n",
      "Epoch 3986: train loss: 2.6415331376483664e-05\n",
      "Epoch 3987: train loss: 2.6383428121334873e-05\n",
      "Epoch 3988: train loss: 2.63520469161449e-05\n",
      "Epoch 3989: train loss: 2.6320449251215905e-05\n",
      "Epoch 3990: train loss: 2.6288900699000806e-05\n",
      "Epoch 3991: train loss: 2.62572466454003e-05\n",
      "Epoch 3992: train loss: 2.62259545706911e-05\n",
      "Epoch 3993: train loss: 2.619449696794618e-05\n",
      "Epoch 3994: train loss: 2.6163195798289962e-05\n",
      "Epoch 3995: train loss: 2.613195829326287e-05\n",
      "Epoch 3996: train loss: 2.6100457034772262e-05\n",
      "Epoch 3997: train loss: 2.6068795705214143e-05\n",
      "Epoch 3998: train loss: 2.603801476652734e-05\n",
      "Epoch 3999: train loss: 2.6006606276496314e-05\n",
      "Epoch 4000: train loss: 2.5975406970246695e-05\n",
      "Epoch 4001: train loss: 2.5944611479644664e-05\n",
      "Epoch 4002: train loss: 2.591325574030634e-05\n",
      "Epoch 4003: train loss: 2.5881934561766684e-05\n",
      "Epoch 4004: train loss: 2.5851120881270617e-05\n",
      "Epoch 4005: train loss: 2.5820185328484513e-05\n",
      "Epoch 4006: train loss: 2.5789031496969983e-05\n",
      "Epoch 4007: train loss: 2.5758272386156023e-05\n",
      "Epoch 4008: train loss: 2.572715857240837e-05\n",
      "Epoch 4009: train loss: 2.5696226657601073e-05\n",
      "Epoch 4010: train loss: 2.5665754947112873e-05\n",
      "Epoch 4011: train loss: 2.5635201382101513e-05\n",
      "Epoch 4012: train loss: 2.560420034569688e-05\n",
      "Epoch 4013: train loss: 2.5573459424776956e-05\n",
      "Epoch 4014: train loss: 2.554272214183584e-05\n",
      "Epoch 4015: train loss: 2.551226680225227e-05\n",
      "Epoch 4016: train loss: 2.5481811462668702e-05\n",
      "Epoch 4017: train loss: 2.5451177862123586e-05\n",
      "Epoch 4018: train loss: 2.5420560632483102e-05\n",
      "Epoch 4019: train loss: 2.53900871030055e-05\n",
      "Epoch 4020: train loss: 2.5359757273690775e-05\n",
      "Epoch 4021: train loss: 2.532931830501184e-05\n",
      "Epoch 4022: train loss: 2.5298748369095847e-05\n",
      "Epoch 4023: train loss: 2.5268609533668496e-05\n",
      "Epoch 4024: train loss: 2.523841794754844e-05\n",
      "Epoch 4025: train loss: 2.5208008082699962e-05\n",
      "Epoch 4026: train loss: 2.5177923816954717e-05\n",
      "Epoch 4027: train loss: 2.5147957785520703e-05\n",
      "Epoch 4028: train loss: 2.5117662517004646e-05\n",
      "Epoch 4029: train loss: 2.5087287212954834e-05\n",
      "Epoch 4030: train loss: 2.5057379389181733e-05\n",
      "Epoch 4031: train loss: 2.5027658921317197e-05\n",
      "Epoch 4032: train loss: 2.4997543732752092e-05\n",
      "Epoch 4033: train loss: 2.496735396562144e-05\n",
      "Epoch 4034: train loss: 2.4936996851465665e-05\n",
      "Epoch 4035: train loss: 2.4907683837227523e-05\n",
      "Epoch 4036: train loss: 2.487790879968088e-05\n",
      "Epoch 4037: train loss: 2.484789911250118e-05\n",
      "Epoch 4038: train loss: 2.481812407495454e-05\n",
      "Epoch 4039: train loss: 2.4788578230072744e-05\n",
      "Epoch 4040: train loss: 2.4758717700024135e-05\n",
      "Epoch 4041: train loss: 2.4728899006731808e-05\n",
      "Epoch 4042: train loss: 2.4699185814824887e-05\n",
      "Epoch 4043: train loss: 2.4669718186487444e-05\n",
      "Epoch 4044: train loss: 2.464053250150755e-05\n",
      "Epoch 4045: train loss: 2.4610684704384767e-05\n",
      "Epoch 4046: train loss: 2.4581311663496308e-05\n",
      "Epoch 4047: train loss: 2.4551596652599983e-05\n",
      "Epoch 4048: train loss: 2.452217995596584e-05\n",
      "Epoch 4049: train loss: 2.44929196924204e-05\n",
      "Epoch 4050: train loss: 2.4463459340040572e-05\n",
      "Epoch 4051: train loss: 2.4433953512925655e-05\n",
      "Epoch 4052: train loss: 2.4405204385402612e-05\n",
      "Epoch 4053: train loss: 2.437556213408243e-05\n",
      "Epoch 4054: train loss: 2.4346200007130392e-05\n",
      "Epoch 4055: train loss: 2.4317052520927973e-05\n",
      "Epoch 4056: train loss: 2.428817424515728e-05\n",
      "Epoch 4057: train loss: 2.4259188649011776e-05\n",
      "Epoch 4058: train loss: 2.4230264898505993e-05\n",
      "Epoch 4059: train loss: 2.4200931875384413e-05\n",
      "Epoch 4060: train loss: 2.4172108169295825e-05\n",
      "Epoch 4061: train loss: 2.4142984329955652e-05\n",
      "Epoch 4062: train loss: 2.4114200641633943e-05\n",
      "Epoch 4063: train loss: 2.408480941085145e-05\n",
      "Epoch 4064: train loss: 2.4056367692537606e-05\n",
      "Epoch 4065: train loss: 2.4027080144151114e-05\n",
      "Epoch 4066: train loss: 2.3998758479137905e-05\n",
      "Epoch 4067: train loss: 2.396976742602419e-05\n",
      "Epoch 4068: train loss: 2.39409928326495e-05\n",
      "Epoch 4069: train loss: 2.3912449250929058e-05\n",
      "Epoch 4070: train loss: 2.388371831330005e-05\n",
      "Epoch 4071: train loss: 2.385499101364985e-05\n",
      "Epoch 4072: train loss: 2.3826474716770463e-05\n",
      "Epoch 4073: train loss: 2.3797932954039425e-05\n",
      "Epoch 4074: train loss: 2.3769118342897855e-05\n",
      "Epoch 4075: train loss: 2.374060386500787e-05\n",
      "Epoch 4076: train loss: 2.3712171241641045e-05\n",
      "Epoch 4077: train loss: 2.3683991457801312e-05\n",
      "Epoch 4078: train loss: 2.3655409677303396e-05\n",
      "Epoch 4079: train loss: 2.3627144400961697e-05\n",
      "Epoch 4080: train loss: 2.3598493498866446e-05\n",
      "Epoch 4081: train loss: 2.3570479243062437e-05\n",
      "Epoch 4082: train loss: 2.3542012058896944e-05\n",
      "Epoch 4083: train loss: 2.3514086933573708e-05\n",
      "Epoch 4084: train loss: 2.3485450583393686e-05\n",
      "Epoch 4085: train loss: 2.3457570932805538e-05\n",
      "Epoch 4086: train loss: 2.3429092834703624e-05\n",
      "Epoch 4087: train loss: 2.3401253201882355e-05\n",
      "Epoch 4088: train loss: 2.3373053409159184e-05\n",
      "Epoch 4089: train loss: 2.3345259251073003e-05\n",
      "Epoch 4090: train loss: 2.3316908482229337e-05\n",
      "Epoch 4091: train loss: 2.3289136152016e-05\n",
      "Epoch 4092: train loss: 2.326135836483445e-05\n",
      "Epoch 4093: train loss: 2.3233049432747066e-05\n",
      "Epoch 4094: train loss: 2.3205238903756253e-05\n",
      "Epoch 4095: train loss: 2.3177464754553512e-05\n",
      "Epoch 4096: train loss: 2.3149312255554833e-05\n",
      "Epoch 4097: train loss: 2.3122238417272456e-05\n",
      "Epoch 4098: train loss: 2.3094275093171746e-05\n",
      "Epoch 4099: train loss: 2.3066337234922685e-05\n",
      "Epoch 4100: train loss: 2.3038415747578256e-05\n",
      "Epoch 4101: train loss: 2.3011180019238964e-05\n",
      "Epoch 4102: train loss: 2.2983145754551515e-05\n",
      "Epoch 4103: train loss: 2.295545709785074e-05\n",
      "Epoch 4104: train loss: 2.2928497855900787e-05\n",
      "Epoch 4105: train loss: 2.290051088493783e-05\n",
      "Epoch 4106: train loss: 2.2873186026117764e-05\n",
      "Epoch 4107: train loss: 2.2845402781968005e-05\n",
      "Epoch 4108: train loss: 2.2818290744908154e-05\n",
      "Epoch 4109: train loss: 2.279072032251861e-05\n",
      "Epoch 4110: train loss: 2.2763226297684014e-05\n",
      "Epoch 4111: train loss: 2.2736197934136726e-05\n",
      "Epoch 4112: train loss: 2.2708700271323323e-05\n",
      "Epoch 4113: train loss: 2.2681189875584096e-05\n",
      "Epoch 4114: train loss: 2.265445982629899e-05\n",
      "Epoch 4115: train loss: 2.2626760255661793e-05\n",
      "Epoch 4116: train loss: 2.2600039301323704e-05\n",
      "Epoch 4117: train loss: 2.2572861780645326e-05\n",
      "Epoch 4118: train loss: 2.2545504180015996e-05\n",
      "Epoch 4119: train loss: 2.2518121113535017e-05\n",
      "Epoch 4120: train loss: 2.2490938135888427e-05\n",
      "Epoch 4121: train loss: 2.2464275389211252e-05\n",
      "Epoch 4122: train loss: 2.2437288862420246e-05\n",
      "Epoch 4123: train loss: 2.241013316961471e-05\n",
      "Epoch 4124: train loss: 2.2383024770533666e-05\n",
      "Epoch 4125: train loss: 2.235658212157432e-05\n",
      "Epoch 4126: train loss: 2.232912265753839e-05\n",
      "Epoch 4127: train loss: 2.2302565412246622e-05\n",
      "Epoch 4128: train loss: 2.227563709311653e-05\n",
      "Epoch 4129: train loss: 2.2248897948884405e-05\n",
      "Epoch 4130: train loss: 2.2222257030080073e-05\n",
      "Epoch 4131: train loss: 2.2195465135155246e-05\n",
      "Epoch 4132: train loss: 2.216831489931792e-05\n",
      "Epoch 4133: train loss: 2.2142043235362507e-05\n",
      "Epoch 4134: train loss: 2.211503124271985e-05\n",
      "Epoch 4135: train loss: 2.2088541300036013e-05\n",
      "Epoch 4136: train loss: 2.2062165953684598e-05\n",
      "Epoch 4137: train loss: 2.2035228539607488e-05\n",
      "Epoch 4138: train loss: 2.2009024178260006e-05\n",
      "Epoch 4139: train loss: 2.1982630642014556e-05\n",
      "Epoch 4140: train loss: 2.1956040654913522e-05\n",
      "Epoch 4141: train loss: 2.1929598005954176e-05\n",
      "Epoch 4142: train loss: 2.190310988225974e-05\n",
      "Epoch 4143: train loss: 2.1876861865166575e-05\n",
      "Epoch 4144: train loss: 2.1850313714821823e-05\n",
      "Epoch 4145: train loss: 2.182401658501476e-05\n",
      "Epoch 4146: train loss: 2.179743387387134e-05\n",
      "Epoch 4147: train loss: 2.1771340470877476e-05\n",
      "Epoch 4148: train loss: 2.1745217964053154e-05\n",
      "Epoch 4149: train loss: 2.1718897187383845e-05\n",
      "Epoch 4150: train loss: 2.169299295928795e-05\n",
      "Epoch 4151: train loss: 2.1666868633474223e-05\n",
      "Epoch 4152: train loss: 2.164024772355333e-05\n",
      "Epoch 4153: train loss: 2.161414158763364e-05\n",
      "Epoch 4154: train loss: 2.1588233721558936e-05\n",
      "Epoch 4155: train loss: 2.156219670723658e-05\n",
      "Epoch 4156: train loss: 2.1536257918342017e-05\n",
      "Epoch 4157: train loss: 2.15104464587057e-05\n",
      "Epoch 4158: train loss: 2.1484214812517166e-05\n",
      "Epoch 4159: train loss: 2.1458317860378884e-05\n",
      "Epoch 4160: train loss: 2.143251731467899e-05\n",
      "Epoch 4161: train loss: 2.140678043360822e-05\n",
      "Epoch 4162: train loss: 2.1380688849603757e-05\n",
      "Epoch 4163: train loss: 2.135523573087994e-05\n",
      "Epoch 4164: train loss: 2.1329127775970846e-05\n",
      "Epoch 4165: train loss: 2.130374741682317e-05\n",
      "Epoch 4166: train loss: 2.1277955966070294e-05\n",
      "Epoch 4167: train loss: 2.125231912941672e-05\n",
      "Epoch 4168: train loss: 2.1226924218353815e-05\n",
      "Epoch 4169: train loss: 2.12010618270142e-05\n",
      "Epoch 4170: train loss: 2.117561234626919e-05\n",
      "Epoch 4171: train loss: 2.1150253814994358e-05\n",
      "Epoch 4172: train loss: 2.1125342755112797e-05\n",
      "Epoch 4173: train loss: 2.1100184312672354e-05\n",
      "Epoch 4174: train loss: 2.107488217006903e-05\n",
      "Epoch 4175: train loss: 2.1050005670986138e-05\n",
      "Epoch 4176: train loss: 2.1025294699938968e-05\n",
      "Epoch 4177: train loss: 2.1001496861572377e-05\n",
      "Epoch 4178: train loss: 2.097824472002685e-05\n",
      "Epoch 4179: train loss: 2.095584750350099e-05\n",
      "Epoch 4180: train loss: 2.0934576241415925e-05\n",
      "Epoch 4181: train loss: 2.09153258765582e-05\n",
      "Epoch 4182: train loss: 2.089821282424964e-05\n",
      "Epoch 4183: train loss: 2.0885419871774502e-05\n",
      "Epoch 4184: train loss: 2.0878844225080684e-05\n",
      "Epoch 4185: train loss: 2.0880372176179662e-05\n",
      "Epoch 4186: train loss: 2.089659756165929e-05\n",
      "Epoch 4187: train loss: 2.093077273457311e-05\n",
      "Epoch 4188: train loss: 2.1001469576731324e-05\n",
      "Epoch 4189: train loss: 2.111219146172516e-05\n",
      "Epoch 4190: train loss: 2.131538713001646e-05\n",
      "Epoch 4191: train loss: 2.1603114873869345e-05\n",
      "Epoch 4192: train loss: 2.213256448158063e-05\n",
      "Epoch 4193: train loss: 2.2830936359241605e-05\n",
      "Epoch 4194: train loss: 2.4179982574423775e-05\n",
      "Epoch 4195: train loss: 2.578558815002907e-05\n",
      "Epoch 4196: train loss: 2.9159540645196103e-05\n",
      "Epoch 4197: train loss: 3.2512994948774576e-05\n",
      "Epoch 4198: train loss: 4.047388938488439e-05\n",
      "Epoch 4199: train loss: 4.598502709995955e-05\n",
      "Epoch 4200: train loss: 6.161646160762757e-05\n",
      "Epoch 4201: train loss: 6.544108327943832e-05\n",
      "Epoch 4202: train loss: 8.325431554112583e-05\n",
      "Epoch 4203: train loss: 7.307635678444058e-05\n",
      "Epoch 4204: train loss: 6.895625119796023e-05\n",
      "Epoch 4205: train loss: 4.398925739224069e-05\n",
      "Epoch 4206: train loss: 2.675180439837277e-05\n",
      "Epoch 4207: train loss: 2.2552660084329545e-05\n",
      "Epoch 4208: train loss: 3.1255553039954975e-05\n",
      "Epoch 4209: train loss: 4.303809328121133e-05\n",
      "Epoch 4210: train loss: 4.396991789690219e-05\n",
      "Epoch 4211: train loss: 3.963850031141192e-05\n",
      "Epoch 4212: train loss: 2.841497007466387e-05\n",
      "Epoch 4213: train loss: 2.1697229385608807e-05\n",
      "Epoch 4214: train loss: 2.0644823962356895e-05\n",
      "Epoch 4215: train loss: 2.4304388716700487e-05\n",
      "Epoch 4216: train loss: 3.14616336254403e-05\n",
      "Epoch 4217: train loss: 3.86936662835069e-05\n",
      "Epoch 4218: train loss: 5.350351784727536e-05\n",
      "Epoch 4219: train loss: 6.204460805747658e-05\n",
      "Epoch 4220: train loss: 9.416812099516392e-05\n",
      "Epoch 4221: train loss: 0.00010146042768610641\n",
      "Epoch 4222: train loss: 0.00014723418280482292\n",
      "Epoch 4223: train loss: 0.00012081325985491276\n",
      "Epoch 4224: train loss: 0.0001035301320371218\n",
      "Epoch 4225: train loss: 4.6401710278587416e-05\n",
      "Epoch 4226: train loss: 2.458995186316315e-05\n",
      "Epoch 4227: train loss: 4.6885295887477696e-05\n",
      "Epoch 4228: train loss: 7.175735663622618e-05\n",
      "Epoch 4229: train loss: 6.292652687989175e-05\n",
      "Epoch 4230: train loss: 3.28260866808705e-05\n",
      "Epoch 4231: train loss: 2.4810595277813263e-05\n",
      "Epoch 4232: train loss: 4.0639275539433584e-05\n",
      "Epoch 4233: train loss: 5.35282633791212e-05\n",
      "Epoch 4234: train loss: 5.48132011317648e-05\n",
      "Epoch 4235: train loss: 3.458221908658743e-05\n",
      "Epoch 4236: train loss: 2.1999881937517785e-05\n",
      "Epoch 4237: train loss: 2.1234669475234114e-05\n",
      "Epoch 4238: train loss: 3.092036422458477e-05\n",
      "Epoch 4239: train loss: 4.608324161381461e-05\n",
      "Epoch 4240: train loss: 5.649096056004055e-05\n",
      "Epoch 4241: train loss: 8.176617120625451e-05\n",
      "Epoch 4242: train loss: 7.458520121872425e-05\n",
      "Epoch 4243: train loss: 8.495816291542724e-05\n",
      "Epoch 4244: train loss: 6.361013220157474e-05\n",
      "Epoch 4245: train loss: 4.604938658303581e-05\n",
      "Epoch 4246: train loss: 2.549705641285982e-05\n",
      "Epoch 4247: train loss: 2.0542005586321466e-05\n",
      "Epoch 4248: train loss: 2.9596583772217855e-05\n",
      "Epoch 4249: train loss: 3.7622161471517757e-05\n",
      "Epoch 4250: train loss: 3.746229413081892e-05\n",
      "Epoch 4251: train loss: 2.742965989455115e-05\n",
      "Epoch 4252: train loss: 2.073902760457713e-05\n",
      "Epoch 4253: train loss: 2.2769138013245538e-05\n",
      "Epoch 4254: train loss: 2.868420779122971e-05\n",
      "Epoch 4255: train loss: 3.177080725436099e-05\n",
      "Epoch 4256: train loss: 2.8031205147271976e-05\n",
      "Epoch 4257: train loss: 2.3207829144666903e-05\n",
      "Epoch 4258: train loss: 1.9622681065811776e-05\n",
      "Epoch 4259: train loss: 1.9642713596113026e-05\n",
      "Epoch 4260: train loss: 2.2362248273566365e-05\n",
      "Epoch 4261: train loss: 2.5619094230933115e-05\n",
      "Epoch 4262: train loss: 2.9103906854288653e-05\n",
      "Epoch 4263: train loss: 2.9946577342343517e-05\n",
      "Epoch 4264: train loss: 3.138844112982042e-05\n",
      "Epoch 4265: train loss: 2.914822289312724e-05\n",
      "Epoch 4266: train loss: 2.774210588540882e-05\n",
      "Epoch 4267: train loss: 2.4465241949656047e-05\n",
      "Epoch 4268: train loss: 2.181320269301068e-05\n",
      "Epoch 4269: train loss: 1.9687244275701232e-05\n",
      "Epoch 4270: train loss: 1.8885913959820755e-05\n",
      "Epoch 4271: train loss: 1.9307972252136096e-05\n",
      "Epoch 4272: train loss: 2.0368044715723954e-05\n",
      "Epoch 4273: train loss: 2.1354346245061606e-05\n",
      "Epoch 4274: train loss: 2.1404946892289445e-05\n",
      "Epoch 4275: train loss: 2.0835250325035304e-05\n",
      "Epoch 4276: train loss: 1.9790786609519273e-05\n",
      "Epoch 4277: train loss: 1.899657945614308e-05\n",
      "Epoch 4278: train loss: 1.871901986305602e-05\n",
      "Epoch 4279: train loss: 1.8944183466373943e-05\n",
      "Epoch 4280: train loss: 1.943174902407918e-05\n",
      "Epoch 4281: train loss: 1.9816903659375384e-05\n",
      "Epoch 4282: train loss: 2.002771543629933e-05\n",
      "Epoch 4283: train loss: 1.988720214285422e-05\n",
      "Epoch 4284: train loss: 1.961589077836834e-05\n",
      "Epoch 4285: train loss: 1.9216269720345736e-05\n",
      "Epoch 4286: train loss: 1.887631879071705e-05\n",
      "Epoch 4287: train loss: 1.8604696379043162e-05\n",
      "Epoch 4288: train loss: 1.845230144681409e-05\n",
      "Epoch 4289: train loss: 1.840460390667431e-05\n",
      "Epoch 4290: train loss: 1.8438935512676835e-05\n",
      "Epoch 4291: train loss: 1.8524862753110938e-05\n",
      "Epoch 4292: train loss: 1.862084354797844e-05\n",
      "Epoch 4293: train loss: 1.870928463176824e-05\n",
      "Epoch 4294: train loss: 1.8746299247140996e-05\n",
      "Epoch 4295: train loss: 1.8749587979982607e-05\n",
      "Epoch 4296: train loss: 1.8686043404159136e-05\n",
      "Epoch 4297: train loss: 1.859495932876598e-05\n",
      "Epoch 4298: train loss: 1.8469692804501392e-05\n",
      "Epoch 4299: train loss: 1.8349086531088687e-05\n",
      "Epoch 4300: train loss: 1.8237518816022202e-05\n",
      "Epoch 4301: train loss: 1.8154914869228378e-05\n",
      "Epoch 4302: train loss: 1.810239155020099e-05\n",
      "Epoch 4303: train loss: 1.807820808608085e-05\n",
      "Epoch 4304: train loss: 1.8076019841828384e-05\n",
      "Epoch 4305: train loss: 1.8084749171976e-05\n",
      "Epoch 4306: train loss: 1.809782042982988e-05\n",
      "Epoch 4307: train loss: 1.8106409697793424e-05\n",
      "Epoch 4308: train loss: 1.811073707358446e-05\n",
      "Epoch 4309: train loss: 1.8104423361364752e-05\n",
      "Epoch 4310: train loss: 1.8093780454364605e-05\n",
      "Epoch 4311: train loss: 1.8073635146720335e-05\n",
      "Epoch 4312: train loss: 1.8052880477625877e-05\n",
      "Epoch 4313: train loss: 1.8025635654339567e-05\n",
      "Epoch 4314: train loss: 1.7999902411247604e-05\n",
      "Epoch 4315: train loss: 1.7970554836210795e-05\n",
      "Epoch 4316: train loss: 1.794429772417061e-05\n",
      "Epoch 4317: train loss: 1.791555951058399e-05\n",
      "Epoch 4318: train loss: 1.789015732356347e-05\n",
      "Epoch 4319: train loss: 1.7864371329778805e-05\n",
      "Epoch 4320: train loss: 1.7841723092715256e-05\n",
      "Epoch 4321: train loss: 1.7817947082221508e-05\n",
      "Epoch 4322: train loss: 1.779740341589786e-05\n",
      "Epoch 4323: train loss: 1.7776614186004736e-05\n",
      "Epoch 4324: train loss: 1.7759439288056456e-05\n",
      "Epoch 4325: train loss: 1.7741796909831464e-05\n",
      "Epoch 4326: train loss: 1.77282727236161e-05\n",
      "Epoch 4327: train loss: 1.771526331140194e-05\n",
      "Epoch 4328: train loss: 1.7708509403746575e-05\n",
      "Epoch 4329: train loss: 1.770205381035339e-05\n",
      "Epoch 4330: train loss: 1.7705604477669112e-05\n",
      "Epoch 4331: train loss: 1.7711885448079556e-05\n",
      "Epoch 4332: train loss: 1.7733034837874584e-05\n",
      "Epoch 4333: train loss: 1.77607453224482e-05\n",
      "Epoch 4334: train loss: 1.7815611499827355e-05\n",
      "Epoch 4335: train loss: 1.7883028704090975e-05\n",
      "Epoch 4336: train loss: 1.8001297576120123e-05\n",
      "Epoch 4337: train loss: 1.814292772905901e-05\n",
      "Epoch 4338: train loss: 1.8385380826657638e-05\n",
      "Epoch 4339: train loss: 1.8669097698875703e-05\n",
      "Epoch 4340: train loss: 1.91594117495697e-05\n",
      "Epoch 4341: train loss: 1.970965058717411e-05\n",
      "Epoch 4342: train loss: 2.0696672436315566e-05\n",
      "Epoch 4343: train loss: 2.1726824343204498e-05\n",
      "Epoch 4344: train loss: 2.3678016077610664e-05\n",
      "Epoch 4345: train loss: 2.5477365852566436e-05\n",
      "Epoch 4346: train loss: 2.9162696591811255e-05\n",
      "Epoch 4347: train loss: 3.188212212990038e-05\n",
      "Epoch 4348: train loss: 3.809199552051723e-05\n",
      "Epoch 4349: train loss: 4.100267688045278e-05\n",
      "Epoch 4350: train loss: 4.9051333917304873e-05\n",
      "Epoch 4351: train loss: 4.931592047796585e-05\n",
      "Epoch 4352: train loss: 5.437756044557318e-05\n",
      "Epoch 4353: train loss: 4.815139254787937e-05\n",
      "Epoch 4354: train loss: 4.408095992403105e-05\n",
      "Epoch 4355: train loss: 3.3284653909504414e-05\n",
      "Epoch 4356: train loss: 2.4951688828878105e-05\n",
      "Epoch 4357: train loss: 1.9047449313802645e-05\n",
      "Epoch 4358: train loss: 1.7655262126936577e-05\n",
      "Epoch 4359: train loss: 1.9984097889391705e-05\n",
      "Epoch 4360: train loss: 2.4076374756987207e-05\n",
      "Epoch 4361: train loss: 2.9089380404911935e-05\n",
      "Epoch 4362: train loss: 3.203030792064965e-05\n",
      "Epoch 4363: train loss: 3.670271689770743e-05\n",
      "Epoch 4364: train loss: 3.784114960581064e-05\n",
      "Epoch 4365: train loss: 4.360997263574973e-05\n",
      "Epoch 4366: train loss: 4.478653136175126e-05\n",
      "Epoch 4367: train loss: 5.369207065086812e-05\n",
      "Epoch 4368: train loss: 5.459694511955604e-05\n",
      "Epoch 4369: train loss: 6.481478339992464e-05\n",
      "Epoch 4370: train loss: 6.087770816520788e-05\n",
      "Epoch 4371: train loss: 6.26313776592724e-05\n",
      "Epoch 4372: train loss: 4.921073923469521e-05\n",
      "Epoch 4373: train loss: 3.7524641811614856e-05\n",
      "Epoch 4374: train loss: 2.4423108698101714e-05\n",
      "Epoch 4375: train loss: 1.8034981621894985e-05\n",
      "Epoch 4376: train loss: 1.9183904441888444e-05\n",
      "Epoch 4377: train loss: 2.5076684323721565e-05\n",
      "Epoch 4378: train loss: 3.1849867809796706e-05\n",
      "Epoch 4379: train loss: 3.412152727833018e-05\n",
      "Epoch 4380: train loss: 3.465413465164602e-05\n",
      "Epoch 4381: train loss: 3.014023786818143e-05\n",
      "Epoch 4382: train loss: 2.6486904971534386e-05\n",
      "Epoch 4383: train loss: 2.2167978386278264e-05\n",
      "Epoch 4384: train loss: 1.9470142433419824e-05\n",
      "Epoch 4385: train loss: 1.763164618751034e-05\n",
      "Epoch 4386: train loss: 1.6743630112614483e-05\n",
      "Epoch 4387: train loss: 1.6462683561258018e-05\n",
      "Epoch 4388: train loss: 1.6608950318186544e-05\n",
      "Epoch 4389: train loss: 1.7098747775889933e-05\n",
      "Epoch 4390: train loss: 1.7916168872034177e-05\n",
      "Epoch 4391: train loss: 1.929245627252385e-05\n",
      "Epoch 4392: train loss: 2.1127456420799717e-05\n",
      "Epoch 4393: train loss: 2.4437371394014917e-05\n",
      "Epoch 4394: train loss: 2.8019278033752926e-05\n",
      "Epoch 4395: train loss: 3.49780275428202e-05\n",
      "Epoch 4396: train loss: 4.017819446744397e-05\n",
      "Epoch 4397: train loss: 5.080953269498423e-05\n",
      "Epoch 4398: train loss: 5.347382102627307e-05\n",
      "Epoch 4399: train loss: 6.0584774473682046e-05\n",
      "Epoch 4400: train loss: 5.3608237067237496e-05\n",
      "Epoch 4401: train loss: 4.715377144748345e-05\n",
      "Epoch 4402: train loss: 3.326363366795704e-05\n",
      "Epoch 4403: train loss: 2.265938564960379e-05\n",
      "Epoch 4404: train loss: 1.7320546248811297e-05\n",
      "Epoch 4405: train loss: 1.8598464521346614e-05\n",
      "Epoch 4406: train loss: 2.411391142231878e-05\n",
      "Epoch 4407: train loss: 2.9637478291988373e-05\n",
      "Epoch 4408: train loss: 3.4360215067863464e-05\n",
      "Epoch 4409: train loss: 3.411576835787855e-05\n",
      "Epoch 4410: train loss: 3.4119253541575745e-05\n",
      "Epoch 4411: train loss: 3.062866744585335e-05\n",
      "Epoch 4412: train loss: 2.9079023079248145e-05\n",
      "Epoch 4413: train loss: 2.6199071726296097e-05\n",
      "Epoch 4414: train loss: 2.5243345589842647e-05\n",
      "Epoch 4415: train loss: 2.3771386622684076e-05\n",
      "Epoch 4416: train loss: 2.3760334443068132e-05\n",
      "Epoch 4417: train loss: 2.340628816455137e-05\n",
      "Epoch 4418: train loss: 2.4387096345890313e-05\n",
      "Epoch 4419: train loss: 2.4856723030097783e-05\n",
      "Epoch 4420: train loss: 2.6854480893234722e-05\n",
      "Epoch 4421: train loss: 2.7818954549729824e-05\n",
      "Epoch 4422: train loss: 3.05302492051851e-05\n",
      "Epoch 4423: train loss: 3.120418841717765e-05\n",
      "Epoch 4424: train loss: 3.359668698976748e-05\n",
      "Epoch 4425: train loss: 3.274135815445334e-05\n",
      "Epoch 4426: train loss: 3.307878796476871e-05\n",
      "Epoch 4427: train loss: 3.0074819733272307e-05\n",
      "Epoch 4428: train loss: 2.774409222183749e-05\n",
      "Epoch 4429: train loss: 2.3776865418767557e-05\n",
      "Epoch 4430: train loss: 2.067864807031583e-05\n",
      "Epoch 4431: train loss: 1.7950218534679152e-05\n",
      "Epoch 4432: train loss: 1.6333080566255376e-05\n",
      "Epoch 4433: train loss: 1.5699768482591026e-05\n",
      "Epoch 4434: train loss: 1.5897532648523338e-05\n",
      "Epoch 4435: train loss: 1.6709313058527187e-05\n",
      "Epoch 4436: train loss: 1.793095907487441e-05\n",
      "Epoch 4437: train loss: 1.973514554265421e-05\n",
      "Epoch 4438: train loss: 2.1824274881510064e-05\n",
      "Epoch 4439: train loss: 2.5544777599861845e-05\n",
      "Epoch 4440: train loss: 2.9634933525812812e-05\n",
      "Epoch 4441: train loss: 3.8494777982123196e-05\n",
      "Epoch 4442: train loss: 4.688429180532694e-05\n",
      "Epoch 4443: train loss: 6.715707422699779e-05\n",
      "Epoch 4444: train loss: 7.950607687234879e-05\n",
      "Epoch 4445: train loss: 0.00011130695202155039\n",
      "Epoch 4446: train loss: 0.00011286691733403131\n",
      "Epoch 4447: train loss: 0.00012234083260409534\n",
      "Epoch 4448: train loss: 9.01110252016224e-05\n",
      "Epoch 4449: train loss: 5.496273297467269e-05\n",
      "Epoch 4450: train loss: 2.4831275368342176e-05\n",
      "Epoch 4451: train loss: 1.9723771401913837e-05\n",
      "Epoch 4452: train loss: 3.6113928217673674e-05\n",
      "Epoch 4453: train loss: 5.460939428303391e-05\n",
      "Epoch 4454: train loss: 6.231834413483739e-05\n",
      "Epoch 4455: train loss: 4.935801916872151e-05\n",
      "Epoch 4456: train loss: 3.257092976127751e-05\n",
      "Epoch 4457: train loss: 1.8983480913448147e-05\n",
      "Epoch 4458: train loss: 1.5716641428298317e-05\n",
      "Epoch 4459: train loss: 2.108386433974374e-05\n",
      "Epoch 4460: train loss: 3.156004459015094e-05\n",
      "Epoch 4461: train loss: 4.864958827965893e-05\n",
      "Epoch 4462: train loss: 6.255952757783234e-05\n",
      "Epoch 4463: train loss: 9.691533341538161e-05\n",
      "Epoch 4464: train loss: 0.00011215241102036089\n",
      "Epoch 4465: train loss: 0.0001665082818362862\n",
      "Epoch 4466: train loss: 0.0001621098053874448\n",
      "Epoch 4467: train loss: 0.00017299046157859266\n",
      "Epoch 4468: train loss: 0.00010787924111355096\n",
      "Epoch 4469: train loss: 4.60823139292188e-05\n",
      "Epoch 4470: train loss: 1.9512101061991416e-05\n",
      "Epoch 4471: train loss: 4.435743176145479e-05\n",
      "Epoch 4472: train loss: 8.010047167772427e-05\n",
      "Epoch 4473: train loss: 8.352887380169705e-05\n",
      "Epoch 4474: train loss: 5.7270615798188373e-05\n",
      "Epoch 4475: train loss: 2.430756103422027e-05\n",
      "Epoch 4476: train loss: 2.209594640589785e-05\n",
      "Epoch 4477: train loss: 4.383883060654625e-05\n",
      "Epoch 4478: train loss: 6.403038423741236e-05\n",
      "Epoch 4479: train loss: 6.650061550317332e-05\n",
      "Epoch 4480: train loss: 5.135196988703683e-05\n",
      "Epoch 4481: train loss: 4.1465718823019415e-05\n",
      "Epoch 4482: train loss: 2.2670861653750762e-05\n",
      "Epoch 4483: train loss: 1.6447866073576733e-05\n",
      "Epoch 4484: train loss: 1.82777366717346e-05\n",
      "Epoch 4485: train loss: 2.4921298972913064e-05\n",
      "Epoch 4486: train loss: 3.730129901668988e-05\n",
      "Epoch 4487: train loss: 4.813317718799226e-05\n",
      "Epoch 4488: train loss: 6.349486648105085e-05\n",
      "Epoch 4489: train loss: 5.9430694818729535e-05\n",
      "Epoch 4490: train loss: 6.0775400925194845e-05\n",
      "Epoch 4491: train loss: 4.5739183406112716e-05\n",
      "Epoch 4492: train loss: 3.088465382461436e-05\n",
      "Epoch 4493: train loss: 1.8364506104262546e-05\n",
      "Epoch 4494: train loss: 1.599053939571604e-05\n",
      "Epoch 4495: train loss: 2.166098238376435e-05\n",
      "Epoch 4496: train loss: 2.8872878829133697e-05\n",
      "Epoch 4497: train loss: 3.262174141127616e-05\n",
      "Epoch 4498: train loss: 2.7452299036667682e-05\n",
      "Epoch 4499: train loss: 2.0715715436381288e-05\n",
      "Epoch 4500: train loss: 1.5717605492682196e-05\n",
      "Epoch 4501: train loss: 1.5483326933463104e-05\n",
      "Epoch 4502: train loss: 1.8603750504553318e-05\n",
      "Epoch 4503: train loss: 2.2177084247232415e-05\n",
      "Epoch 4504: train loss: 2.5608895157347433e-05\n",
      "Epoch 4505: train loss: 2.546810901549179e-05\n",
      "Epoch 4506: train loss: 2.5093824660871178e-05\n",
      "Epoch 4507: train loss: 2.2851769244880415e-05\n",
      "Epoch 4508: train loss: 2.0675957784987986e-05\n",
      "Epoch 4509: train loss: 1.829787106544245e-05\n",
      "Epoch 4510: train loss: 1.6704589143046178e-05\n",
      "Epoch 4511: train loss: 1.530334156996105e-05\n",
      "Epoch 4512: train loss: 1.4622202797909267e-05\n",
      "Epoch 4513: train loss: 1.4441039638768416e-05\n",
      "Epoch 4514: train loss: 1.4583712982130237e-05\n",
      "Epoch 4515: train loss: 1.5062445527291857e-05\n",
      "Epoch 4516: train loss: 1.5560044630547054e-05\n",
      "Epoch 4517: train loss: 1.5956185961840674e-05\n",
      "Epoch 4518: train loss: 1.611119478184264e-05\n",
      "Epoch 4519: train loss: 1.6069019693532027e-05\n",
      "Epoch 4520: train loss: 1.5725145203759894e-05\n",
      "Epoch 4521: train loss: 1.5319514204747975e-05\n",
      "Epoch 4522: train loss: 1.483678261138266e-05\n",
      "Epoch 4523: train loss: 1.447433987777913e-05\n",
      "Epoch 4524: train loss: 1.4247931176214479e-05\n",
      "Epoch 4525: train loss: 1.4152414223644882e-05\n",
      "Epoch 4526: train loss: 1.4205093066266272e-05\n",
      "Epoch 4527: train loss: 1.4324384210340213e-05\n",
      "Epoch 4528: train loss: 1.4475785064860247e-05\n",
      "Epoch 4529: train loss: 1.4632619240728673e-05\n",
      "Epoch 4530: train loss: 1.476036231906619e-05\n",
      "Epoch 4531: train loss: 1.4848978025838733e-05\n",
      "Epoch 4532: train loss: 1.4946237570256926e-05\n",
      "Epoch 4533: train loss: 1.4967340575822163e-05\n",
      "Epoch 4534: train loss: 1.5013789379736409e-05\n",
      "Epoch 4535: train loss: 1.5021078979771119e-05\n",
      "Epoch 4536: train loss: 1.5040230209706351e-05\n",
      "Epoch 4537: train loss: 1.502931809227448e-05\n",
      "Epoch 4538: train loss: 1.5060727491800208e-05\n",
      "Epoch 4539: train loss: 1.5039860954857431e-05\n",
      "Epoch 4540: train loss: 1.506455009803176e-05\n",
      "Epoch 4541: train loss: 1.5049676221678965e-05\n",
      "Epoch 4542: train loss: 1.5065070329001173e-05\n",
      "Epoch 4543: train loss: 1.5040297512314282e-05\n",
      "Epoch 4544: train loss: 1.5060748410178348e-05\n",
      "Epoch 4545: train loss: 1.5035516298667062e-05\n",
      "Epoch 4546: train loss: 1.5057769815030042e-05\n",
      "Epoch 4547: train loss: 1.5047830856929068e-05\n",
      "Epoch 4548: train loss: 1.5093706679181196e-05\n",
      "Epoch 4549: train loss: 1.511038590251701e-05\n",
      "Epoch 4550: train loss: 1.5209420780593064e-05\n",
      "Epoch 4551: train loss: 1.5290042938431725e-05\n",
      "Epoch 4552: train loss: 1.5479896319448017e-05\n",
      "Epoch 4553: train loss: 1.5672041627112776e-05\n",
      "Epoch 4554: train loss: 1.6050506019382738e-05\n",
      "Epoch 4555: train loss: 1.6437290469184518e-05\n",
      "Epoch 4556: train loss: 1.7157737602246925e-05\n",
      "Epoch 4557: train loss: 1.7913762349053286e-05\n",
      "Epoch 4558: train loss: 1.927034645632375e-05\n",
      "Epoch 4559: train loss: 2.0662615497712977e-05\n",
      "Epoch 4560: train loss: 2.3244181647896767e-05\n",
      "Epoch 4561: train loss: 2.5640074454713613e-05\n",
      "Epoch 4562: train loss: 3.0275034077931195e-05\n",
      "Epoch 4563: train loss: 3.398771877982654e-05\n",
      "Epoch 4564: train loss: 4.1425588278798386e-05\n",
      "Epoch 4565: train loss: 4.5760698412777856e-05\n",
      "Epoch 4566: train loss: 5.523044092115015e-05\n",
      "Epoch 4567: train loss: 5.7265620853286237e-05\n",
      "Epoch 4568: train loss: 6.364892760757357e-05\n",
      "Epoch 4569: train loss: 5.888871237402782e-05\n",
      "Epoch 4570: train loss: 5.551915455725975e-05\n",
      "Epoch 4571: train loss: 4.35038709838409e-05\n",
      "Epoch 4572: train loss: 3.287467916379683e-05\n",
      "Epoch 4573: train loss: 2.2221238396014087e-05\n",
      "Epoch 4574: train loss: 1.584823075972963e-05\n",
      "Epoch 4575: train loss: 1.3976650734548457e-05\n",
      "Epoch 4576: train loss: 1.5960060409270227e-05\n",
      "Epoch 4577: train loss: 2.04385014512809e-05\n",
      "Epoch 4578: train loss: 2.5686835215310566e-05\n",
      "Epoch 4579: train loss: 3.2605712476652116e-05\n",
      "Epoch 4580: train loss: 3.8162128475960344e-05\n",
      "Epoch 4581: train loss: 4.893693403573707e-05\n",
      "Epoch 4582: train loss: 5.653955304296687e-05\n",
      "Epoch 4583: train loss: 7.667641330044717e-05\n",
      "Epoch 4584: train loss: 8.718086610315368e-05\n",
      "Epoch 4585: train loss: 0.00011711678234860301\n",
      "Epoch 4586: train loss: 0.00011967576574534178\n",
      "Epoch 4587: train loss: 0.00013152566680219024\n",
      "Epoch 4588: train loss: 0.0001045768876792863\n",
      "Epoch 4589: train loss: 7.248097244882956e-05\n",
      "Epoch 4590: train loss: 3.541423211572692e-05\n",
      "Epoch 4591: train loss: 1.6769618014222942e-05\n",
      "Epoch 4592: train loss: 2.1645477318088524e-05\n",
      "Epoch 4593: train loss: 4.014690421172418e-05\n",
      "Epoch 4594: train loss: 5.729721669922583e-05\n",
      "Epoch 4595: train loss: 5.8504876506049186e-05\n",
      "Epoch 4596: train loss: 4.9541748012416065e-05\n",
      "Epoch 4597: train loss: 3.215433025616221e-05\n",
      "Epoch 4598: train loss: 1.931665428855922e-05\n",
      "Epoch 4599: train loss: 1.354640244244365e-05\n",
      "Epoch 4600: train loss: 1.4868252947053406e-05\n",
      "Epoch 4601: train loss: 2.142386802006513e-05\n",
      "Epoch 4602: train loss: 3.119097164017148e-05\n",
      "Epoch 4603: train loss: 4.764826371683739e-05\n",
      "Epoch 4604: train loss: 6.189145642565563e-05\n",
      "Epoch 4605: train loss: 9.42111510084942e-05\n",
      "Epoch 4606: train loss: 0.00011172344966325909\n",
      "Epoch 4607: train loss: 0.0001572041801409796\n",
      "Epoch 4608: train loss: 0.0001510203437646851\n",
      "Epoch 4609: train loss: 0.00014593823289033026\n",
      "Epoch 4610: train loss: 9.209117706632242e-05\n",
      "Epoch 4611: train loss: 3.9959159039426595e-05\n",
      "Epoch 4612: train loss: 1.6677116946084425e-05\n",
      "Epoch 4613: train loss: 3.239220677642152e-05\n",
      "Epoch 4614: train loss: 6.275311898207292e-05\n",
      "Epoch 4615: train loss: 7.55429791752249e-05\n",
      "Epoch 4616: train loss: 6.355153891490772e-05\n",
      "Epoch 4617: train loss: 3.426839248277247e-05\n",
      "Epoch 4618: train loss: 1.5951391105772927e-05\n",
      "Epoch 4619: train loss: 1.8499795260140672e-05\n",
      "Epoch 4620: train loss: 3.5140139516443014e-05\n",
      "Epoch 4621: train loss: 5.213706390350126e-05\n",
      "Epoch 4622: train loss: 6.0330468841129914e-05\n",
      "Epoch 4623: train loss: 7.163945701904595e-05\n",
      "Epoch 4624: train loss: 5.8489818911766633e-05\n",
      "Epoch 4625: train loss: 5.0939765060320497e-05\n",
      "Epoch 4626: train loss: 3.789550828514621e-05\n",
      "Epoch 4627: train loss: 2.862486508092843e-05\n",
      "Epoch 4628: train loss: 1.983430774998851e-05\n",
      "Epoch 4629: train loss: 1.4630521945946384e-05\n",
      "Epoch 4630: train loss: 1.3115647561789956e-05\n",
      "Epoch 4631: train loss: 1.4319442016130779e-05\n",
      "Epoch 4632: train loss: 1.6639243767713197e-05\n",
      "Epoch 4633: train loss: 1.950026853592135e-05\n",
      "Epoch 4634: train loss: 2.1174102585064247e-05\n",
      "Epoch 4635: train loss: 2.0626690456992947e-05\n",
      "Epoch 4636: train loss: 1.938666173373349e-05\n",
      "Epoch 4637: train loss: 1.6501855498063378e-05\n",
      "Epoch 4638: train loss: 1.4322066817840096e-05\n",
      "Epoch 4639: train loss: 1.3037150893069338e-05\n",
      "Epoch 4640: train loss: 1.285464531974867e-05\n",
      "Epoch 4641: train loss: 1.3786306226393208e-05\n",
      "Epoch 4642: train loss: 1.4845652913209051e-05\n",
      "Epoch 4643: train loss: 1.5931425878079608e-05\n",
      "Epoch 4644: train loss: 1.6696452803444117e-05\n",
      "Epoch 4645: train loss: 1.7147805920103565e-05\n",
      "Epoch 4646: train loss: 1.687244548520539e-05\n",
      "Epoch 4647: train loss: 1.6548878193134442e-05\n",
      "Epoch 4648: train loss: 1.5840154446777888e-05\n",
      "Epoch 4649: train loss: 1.5273208191501908e-05\n",
      "Epoch 4650: train loss: 1.4522385754389688e-05\n",
      "Epoch 4651: train loss: 1.3923651749792043e-05\n",
      "Epoch 4652: train loss: 1.3409583516477142e-05\n",
      "Epoch 4653: train loss: 1.2963097105966881e-05\n",
      "Epoch 4654: train loss: 1.2655977116082795e-05\n",
      "Epoch 4655: train loss: 1.2512774446804542e-05\n",
      "Epoch 4656: train loss: 1.2398570106597617e-05\n",
      "Epoch 4657: train loss: 1.2406890164129436e-05\n",
      "Epoch 4658: train loss: 1.2496117051341571e-05\n",
      "Epoch 4659: train loss: 1.255388451681938e-05\n",
      "Epoch 4660: train loss: 1.2663402230828069e-05\n",
      "Epoch 4661: train loss: 1.2773585694958456e-05\n",
      "Epoch 4662: train loss: 1.2843589502153918e-05\n",
      "Epoch 4663: train loss: 1.291417265747441e-05\n",
      "Epoch 4664: train loss: 1.2986558431293815e-05\n",
      "Epoch 4665: train loss: 1.301251813856652e-05\n",
      "Epoch 4666: train loss: 1.3058254808129277e-05\n",
      "Epoch 4667: train loss: 1.3081422366667539e-05\n",
      "Epoch 4668: train loss: 1.3135954759491142e-05\n",
      "Epoch 4669: train loss: 1.3185122952563688e-05\n",
      "Epoch 4670: train loss: 1.3281898645800538e-05\n",
      "Epoch 4671: train loss: 1.3383166333369445e-05\n",
      "Epoch 4672: train loss: 1.3554380529967602e-05\n",
      "Epoch 4673: train loss: 1.3731173567066435e-05\n",
      "Epoch 4674: train loss: 1.404041722707916e-05\n",
      "Epoch 4675: train loss: 1.4362008187163156e-05\n",
      "Epoch 4676: train loss: 1.4903863302606624e-05\n",
      "Epoch 4677: train loss: 1.5485255062230863e-05\n",
      "Epoch 4678: train loss: 1.6410553143941797e-05\n",
      "Epoch 4679: train loss: 1.7374441085848957e-05\n",
      "Epoch 4680: train loss: 1.898784830700606e-05\n",
      "Epoch 4681: train loss: 2.051957926596515e-05\n",
      "Epoch 4682: train loss: 2.3239108486450277e-05\n",
      "Epoch 4683: train loss: 2.5693185307318345e-05\n",
      "Epoch 4684: train loss: 3.005690814461559e-05\n",
      "Epoch 4685: train loss: 3.344779179315083e-05\n",
      "Epoch 4686: train loss: 3.992306301370263e-05\n",
      "Epoch 4687: train loss: 4.360052480478771e-05\n",
      "Epoch 4688: train loss: 5.126106407260522e-05\n",
      "Epoch 4689: train loss: 5.355933899409138e-05\n",
      "Epoch 4690: train loss: 5.9384485211921856e-05\n",
      "Epoch 4691: train loss: 5.6931581639219075e-05\n",
      "Epoch 4692: train loss: 5.638892616843805e-05\n",
      "Epoch 4693: train loss: 4.7888595872791484e-05\n",
      "Epoch 4694: train loss: 4.026779788546264e-05\n",
      "Epoch 4695: train loss: 2.9833976441295817e-05\n",
      "Epoch 4696: train loss: 2.168507853639312e-05\n",
      "Epoch 4697: train loss: 1.554798291181214e-05\n",
      "Epoch 4698: train loss: 1.2567623343784362e-05\n",
      "Epoch 4699: train loss: 1.2322942893661093e-05\n",
      "Epoch 4700: train loss: 1.415588121744804e-05\n",
      "Epoch 4701: train loss: 1.7524997019791044e-05\n",
      "Epoch 4702: train loss: 2.1806650693179108e-05\n",
      "Epoch 4703: train loss: 2.830658013408538e-05\n",
      "Epoch 4704: train loss: 3.5456228943075985e-05\n",
      "Epoch 4705: train loss: 4.961652302881703e-05\n",
      "Epoch 4706: train loss: 6.335811485769227e-05\n",
      "Epoch 4707: train loss: 9.504741319688037e-05\n",
      "Epoch 4708: train loss: 0.00011706404620781541\n",
      "Epoch 4709: train loss: 0.0001664831070229411\n",
      "Epoch 4710: train loss: 0.00017580435087438673\n",
      "Epoch 4711: train loss: 0.00018824970175046474\n",
      "Epoch 4712: train loss: 0.0001432318676961586\n",
      "Epoch 4713: train loss: 8.449242159258574e-05\n",
      "Epoch 4714: train loss: 3.1748946639709175e-05\n",
      "Epoch 4715: train loss: 1.6520792996743694e-05\n",
      "Epoch 4716: train loss: 3.7976456951582804e-05\n",
      "Epoch 4717: train loss: 7.035529415588826e-05\n",
      "Epoch 4718: train loss: 8.883154077921063e-05\n",
      "Epoch 4719: train loss: 7.607549923704937e-05\n",
      "Epoch 4720: train loss: 4.913024167763069e-05\n",
      "Epoch 4721: train loss: 2.270839104312472e-05\n",
      "Epoch 4722: train loss: 1.2335416613495909e-05\n",
      "Epoch 4723: train loss: 1.760350824042689e-05\n",
      "Epoch 4724: train loss: 3.2856893085408956e-05\n",
      "Epoch 4725: train loss: 5.7209319493267685e-05\n",
      "Epoch 4726: train loss: 7.607190491398796e-05\n",
      "Epoch 4727: train loss: 0.00011355420429026708\n",
      "Epoch 4728: train loss: 0.0001240410201717168\n",
      "Epoch 4729: train loss: 0.000155713947606273\n",
      "Epoch 4730: train loss: 0.00013968668645247817\n",
      "Epoch 4731: train loss: 0.00012679507199209183\n",
      "Epoch 4732: train loss: 7.521956285927445e-05\n",
      "Epoch 4733: train loss: 3.23937383654993e-05\n",
      "Epoch 4734: train loss: 1.4233524780138396e-05\n",
      "Epoch 4735: train loss: 2.8000193196930923e-05\n",
      "Epoch 4736: train loss: 5.340293137123808e-05\n",
      "Epoch 4737: train loss: 6.499524897662923e-05\n",
      "Epoch 4738: train loss: 5.696973676094785e-05\n",
      "Epoch 4739: train loss: 3.243343235226348e-05\n",
      "Epoch 4740: train loss: 1.5737570720375516e-05\n",
      "Epoch 4741: train loss: 1.4637689673691057e-05\n",
      "Epoch 4742: train loss: 2.6833942683879286e-05\n",
      "Epoch 4743: train loss: 4.021127460873686e-05\n",
      "Epoch 4744: train loss: 4.742172677651979e-05\n",
      "Epoch 4745: train loss: 5.4618052672594786e-05\n",
      "Epoch 4746: train loss: 4.4657379476120695e-05\n",
      "Epoch 4747: train loss: 3.782200292334892e-05\n",
      "Epoch 4748: train loss: 2.7814428904093802e-05\n",
      "Epoch 4749: train loss: 1.9997649360448122e-05\n",
      "Epoch 4750: train loss: 1.4877916328259744e-05\n",
      "Epoch 4751: train loss: 1.2140903891122434e-05\n",
      "Epoch 4752: train loss: 1.1705307770171203e-05\n",
      "Epoch 4753: train loss: 1.3550683434004895e-05\n",
      "Epoch 4754: train loss: 1.574729867570568e-05\n",
      "Epoch 4755: train loss: 1.7586231479072012e-05\n",
      "Epoch 4756: train loss: 1.8842954887077212e-05\n",
      "Epoch 4757: train loss: 1.8286540580447763e-05\n",
      "Epoch 4758: train loss: 1.721236912999302e-05\n",
      "Epoch 4759: train loss: 1.483899359300267e-05\n",
      "Epoch 4760: train loss: 1.29563350128592e-05\n",
      "Epoch 4761: train loss: 1.1884231753356289e-05\n",
      "Epoch 4762: train loss: 1.140359108831035e-05\n",
      "Epoch 4763: train loss: 1.1903954145964235e-05\n",
      "Epoch 4764: train loss: 1.2795729162462521e-05\n",
      "Epoch 4765: train loss: 1.3509802556654904e-05\n",
      "Epoch 4766: train loss: 1.4110125448496547e-05\n",
      "Epoch 4767: train loss: 1.44648856803542e-05\n",
      "Epoch 4768: train loss: 1.4167998415359762e-05\n",
      "Epoch 4769: train loss: 1.3782268979412038e-05\n",
      "Epoch 4770: train loss: 1.3137929272488691e-05\n",
      "Epoch 4771: train loss: 1.2614551451406442e-05\n",
      "Epoch 4772: train loss: 1.2104941561119631e-05\n",
      "Epoch 4773: train loss: 1.1661826647468843e-05\n",
      "Epoch 4774: train loss: 1.140980839409167e-05\n",
      "Epoch 4775: train loss: 1.1233822988288011e-05\n",
      "Epoch 4776: train loss: 1.1090270163549576e-05\n",
      "Epoch 4777: train loss: 1.1106597412435804e-05\n",
      "Epoch 4778: train loss: 1.1139889465994202e-05\n",
      "Epoch 4779: train loss: 1.1183947208337486e-05\n",
      "Epoch 4780: train loss: 1.1299855941615533e-05\n",
      "Epoch 4781: train loss: 1.137743856816087e-05\n",
      "Epoch 4782: train loss: 1.1462275324447546e-05\n",
      "Epoch 4783: train loss: 1.154326309915632e-05\n",
      "Epoch 4784: train loss: 1.158666236733552e-05\n",
      "Epoch 4785: train loss: 1.162698845291743e-05\n",
      "Epoch 4786: train loss: 1.1665968486340716e-05\n",
      "Epoch 4787: train loss: 1.1652480679913424e-05\n",
      "Epoch 4788: train loss: 1.1671091669995803e-05\n",
      "Epoch 4789: train loss: 1.1660587915685028e-05\n",
      "Epoch 4790: train loss: 1.1651246495603118e-05\n",
      "Epoch 4791: train loss: 1.165326375485165e-05\n",
      "Epoch 4792: train loss: 1.1678259397740476e-05\n",
      "Epoch 4793: train loss: 1.1695573448378127e-05\n",
      "Epoch 4794: train loss: 1.1764770533773117e-05\n",
      "Epoch 4795: train loss: 1.18403768283315e-05\n",
      "Epoch 4796: train loss: 1.1976530004176311e-05\n",
      "Epoch 4797: train loss: 1.21349867185927e-05\n",
      "Epoch 4798: train loss: 1.2406850146362558e-05\n",
      "Epoch 4799: train loss: 1.2728284673357848e-05\n",
      "Epoch 4800: train loss: 1.3241140550235286e-05\n",
      "Epoch 4801: train loss: 1.3853798918717075e-05\n",
      "Epoch 4802: train loss: 1.4844343240838498e-05\n",
      "Epoch 4803: train loss: 1.594637069501914e-05\n",
      "Epoch 4804: train loss: 1.77996298589278e-05\n",
      "Epoch 4805: train loss: 1.9826244169962592e-05\n",
      "Epoch 4806: train loss: 2.3250850063050166e-05\n",
      "Epoch 4807: train loss: 2.67437153524952e-05\n",
      "Epoch 4808: train loss: 3.2945834391284734e-05\n",
      "Epoch 4809: train loss: 3.823747465503402e-05\n",
      "Epoch 4810: train loss: 4.800518581760116e-05\n",
      "Epoch 4811: train loss: 5.459308886202052e-05\n",
      "Epoch 4812: train loss: 6.68223510729149e-05\n",
      "Epoch 4813: train loss: 7.104712130967528e-05\n",
      "Epoch 4814: train loss: 7.983912655618042e-05\n",
      "Epoch 4815: train loss: 7.547678251285106e-05\n",
      "Epoch 4816: train loss: 7.164557609939948e-05\n",
      "Epoch 4817: train loss: 5.68632131034974e-05\n",
      "Epoch 4818: train loss: 4.254605300957337e-05\n",
      "Epoch 4819: train loss: 2.706489249248989e-05\n",
      "Epoch 4820: train loss: 1.655372579989489e-05\n",
      "Epoch 4821: train loss: 1.16178416647017e-05\n",
      "Epoch 4822: train loss: 1.2162044185970444e-05\n",
      "Epoch 4823: train loss: 1.6657097148709e-05\n",
      "Epoch 4824: train loss: 2.317111284355633e-05\n",
      "Epoch 4825: train loss: 3.1618576031178236e-05\n",
      "Epoch 4826: train loss: 3.942692273994908e-05\n",
      "Epoch 4827: train loss: 5.248155866866e-05\n",
      "Epoch 4828: train loss: 6.270716403378174e-05\n",
      "Epoch 4829: train loss: 8.623755275039002e-05\n",
      "Epoch 4830: train loss: 0.00010049273259937763\n",
      "Epoch 4831: train loss: 0.00013489459524862468\n",
      "Epoch 4832: train loss: 0.00014154530072119087\n",
      "Epoch 4833: train loss: 0.00015587417874485254\n",
      "Epoch 4834: train loss: 0.00012879721180070192\n",
      "Epoch 4835: train loss: 9.226622205460444e-05\n",
      "Epoch 4836: train loss: 4.6365348680410534e-05\n",
      "Epoch 4837: train loss: 1.779370904841926e-05\n",
      "Epoch 4838: train loss: 1.575956412125379e-05\n",
      "Epoch 4839: train loss: 3.4015392884612083e-05\n",
      "Epoch 4840: train loss: 5.662953117280267e-05\n",
      "Epoch 4841: train loss: 6.622930231969804e-05\n",
      "Epoch 4842: train loss: 6.229356222320348e-05\n",
      "Epoch 4843: train loss: 4.4244021410122514e-05\n",
      "Epoch 4844: train loss: 2.684100400074385e-05\n",
      "Epoch 4845: train loss: 1.446086844225647e-05\n",
      "Epoch 4846: train loss: 1.055538268701639e-05\n",
      "Epoch 4847: train loss: 1.3531056538340636e-05\n",
      "Epoch 4848: train loss: 2.140614560630638e-05\n",
      "Epoch 4849: train loss: 3.433335950830951e-05\n",
      "Epoch 4850: train loss: 4.7585152060491964e-05\n",
      "Epoch 4851: train loss: 7.134430052246898e-05\n",
      "Epoch 4852: train loss: 8.752102439757437e-05\n",
      "Epoch 4853: train loss: 0.00012232361768838018\n",
      "Epoch 4854: train loss: 0.0001275192480534315\n",
      "Epoch 4855: train loss: 0.00013927044346928596\n",
      "Epoch 4856: train loss: 0.00010856927838176489\n",
      "Epoch 4857: train loss: 6.87459614709951e-05\n",
      "Epoch 4858: train loss: 2.8955422749277204e-05\n",
      "Epoch 4859: train loss: 1.3084941201668698e-05\n",
      "Epoch 4860: train loss: 2.397717980784364e-05\n",
      "Epoch 4861: train loss: 4.6498069423250854e-05\n",
      "Epoch 4862: train loss: 6.217672489583492e-05\n",
      "Epoch 4863: train loss: 5.6843906349968165e-05\n",
      "Epoch 4864: train loss: 3.889694198733196e-05\n",
      "Epoch 4865: train loss: 1.960344161489047e-05\n",
      "Epoch 4866: train loss: 1.1119978807982989e-05\n",
      "Epoch 4867: train loss: 1.4353180631587747e-05\n",
      "Epoch 4868: train loss: 2.4961924282251857e-05\n",
      "Epoch 4869: train loss: 3.963888593716547e-05\n",
      "Epoch 4870: train loss: 4.921508298139088e-05\n",
      "Epoch 4871: train loss: 6.18855920038186e-05\n",
      "Epoch 4872: train loss: 6.30832146271132e-05\n",
      "Epoch 4873: train loss: 6.82025493006222e-05\n",
      "Epoch 4874: train loss: 6.037009006831795e-05\n",
      "Epoch 4875: train loss: 5.6941902585094795e-05\n",
      "Epoch 4876: train loss: 4.2377676436444744e-05\n",
      "Epoch 4877: train loss: 2.9735747375525534e-05\n",
      "Epoch 4878: train loss: 1.7768743418855593e-05\n",
      "Epoch 4879: train loss: 1.1327008905936964e-05\n",
      "Epoch 4880: train loss: 1.17354193207575e-05\n",
      "Epoch 4881: train loss: 1.6803051039460115e-05\n",
      "Epoch 4882: train loss: 2.2853380869491957e-05\n",
      "Epoch 4883: train loss: 2.5487039238214493e-05\n",
      "Epoch 4884: train loss: 2.4820743419695646e-05\n",
      "Epoch 4885: train loss: 2.044170287263114e-05\n",
      "Epoch 4886: train loss: 1.542379686725326e-05\n",
      "Epoch 4887: train loss: 1.1639168405963574e-05\n",
      "Epoch 4888: train loss: 1.0218762326985598e-05\n",
      "Epoch 4889: train loss: 1.084370887838304e-05\n",
      "Epoch 4890: train loss: 1.288472685700981e-05\n",
      "Epoch 4891: train loss: 1.5743198673590086e-05\n",
      "Epoch 4892: train loss: 1.8298544091521762e-05\n",
      "Epoch 4893: train loss: 2.101348218275234e-05\n",
      "Epoch 4894: train loss: 2.258501081087161e-05\n",
      "Epoch 4895: train loss: 2.469945866323542e-05\n",
      "Epoch 4896: train loss: 2.4983042749227025e-05\n",
      "Epoch 4897: train loss: 2.6134295694646426e-05\n",
      "Epoch 4898: train loss: 2.504775329725817e-05\n",
      "Epoch 4899: train loss: 2.4095508706523106e-05\n",
      "Epoch 4900: train loss: 2.1296342310961336e-05\n",
      "Epoch 4901: train loss: 1.8530477973399684e-05\n",
      "Epoch 4902: train loss: 1.520973728474928e-05\n",
      "Epoch 4903: train loss: 1.2603624782059342e-05\n",
      "Epoch 4904: train loss: 1.0810017556650564e-05\n",
      "Epoch 4905: train loss: 1.0070222742797341e-05\n",
      "Epoch 4906: train loss: 1.0274486157868523e-05\n",
      "Epoch 4907: train loss: 1.1100847586931195e-05\n",
      "Epoch 4908: train loss: 1.2191317182441708e-05\n",
      "Epoch 4909: train loss: 1.31994929688517e-05\n",
      "Epoch 4910: train loss: 1.411787343386095e-05\n",
      "Epoch 4911: train loss: 1.4592920706490986e-05\n",
      "Epoch 4912: train loss: 1.5045033251226414e-05\n",
      "Epoch 4913: train loss: 1.512525250291219e-05\n",
      "Epoch 4914: train loss: 1.5365018043667078e-05\n",
      "Epoch 4915: train loss: 1.5322006220230833e-05\n",
      "Epoch 4916: train loss: 1.5646022802684456e-05\n",
      "Epoch 4917: train loss: 1.5747149518574588e-05\n",
      "Epoch 4918: train loss: 1.628809332032688e-05\n",
      "Epoch 4919: train loss: 1.661619171500206e-05\n",
      "Epoch 4920: train loss: 1.74380220414605e-05\n",
      "Epoch 4921: train loss: 1.7874252080218866e-05\n",
      "Epoch 4922: train loss: 1.885246456367895e-05\n",
      "Epoch 4923: train loss: 1.9287625036668032e-05\n",
      "Epoch 4924: train loss: 2.0222654711687937e-05\n",
      "Epoch 4925: train loss: 2.0418821804923937e-05\n",
      "Epoch 4926: train loss: 2.1080690203234553e-05\n",
      "Epoch 4927: train loss: 2.0803592633455992e-05\n",
      "Epoch 4928: train loss: 2.093173134198878e-05\n",
      "Epoch 4929: train loss: 2.0197649064357392e-05\n",
      "Epoch 4930: train loss: 1.98547186300857e-05\n",
      "Epoch 4931: train loss: 1.8856295355362818e-05\n",
      "Epoch 4932: train loss: 1.8320815797778778e-05\n",
      "Epoch 4933: train loss: 1.7358946934109554e-05\n",
      "Epoch 4934: train loss: 1.6867363228811882e-05\n",
      "Epoch 4935: train loss: 1.617448651813902e-05\n",
      "Epoch 4936: train loss: 1.597090158611536e-05\n",
      "Epoch 4937: train loss: 1.567955951031763e-05\n",
      "Epoch 4938: train loss: 1.5951278328429908e-05\n",
      "Epoch 4939: train loss: 1.621043156774249e-05\n",
      "Epoch 4940: train loss: 1.7196489352500066e-05\n",
      "Epoch 4941: train loss: 1.82570620381739e-05\n",
      "Epoch 4942: train loss: 2.0508712623268366e-05\n",
      "Epoch 4943: train loss: 2.2878059098729864e-05\n",
      "Epoch 4944: train loss: 2.744581433944404e-05\n",
      "Epoch 4945: train loss: 3.199779530405067e-05\n",
      "Epoch 4946: train loss: 4.052082294947468e-05\n",
      "Epoch 4947: train loss: 4.7863475629128516e-05\n",
      "Epoch 4948: train loss: 6.145220686448738e-05\n",
      "Epoch 4949: train loss: 6.993121496634558e-05\n",
      "Epoch 4950: train loss: 8.502142009092495e-05\n",
      "Epoch 4951: train loss: 8.815427281660959e-05\n",
      "Epoch 4952: train loss: 9.355986549053341e-05\n",
      "Epoch 4953: train loss: 8.272127888631076e-05\n",
      "Epoch 4954: train loss: 7.002073834883049e-05\n",
      "Epoch 4955: train loss: 4.8664576752344146e-05\n",
      "Epoch 4956: train loss: 3.0045679523027502e-05\n",
      "Epoch 4957: train loss: 1.6189243979169987e-05\n",
      "Epoch 4958: train loss: 1.0535753062868025e-05\n",
      "Epoch 4959: train loss: 1.2330797289905604e-05\n",
      "Epoch 4960: train loss: 1.9110922949039377e-05\n",
      "Epoch 4961: train loss: 2.8599428333109245e-05\n",
      "Epoch 4962: train loss: 3.751947951968759e-05\n",
      "Epoch 4963: train loss: 4.8774632887216285e-05\n",
      "Epoch 4964: train loss: 5.631566091324203e-05\n",
      "Epoch 4965: train loss: 7.144307892303914e-05\n",
      "Epoch 4966: train loss: 7.930475112516433e-05\n",
      "Epoch 4967: train loss: 0.00010061751527246088\n",
      "Epoch 4968: train loss: 0.00010606532305246219\n",
      "Epoch 4969: train loss: 0.00012186969979666173\n",
      "Epoch 4970: train loss: 0.00011153597733937204\n",
      "Epoch 4971: train loss: 9.8706383141689e-05\n",
      "Epoch 4972: train loss: 6.743796257069334e-05\n",
      "Epoch 4973: train loss: 3.788853427977301e-05\n",
      "Epoch 4974: train loss: 1.668046934355516e-05\n",
      "Epoch 4975: train loss: 1.1390739018679596e-05\n",
      "Epoch 4976: train loss: 1.9918810721719638e-05\n",
      "Epoch 4977: train loss: 3.41278500854969e-05\n",
      "Epoch 4978: train loss: 4.6495479182340205e-05\n",
      "Epoch 4979: train loss: 4.939389691571705e-05\n",
      "Epoch 4980: train loss: 4.6661985834361985e-05\n",
      "Epoch 4981: train loss: 3.6483554140431806e-05\n",
      "Epoch 4982: train loss: 2.7249878257862292e-05\n",
      "Epoch 4983: train loss: 1.844244616222568e-05\n",
      "Epoch 4984: train loss: 1.3006014341954142e-05\n",
      "Epoch 4985: train loss: 1.0082390872412361e-05\n",
      "Epoch 4986: train loss: 9.360719559481367e-06\n",
      "Epoch 4987: train loss: 1.024244829750387e-05\n",
      "Epoch 4988: train loss: 1.2308571058383677e-05\n",
      "Epoch 4989: train loss: 1.559749034640845e-05\n",
      "Epoch 4990: train loss: 1.968167271115817e-05\n",
      "Epoch 4991: train loss: 2.5955232558771968e-05\n",
      "Epoch 4992: train loss: 3.207464760635048e-05\n",
      "Epoch 4993: train loss: 4.188177626929246e-05\n",
      "Epoch 4994: train loss: 4.8219546442851424e-05\n",
      "Epoch 4995: train loss: 5.754855374107137e-05\n",
      "Epoch 4996: train loss: 5.832100578118116e-05\n",
      "Epoch 4997: train loss: 5.855553536093794e-05\n",
      "Epoch 4998: train loss: 4.903012450085953e-05\n",
      "Epoch 4999: train loss: 3.8136771763674915e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "epoch = 5000\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train_tensor)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    \n",
    "    # Zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after training 0.06700544059276581\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X_val_tensor)\n",
    "after_train = criterion(y_pred.squeeze(), y_val_tensor)\n",
    "print('Test loss after training' , after_train.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a test using predefined values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.7000, 2.6000, 3.5000, 1.0000]), 'versicolor')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_tensor[5], iris.target_names[y_train_tensor[5].int().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0013], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input = X_train_tensor[5]\n",
    "\n",
    "model.eval()\n",
    "model(X_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
